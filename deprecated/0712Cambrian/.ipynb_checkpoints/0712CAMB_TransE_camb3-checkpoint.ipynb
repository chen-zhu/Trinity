{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component/Abstraction Representation Learning using TransE\n",
    "- Stage: Cambrian\n",
    "- Version: Charniodiscus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "print(\"use_cuda: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morpheus Version\n",
    "from utils_morpheus import *\n",
    "from ProgramSpace import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphGenDataset(Dataset):\n",
    "    def __init__(self, p_spec=None, p_generator=None, p_interpreter=None, p_dps=None, p_len=None):\n",
    "        self.spec = p_spec\n",
    "        self.generator = p_generator\n",
    "        self.interpreter = p_interpreter\n",
    "        self.dps = p_dps\n",
    "        self.len = p_len\n",
    "        \n",
    "        # construct a shell list/dict so that every function call (shell) has id\n",
    "        self.shell_list = self.dps.get_neighboring_shells()\n",
    "        self.shell_dict = {\n",
    "            self.shell_list[i]:i for i in range(len(self.shell_list))\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, p_ind):\n",
    "        # basically ignore the p_ind parameter and generate randomly\n",
    "        # returns a pair of abstraction maps\n",
    "        # (tmp_in, tmp_out)\n",
    "        # which is (n_maps, CAMB_NROW, CAMB_NCOL)\n",
    "        # currently assuming single input single output\n",
    "        \n",
    "        while True:\n",
    "            tmp_input = self.interpreter.random_table()\n",
    "            tmp_prog, tmp_example = self.generator.generate(\n",
    "                fixed_depth=2, # this should be fixed to 2 since we learn size 1\n",
    "                example=Example(input=[tmp_input], output=None),\n",
    "            )\n",
    "            if tmp_prog is not None and tmp_prog.is_apply():\n",
    "                break\n",
    "\n",
    "        tmp_in = camb_get_abs(tmp_example.input[0])\n",
    "        tmp_out= camb_get_abs(tmp_example.output)\n",
    "\n",
    "        tmp_func = self.shell_dict[\n",
    "            (\n",
    "                self.dps.prod_list.index(tmp_prog.production),\n",
    "                tuple(\n",
    "                    [self.dps.node_list.index(tmp_prog.args[i])\n",
    "                    for i in range(len(tmp_prog.args))]\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # =========================================================\n",
    "        # you should also produce a negative sample here and return\n",
    "\n",
    "        while True:\n",
    "            rep_in = tmp_in\n",
    "            rep_out = tmp_out\n",
    "            rep_func = random.choice(range(len(self.shell_list)))\n",
    "            if rep_func==tmp_func:\n",
    "                continue\n",
    "            else:\n",
    "                # in fact, not safe\n",
    "                # should verify that rep_func(rep_in)!=rep_out\n",
    "                # but never mind here\n",
    "                break\n",
    "        \n",
    "        return (tmp_in, tmp_func, tmp_out,\n",
    "                rep_in, rep_func, rep_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarginLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MarginLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pos, neg, margin):\n",
    "        if use_cuda:\n",
    "            zero_tensor = torch.tensor(pos.size(),dtype=torch.float).cuda()\n",
    "        else:\n",
    "            zero_tensor = torch.tensor(pos.size(),dtype=torch.float)\n",
    "        zero_tensor.zero_()\n",
    "        zero_tensor = Variable(zero_tensor)\n",
    "        return torch.sum(torch.max(pos - neg + margin, zero_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_loss(embeddings, dim=1):\n",
    "    norm = torch.sum(embeddings ** 2, dim=dim, keepdim=True)\n",
    "    if use_cuda:\n",
    "        return torch.sum(\n",
    "            torch.max(\n",
    "                norm - Variable(torch.tensor([1.0],dtype=torch.float)).cuda(), \n",
    "                Variable(torch.tensor([0.0],dtype=torch.float)).cuda(),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        return torch.sum(\n",
    "            torch.max(\n",
    "                norm - Variable(torch.tensor([1.0],dtype=torch.float)), \n",
    "                Variable(torch.tensor([0.0],dtype=torch.float)),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEncoder(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(ValueEncoder, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        self.vocab_size = self.config[\"val\"][\"vocab_size\"]\n",
    "        self.embd_dim = self.config[\"val\"][\"embd_dim\"]\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embd_dim,\n",
    "            self.config[\"val\"][\"IDX_PAD\"],\n",
    "        )\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels = self.config[\"val\"][\"embd_dim\"],\n",
    "            out_channels = self.config[\"val\"][\"conv_n_kernels\"],\n",
    "            kernel_size = self.config[\"val\"][\"conv_kernel_size\"],\n",
    "        )\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size = self.config[\"val\"][\"pool_kernel_size\"],\n",
    "            padding = self.config[\"val\"][\"IDX_PAD\"],\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            self.config[\"val\"][\"conv_n_kernels\"],\n",
    "            self.config[\"embd_dim\"],\n",
    "        )\n",
    "        \n",
    "    def forward(self, bp_map):\n",
    "        # batched maps, (B, map_r, map_c)\n",
    "        # in this version, every value only contains 1 map\n",
    "        B = bp_map.shape[0]\n",
    "        \n",
    "        # (B, map_r, map_c, val_embd_dim) -> (B, val_embd_dim, map_r, map_c)\n",
    "        d_embd = self.embedding(bp_map).permute(0,3,1,2)\n",
    "        \n",
    "        # (B, n_kernel, map_r, 1)\n",
    "        d_conv = F.relu(self.conv(d_embd))\n",
    "        \n",
    "        # (B, n_kernel)\n",
    "        d_pool = self.pool(d_conv).view(B,self.config[\"val\"][\"conv_n_kernels\"])\n",
    "        \n",
    "        # (B, embd_dim)\n",
    "        d_out = torch.sigmoid(\n",
    "            self.fc(d_pool)\n",
    "        )\n",
    "        \n",
    "        return d_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphTransE(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(MorphTransE, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        self.value_encoder = ValueEncoder(p_config=p_config)\n",
    "        \n",
    "        self.fn_vocab_size = self.config[\"fn\"][\"vocab_size\"]\n",
    "        self.embd_dim = self.config[\"embd_dim\"]\n",
    "        \n",
    "        self.fn_embedding = nn.Embedding(\n",
    "            self.fn_vocab_size,\n",
    "            self.embd_dim,\n",
    "        )\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fn_embedding.weight.data)\n",
    "        self.fn_embedding.weight.data = F.normalize(\n",
    "            self.fn_embedding.weight.data, p=2, dim=1,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, batch_triplets):\n",
    "        # print(\"batch_triplets:{}\".format(batch_triplets))\n",
    "        \n",
    "        # v_in = self.val_embedding(batch_triplets[0])\n",
    "        v_in = self.value_encoder(batch_triplets[0])\n",
    "        v_fn = self.fn_embedding(batch_triplets[1])\n",
    "        # v_out = self.val_embedding(batch_triplets[2])\n",
    "        v_out = self.value_encoder(batch_triplets[2])\n",
    "        \n",
    "        # r_in = self.val_embedding(batch_triplets[3])\n",
    "        r_in = self.value_encoder(batch_triplets[3])\n",
    "        r_fn = self.fn_embedding(batch_triplets[4])\n",
    "        # r_out = self.val_embedding(batch_triplets[5])\n",
    "        r_out = self.value_encoder(batch_triplets[5])\n",
    "        \n",
    "        pos_score = torch.sum((v_in + v_fn - v_out) ** 2, 1)\n",
    "        neg_score = torch.sum((r_in + r_fn - r_out) ** 2, 1)\n",
    "        \n",
    "        # (B, 1), ..\n",
    "        return (pos_score, neg_score)\n",
    "    \n",
    "    def infer_fn(self, ios):\n",
    "        # ios: (\n",
    "        #   (B, n_maps, map_r, map_c), --> input\n",
    "        #   (B, n_maps, map_r, map_c), --> output\n",
    "        # )\n",
    "        B = ios[0].shape[0]\n",
    "        \n",
    "        # v_in = self.val_embedding(ios[0]) # (B, embd_dim)\n",
    "        # v_out = self.val_embedding(ios[1]) # (B, embd_dim)\n",
    "        v_in = self.value_encoder(ios[0])\n",
    "        v_out = self.value_encoder(ios[1])\n",
    "        \n",
    "        est_fn = v_out-v_in # (B, embd_dim)\n",
    "        # print(est_fn.shape)\n",
    "        # print(self.fn_embedding.weight.data.shape)\n",
    "        # input(\"PAUSE\")\n",
    "        \n",
    "        dlist = []\n",
    "        for i in range(B):\n",
    "            sublist = []\n",
    "            for j in range(self.fn_vocab_size):\n",
    "                sublist.append(\n",
    "                    torch.dist(est_fn[i,:], self.fn_embedding.weight.data[j,:]) # (1,)\n",
    "                )\n",
    "            dlist.append(\n",
    "                torch.tensor([sublist])\n",
    "            )\n",
    "        \n",
    "        ret_dlist = torch.cat(dlist,dim=0)\n",
    "        \n",
    "        return ret_dlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "use the same ld in trainer if sampling randomly every time\n",
    "'''\n",
    "def MTETester(p_model, p_ld_data):\n",
    "    \n",
    "    # ### FUNCTION prediction ### #\n",
    "    rank_list = []\n",
    "    for batch_idx, bts in enumerate(p_ld_data):\n",
    "        p_model.eval()\n",
    "        B = bts[0].shape[0]\n",
    "        \n",
    "        if use_cuda:\n",
    "            td_bts = [Variable(bts[i]).cuda() for i in range(len(bts))]\n",
    "        else:\n",
    "            td_bts = [Variable(bts[i]) for i in range(len(bts))]\n",
    "        \n",
    "        # feed the true input and output, and infer the function\n",
    "        # return the function scores\n",
    "        # (B, fn_vocab_size)\n",
    "        \n",
    "        d_scores = p_model.infer_fn(\n",
    "            (td_bts[0],td_bts[2])\n",
    "        )\n",
    "        \n",
    "        sorted_scores = torch.argsort(d_scores,dim=1).cpu().numpy()\n",
    "        for i in range(B):\n",
    "            rank_list.append(\n",
    "                sorted_scores[i,:].tolist().index(bts[1][i])\n",
    "            )\n",
    "            \n",
    "        print(\"\\r# TEST/FUNCTION B:{}\".format(batch_idx),end=\"\")\n",
    "        \n",
    "    print()\n",
    "    print(\"# TEST/FUNCTION avg.rank:{:.2f}, b:{:.2f}, w:{:.2f}, 50p:{:.2f}, 75p:{:.2f}, 90p:{:.2f}\".format(\n",
    "        sum(rank_list)/len(rank_list), \n",
    "        min(rank_list), \n",
    "        max(rank_list), \n",
    "        np.percentile(rank_list,50),\n",
    "        np.percentile(rank_list,75),\n",
    "        np.percentile(rank_list,90)\n",
    "    ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MTETrainer(p_nep, p_model, p_ld_train, p_ld_test, p_optim, p_lossfn):\n",
    "    MTETester(p_model, p_ld_test)\n",
    "    for d_ep in range(p_nep):\n",
    "        train_loss_list = []\n",
    "        for batch_idx, bts in enumerate(p_ld_train):\n",
    "            p_model.train()\n",
    "            if use_cuda:\n",
    "                td_bts = [Variable(bts[i]).cuda() for i in range(len(bts))]\n",
    "            else:\n",
    "                td_bts = [Variable(bts[i]) for i in range(len(bts))]\n",
    "            \n",
    "            d_scores = p_model(td_bts) # (pos,neg)\n",
    "            p_optim.zero_grad()\n",
    "            if use_cuda:\n",
    "                margin = Variable(torch.tensor([1],dtype=torch.float)).cuda()\n",
    "            else:\n",
    "                margin = Variable(torch.tensor([1],dtype=torch.float))\n",
    "            d_loss = p_lossfn(d_scores[0],d_scores[1],margin)\n",
    "            d_loss += norm_loss(p_model.value_encoder(td_bts[0]))\n",
    "            d_loss += norm_loss(p_model.fn_embedding(td_bts[1]))\n",
    "            d_loss += norm_loss(p_model.value_encoder(td_bts[2]))\n",
    "            d_loss += norm_loss(p_model.value_encoder(td_bts[3]))\n",
    "            d_loss += norm_loss(p_model.fn_embedding(td_bts[4]))\n",
    "            d_loss += norm_loss(p_model.value_encoder(td_bts[5]))\n",
    "            train_loss_list.append(d_loss)\n",
    "            d_loss.backward()\n",
    "            p_optim.step()\n",
    "            \n",
    "            print(\"\\r# TRAIN EP{}, B:{}, L:{:.4f}, AvgL:{:.4f}\".format(\n",
    "                d_ep, batch_idx, d_loss,\n",
    "                float(sum(train_loss_list))/float(len(train_loss_list))\n",
    "            ),end=\"\")\n",
    "        print()\n",
    "        if d_ep%50==0:\n",
    "            MTETester(p_model, p_ld_test)\n",
    "            # and also save the model\n",
    "            torch.save(\n",
    "                p_model.state_dict(),\n",
    "                \"./saved_models/0712CAMB_TransE_camb3_ep{}.pt\".format(d_ep)\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_interpreter = MorpheusInterpreter()\n",
    "# m_spec = S.parse_file('./example/set_select.tyrell')\n",
    "# m_spec = S.parse_file('./example/set_unite.tyrell')\n",
    "# m_spec = S.parse_file('./example/camb2.tyrell')\n",
    "m_spec = S.parse_file('./example/camb3.tyrell')\n",
    "m_generator = MorpheusGenerator(\n",
    "    spec=m_spec,\n",
    "    interpreter=m_interpreter,\n",
    "    sfn=m_interpreter.sanity_check,\n",
    ")\n",
    "# dumb Program Space\n",
    "m_dps = ProgramSpace(\n",
    "    m_spec, m_interpreter, [None], None\n",
    ")\n",
    "\n",
    "\n",
    "dt_mg_train = MorphGenDataset(p_spec=m_spec, p_generator=m_generator, p_interpreter=m_interpreter, p_dps=m_dps, p_len=1024)\n",
    "ld_mg_train = DataLoader(dataset=dt_mg_train, batch_size=8, shuffle=False)\n",
    "\n",
    "dt_mg_test = MorphGenDataset(p_spec=m_spec, p_generator=m_generator, p_interpreter=m_interpreter, p_dps=m_dps, p_len=512)\n",
    "ld_mg_test = DataLoader(dataset=dt_mg_test, batch_size=8, shuffle=False)\n",
    "\n",
    "m_config = {\n",
    "    \"val\":{\n",
    "        \"vocab_size\": len(CAMB_LIST),\n",
    "        \"embd_dim\": 16, # embedding dim of CAMB abstract token\n",
    "        \"conv_n_kernels\": 512,\n",
    "        \"conv_kernel_size\": (1,CAMB_NCOL), \n",
    "        \"pool_kernel_size\": (CAMB_NROW,1), \n",
    "        \"IDX_PAD\": 0,\n",
    "    },\n",
    "    \"fn\":{\n",
    "        \"vocab_size\": len(dt_mg_train.shell_list)\n",
    "    },\n",
    "    \"embd_dim\":128,\n",
    "}\n",
    "\n",
    "mte = MorphTransE(p_config=m_config)\n",
    "m_loss = MarginLoss()\n",
    "if use_cuda:\n",
    "    mte = mte.cuda()\n",
    "    m_loss = m_loss.cuda()\n",
    "optimizer = torch.optim.Adam(mte.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': {'vocab_size': 150,\n",
       "  'embd_dim': 16,\n",
       "  'conv_n_kernels': 512,\n",
       "  'conv_kernel_size': (1, 15),\n",
       "  'pool_kernel_size': (15, 1),\n",
       "  'IDX_PAD': 0},\n",
       " 'fn': {'vocab_size': 120},\n",
       " 'embd_dim': 128}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:64.36, b:0.00, w:119.00, 50p:65.50, 75p:98.25, 90p:112.90\n",
      "# TRAIN EP0, B:127, L:6.3070, AvgL:44.8701\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:24.59, b:0.00, w:117.00, 50p:17.00, 75p:36.00, 90p:63.90\n",
      "# TRAIN EP1, B:127, L:6.4401, AvgL:6.2535\n",
      "# TRAIN EP2, B:127, L:4.1169, AvgL:5.0864\n",
      "# TRAIN EP3, B:127, L:4.5205, AvgL:5.0505\n",
      "# TRAIN EP4, B:127, L:5.4825, AvgL:4.6656\n",
      "# TRAIN EP5, B:127, L:7.5986, AvgL:4.7268\n",
      "# TRAIN EP6, B:127, L:1.3850, AvgL:4.5625\n",
      "# TRAIN EP7, B:127, L:3.8411, AvgL:4.4511\n",
      "# TRAIN EP8, B:127, L:3.7240, AvgL:4.3942\n",
      "# TRAIN EP9, B:127, L:4.0454, AvgL:4.3394\n",
      "# TRAIN EP10, B:127, L:3.1828, AvgL:4.1786\n",
      "# TRAIN EP11, B:127, L:5.5428, AvgL:4.2589\n",
      "# TRAIN EP12, B:127, L:2.8652, AvgL:4.0529\n",
      "# TRAIN EP13, B:127, L:4.3685, AvgL:4.2449\n",
      "# TRAIN EP14, B:127, L:5.4198, AvgL:3.8443\n",
      "# TRAIN EP15, B:127, L:2.8615, AvgL:3.8709\n",
      "# TRAIN EP16, B:127, L:3.3254, AvgL:3.5339\n",
      "# TRAIN EP17, B:127, L:5.2630, AvgL:3.4284\n",
      "# TRAIN EP18, B:127, L:4.1155, AvgL:3.3483\n",
      "# TRAIN EP19, B:127, L:1.9448, AvgL:3.2235\n",
      "# TRAIN EP20, B:127, L:1.5328, AvgL:3.2002\n",
      "# TRAIN EP21, B:127, L:3.0096, AvgL:2.9460\n",
      "# TRAIN EP22, B:127, L:4.3080, AvgL:3.0125\n",
      "# TRAIN EP23, B:127, L:2.3912, AvgL:2.8156\n",
      "# TRAIN EP24, B:127, L:2.5367, AvgL:2.7242\n",
      "# TRAIN EP25, B:127, L:2.5673, AvgL:2.5501\n",
      "# TRAIN EP26, B:127, L:2.7608, AvgL:2.5384\n",
      "# TRAIN EP27, B:127, L:2.7443, AvgL:2.3181\n",
      "# TRAIN EP28, B:127, L:2.6490, AvgL:2.6470\n",
      "# TRAIN EP29, B:127, L:1.9182, AvgL:2.3249\n",
      "# TRAIN EP30, B:127, L:1.1933, AvgL:2.3515\n",
      "# TRAIN EP31, B:127, L:0.8984, AvgL:2.1283\n",
      "# TRAIN EP32, B:127, L:1.4304, AvgL:2.3243\n",
      "# TRAIN EP33, B:127, L:2.4316, AvgL:2.1753\n",
      "# TRAIN EP34, B:127, L:1.5929, AvgL:2.0065\n",
      "# TRAIN EP35, B:127, L:2.3298, AvgL:2.2191\n",
      "# TRAIN EP36, B:127, L:4.3087, AvgL:2.0665\n",
      "# TRAIN EP37, B:127, L:1.7655, AvgL:2.0398\n",
      "# TRAIN EP38, B:127, L:4.3109, AvgL:2.1764\n",
      "# TRAIN EP39, B:127, L:3.4423, AvgL:1.9145\n",
      "# TRAIN EP40, B:127, L:2.0870, AvgL:1.9095\n",
      "# TRAIN EP41, B:127, L:1.2634, AvgL:1.9707\n",
      "# TRAIN EP42, B:127, L:1.5917, AvgL:1.9902\n",
      "# TRAIN EP43, B:127, L:0.2818, AvgL:1.8172\n",
      "# TRAIN EP44, B:127, L:2.0264, AvgL:1.8232\n",
      "# TRAIN EP45, B:127, L:3.2668, AvgL:1.9130\n",
      "# TRAIN EP46, B:127, L:2.8948, AvgL:1.9229\n",
      "# TRAIN EP47, B:127, L:3.7630, AvgL:1.6980\n",
      "# TRAIN EP48, B:127, L:1.4643, AvgL:1.7836\n",
      "# TRAIN EP49, B:127, L:2.4413, AvgL:1.7588\n",
      "# TRAIN EP50, B:127, L:0.5932, AvgL:1.7411\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:7.27, b:0.00, w:94.00, 50p:4.00, 75p:10.00, 90p:18.90\n",
      "# TRAIN EP51, B:127, L:0.1682, AvgL:1.7119\n",
      "# TRAIN EP52, B:127, L:2.9984, AvgL:1.8466\n",
      "# TRAIN EP53, B:127, L:2.2429, AvgL:1.7524\n",
      "# TRAIN EP54, B:127, L:3.1722, AvgL:1.6757\n",
      "# TRAIN EP55, B:127, L:2.0615, AvgL:1.7080\n",
      "# TRAIN EP56, B:127, L:1.9429, AvgL:1.6261\n",
      "# TRAIN EP57, B:127, L:0.6869, AvgL:1.7088\n",
      "# TRAIN EP58, B:127, L:2.3744, AvgL:1.7287\n",
      "# TRAIN EP59, B:127, L:0.1752, AvgL:1.6569\n",
      "# TRAIN EP60, B:127, L:2.1678, AvgL:1.8144\n",
      "# TRAIN EP61, B:127, L:1.1035, AvgL:1.6606\n",
      "# TRAIN EP62, B:127, L:0.5691, AvgL:1.6485\n",
      "# TRAIN EP63, B:127, L:2.7942, AvgL:1.7680\n",
      "# TRAIN EP64, B:127, L:3.1713, AvgL:1.5225\n",
      "# TRAIN EP65, B:127, L:2.7735, AvgL:1.6578\n",
      "# TRAIN EP66, B:127, L:0.9900, AvgL:1.5088\n",
      "# TRAIN EP67, B:127, L:0.7107, AvgL:1.4114\n",
      "# TRAIN EP68, B:127, L:3.5506, AvgL:1.5963\n",
      "# TRAIN EP69, B:127, L:2.4223, AvgL:1.5374\n",
      "# TRAIN EP70, B:127, L:0.6518, AvgL:1.5876\n",
      "# TRAIN EP71, B:127, L:1.1681, AvgL:1.4655\n",
      "# TRAIN EP72, B:127, L:0.5122, AvgL:1.4344\n",
      "# TRAIN EP73, B:127, L:1.7416, AvgL:1.4006\n",
      "# TRAIN EP74, B:127, L:2.3573, AvgL:1.5903\n",
      "# TRAIN EP75, B:127, L:1.2832, AvgL:1.5661\n",
      "# TRAIN EP76, B:127, L:1.4253, AvgL:1.5347\n",
      "# TRAIN EP77, B:127, L:0.9251, AvgL:1.4170\n",
      "# TRAIN EP78, B:127, L:0.5880, AvgL:1.5823\n",
      "# TRAIN EP79, B:127, L:1.4114, AvgL:1.5994\n",
      "# TRAIN EP80, B:127, L:2.5187, AvgL:1.5088\n",
      "# TRAIN EP81, B:127, L:2.3002, AvgL:1.5144\n",
      "# TRAIN EP82, B:127, L:0.9177, AvgL:1.5689\n",
      "# TRAIN EP83, B:127, L:1.3553, AvgL:1.5067\n",
      "# TRAIN EP84, B:127, L:3.4551, AvgL:1.5004\n",
      "# TRAIN EP85, B:127, L:1.6862, AvgL:1.4519\n",
      "# TRAIN EP86, B:127, L:3.4722, AvgL:1.5501\n",
      "# TRAIN EP87, B:127, L:1.0999, AvgL:1.5314\n",
      "# TRAIN EP88, B:127, L:1.8648, AvgL:1.3576\n",
      "# TRAIN EP89, B:127, L:2.6566, AvgL:1.7004\n",
      "# TRAIN EP90, B:127, L:1.7281, AvgL:1.4959\n",
      "# TRAIN EP91, B:127, L:0.1420, AvgL:1.3416\n",
      "# TRAIN EP92, B:127, L:1.3369, AvgL:1.2787\n",
      "# TRAIN EP93, B:127, L:0.0001, AvgL:1.3938\n",
      "# TRAIN EP94, B:127, L:1.7830, AvgL:1.2928\n",
      "# TRAIN EP95, B:127, L:1.9731, AvgL:1.5575\n",
      "# TRAIN EP96, B:127, L:1.1650, AvgL:1.4830\n",
      "# TRAIN EP97, B:127, L:1.1125, AvgL:1.4098\n",
      "# TRAIN EP98, B:127, L:1.4765, AvgL:1.4110\n",
      "# TRAIN EP99, B:127, L:0.1042, AvgL:1.4152\n",
      "# TRAIN EP100, B:127, L:0.6719, AvgL:1.3823\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:6.14, b:0.00, w:59.00, 50p:3.00, 75p:8.00, 90p:16.80\n",
      "# TRAIN EP101, B:127, L:0.7974, AvgL:1.4964\n",
      "# TRAIN EP102, B:127, L:1.0668, AvgL:1.4541\n",
      "# TRAIN EP103, B:127, L:2.0376, AvgL:1.4949\n",
      "# TRAIN EP104, B:127, L:0.0032, AvgL:1.2805\n",
      "# TRAIN EP105, B:127, L:1.0845, AvgL:1.4517\n",
      "# TRAIN EP106, B:127, L:1.9515, AvgL:1.4751\n",
      "# TRAIN EP107, B:127, L:1.4784, AvgL:1.4549\n",
      "# TRAIN EP108, B:127, L:0.8919, AvgL:1.3627\n",
      "# TRAIN EP109, B:127, L:0.1776, AvgL:1.3042\n",
      "# TRAIN EP110, B:127, L:0.5920, AvgL:1.3225\n",
      "# TRAIN EP111, B:127, L:0.3300, AvgL:1.1195\n",
      "# TRAIN EP112, B:127, L:0.0000, AvgL:1.3703\n",
      "# TRAIN EP113, B:127, L:0.5668, AvgL:1.4649\n",
      "# TRAIN EP114, B:127, L:0.8768, AvgL:1.4879\n",
      "# TRAIN EP115, B:127, L:1.1196, AvgL:1.3583\n",
      "# TRAIN EP116, B:127, L:2.9208, AvgL:1.2638\n",
      "# TRAIN EP117, B:127, L:0.2275, AvgL:1.5163\n",
      "# TRAIN EP118, B:127, L:0.7007, AvgL:1.2852\n",
      "# TRAIN EP119, B:127, L:0.9344, AvgL:1.2290\n",
      "# TRAIN EP120, B:127, L:0.5545, AvgL:1.3105\n",
      "# TRAIN EP121, B:127, L:0.7351, AvgL:1.3983\n",
      "# TRAIN EP122, B:127, L:1.3519, AvgL:1.2837\n",
      "# TRAIN EP123, B:127, L:0.5020, AvgL:1.1542\n",
      "# TRAIN EP124, B:127, L:2.7127, AvgL:1.2901\n",
      "# TRAIN EP125, B:127, L:1.5824, AvgL:1.2910\n",
      "# TRAIN EP126, B:127, L:3.6531, AvgL:1.2036\n",
      "# TRAIN EP127, B:127, L:1.9785, AvgL:1.3208\n",
      "# TRAIN EP128, B:127, L:0.9383, AvgL:1.3374\n",
      "# TRAIN EP129, B:127, L:3.0824, AvgL:1.1481\n",
      "# TRAIN EP130, B:127, L:1.6671, AvgL:1.1273\n",
      "# TRAIN EP131, B:127, L:1.1399, AvgL:1.1835\n",
      "# TRAIN EP132, B:127, L:0.5258, AvgL:1.1710\n",
      "# TRAIN EP133, B:127, L:0.1411, AvgL:1.0254\n",
      "# TRAIN EP134, B:127, L:0.5037, AvgL:1.2728\n",
      "# TRAIN EP135, B:127, L:2.2295, AvgL:1.0618\n",
      "# TRAIN EP136, B:127, L:2.1418, AvgL:1.2087\n",
      "# TRAIN EP137, B:127, L:2.3897, AvgL:1.0221\n",
      "# TRAIN EP138, B:127, L:0.0153, AvgL:0.9496\n",
      "# TRAIN EP139, B:127, L:1.0613, AvgL:1.0808\n",
      "# TRAIN EP140, B:127, L:0.3231, AvgL:1.0864\n",
      "# TRAIN EP141, B:127, L:0.1606, AvgL:1.0381\n",
      "# TRAIN EP142, B:127, L:0.1531, AvgL:0.9567\n",
      "# TRAIN EP143, B:127, L:1.1677, AvgL:1.0866\n",
      "# TRAIN EP144, B:127, L:0.2311, AvgL:1.0128\n",
      "# TRAIN EP145, B:127, L:1.1732, AvgL:1.0640\n",
      "# TRAIN EP146, B:127, L:2.1488, AvgL:1.0173\n",
      "# TRAIN EP147, B:127, L:0.8912, AvgL:1.0419\n",
      "# TRAIN EP148, B:127, L:0.1456, AvgL:1.0332\n",
      "# TRAIN EP149, B:127, L:1.5773, AvgL:1.0527\n",
      "# TRAIN EP150, B:127, L:0.5131, AvgL:0.9853\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:5.73, b:0.00, w:82.00, 50p:3.00, 75p:7.00, 90p:13.00\n",
      "# TRAIN EP151, B:127, L:1.6884, AvgL:0.8998\n",
      "# TRAIN EP152, B:127, L:2.0657, AvgL:1.0384\n",
      "# TRAIN EP153, B:127, L:0.0332, AvgL:0.9661\n",
      "# TRAIN EP154, B:127, L:0.0897, AvgL:0.8701\n",
      "# TRAIN EP155, B:127, L:0.0092, AvgL:0.8081\n",
      "# TRAIN EP156, B:127, L:0.0908, AvgL:0.8891\n",
      "# TRAIN EP157, B:127, L:0.0912, AvgL:0.9880\n",
      "# TRAIN EP158, B:127, L:1.6447, AvgL:0.8530\n",
      "# TRAIN EP159, B:127, L:0.5425, AvgL:0.9232\n",
      "# TRAIN EP160, B:127, L:0.1159, AvgL:0.9830\n",
      "# TRAIN EP161, B:127, L:0.0075, AvgL:0.9156\n",
      "# TRAIN EP162, B:127, L:0.0000, AvgL:0.9984\n",
      "# TRAIN EP163, B:127, L:0.0001, AvgL:1.0229\n",
      "# TRAIN EP164, B:127, L:1.0159, AvgL:0.8374\n",
      "# TRAIN EP165, B:127, L:1.9318, AvgL:1.0482\n",
      "# TRAIN EP166, B:127, L:0.5607, AvgL:0.9047\n",
      "# TRAIN EP167, B:127, L:0.7566, AvgL:0.8762\n",
      "# TRAIN EP168, B:127, L:1.0632, AvgL:0.8924\n",
      "# TRAIN EP169, B:127, L:0.2743, AvgL:0.8984\n",
      "# TRAIN EP170, B:127, L:2.0281, AvgL:0.7610\n",
      "# TRAIN EP171, B:127, L:0.7742, AvgL:0.9175\n",
      "# TRAIN EP172, B:127, L:0.2650, AvgL:0.8531\n",
      "# TRAIN EP173, B:127, L:0.0176, AvgL:0.8817\n",
      "# TRAIN EP174, B:127, L:1.2477, AvgL:0.9771\n",
      "# TRAIN EP175, B:127, L:2.1023, AvgL:0.9501\n",
      "# TRAIN EP176, B:127, L:0.8370, AvgL:0.9235\n",
      "# TRAIN EP177, B:127, L:2.9393, AvgL:1.0592\n",
      "# TRAIN EP178, B:127, L:1.1371, AvgL:0.9547\n",
      "# TRAIN EP179, B:127, L:0.1129, AvgL:0.9995\n",
      "# TRAIN EP180, B:127, L:1.2324, AvgL:0.8439\n",
      "# TRAIN EP181, B:127, L:0.6282, AvgL:0.7984\n",
      "# TRAIN EP182, B:127, L:0.7892, AvgL:0.7811\n",
      "# TRAIN EP183, B:127, L:0.0000, AvgL:0.9281\n",
      "# TRAIN EP184, B:127, L:1.3750, AvgL:0.9819\n",
      "# TRAIN EP185, B:127, L:0.5724, AvgL:0.9234\n",
      "# TRAIN EP186, B:127, L:2.4477, AvgL:0.8822\n",
      "# TRAIN EP187, B:127, L:0.3004, AvgL:0.8846\n",
      "# TRAIN EP188, B:127, L:0.1472, AvgL:0.8643\n",
      "# TRAIN EP189, B:127, L:0.3626, AvgL:0.8984\n",
      "# TRAIN EP190, B:127, L:1.4491, AvgL:0.8744\n",
      "# TRAIN EP191, B:127, L:0.9450, AvgL:0.8366\n",
      "# TRAIN EP192, B:127, L:0.0655, AvgL:0.8596\n",
      "# TRAIN EP193, B:127, L:0.7591, AvgL:0.8801\n",
      "# TRAIN EP194, B:127, L:1.2071, AvgL:0.9267\n",
      "# TRAIN EP195, B:127, L:0.8664, AvgL:0.8588\n",
      "# TRAIN EP196, B:127, L:0.6520, AvgL:0.8472\n",
      "# TRAIN EP197, B:127, L:2.0257, AvgL:0.8751\n",
      "# TRAIN EP198, B:127, L:1.5965, AvgL:0.9699\n",
      "# TRAIN EP199, B:127, L:0.4097, AvgL:0.9115\n",
      "# TRAIN EP200, B:127, L:0.0568, AvgL:0.8730\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.99, b:0.00, w:82.00, 50p:3.00, 75p:7.00, 90p:12.00\n",
      "# TRAIN EP201, B:127, L:1.1366, AvgL:0.8945\n",
      "# TRAIN EP202, B:127, L:1.3220, AvgL:0.9840\n",
      "# TRAIN EP203, B:127, L:0.0066, AvgL:0.6380\n",
      "# TRAIN EP204, B:127, L:3.1391, AvgL:0.7183\n",
      "# TRAIN EP205, B:127, L:0.7036, AvgL:0.7408\n",
      "# TRAIN EP206, B:127, L:0.9760, AvgL:0.7981\n",
      "# TRAIN EP207, B:127, L:0.0822, AvgL:0.8720\n",
      "# TRAIN EP208, B:127, L:0.0000, AvgL:0.8386\n",
      "# TRAIN EP209, B:127, L:1.1701, AvgL:0.8017\n",
      "# TRAIN EP210, B:127, L:2.4874, AvgL:0.8071\n",
      "# TRAIN EP211, B:127, L:1.5819, AvgL:0.8669\n",
      "# TRAIN EP212, B:127, L:0.0807, AvgL:0.8527\n",
      "# TRAIN EP213, B:127, L:2.2501, AvgL:0.7959\n",
      "# TRAIN EP214, B:127, L:1.1491, AvgL:0.7462\n",
      "# TRAIN EP215, B:127, L:0.0730, AvgL:0.8334\n",
      "# TRAIN EP216, B:127, L:1.8815, AvgL:0.8032\n",
      "# TRAIN EP217, B:127, L:0.1292, AvgL:0.8684\n",
      "# TRAIN EP218, B:127, L:1.0459, AvgL:0.7029\n",
      "# TRAIN EP219, B:127, L:0.0000, AvgL:0.8378\n",
      "# TRAIN EP220, B:127, L:0.9738, AvgL:0.7575\n",
      "# TRAIN EP221, B:127, L:0.0000, AvgL:0.9198\n",
      "# TRAIN EP222, B:127, L:0.0000, AvgL:0.7358\n",
      "# TRAIN EP223, B:127, L:1.8197, AvgL:0.8132\n",
      "# TRAIN EP224, B:127, L:3.3745, AvgL:0.7901\n",
      "# TRAIN EP225, B:127, L:3.7358, AvgL:0.7579\n",
      "# TRAIN EP226, B:127, L:1.2846, AvgL:0.7527\n",
      "# TRAIN EP227, B:127, L:1.0183, AvgL:0.7962\n",
      "# TRAIN EP228, B:127, L:0.0110, AvgL:0.7448\n",
      "# TRAIN EP229, B:127, L:0.2712, AvgL:0.8868\n",
      "# TRAIN EP230, B:127, L:0.0000, AvgL:0.7784\n",
      "# TRAIN EP231, B:127, L:1.1231, AvgL:0.7122\n",
      "# TRAIN EP232, B:127, L:0.0664, AvgL:0.7747\n",
      "# TRAIN EP233, B:127, L:0.7937, AvgL:0.8457\n",
      "# TRAIN EP234, B:127, L:1.3219, AvgL:0.6933\n",
      "# TRAIN EP235, B:127, L:0.7745, AvgL:0.7870\n",
      "# TRAIN EP236, B:127, L:1.3948, AvgL:0.7330\n",
      "# TRAIN EP237, B:127, L:0.0706, AvgL:0.8859\n",
      "# TRAIN EP238, B:127, L:0.2180, AvgL:0.7614\n",
      "# TRAIN EP239, B:127, L:0.1295, AvgL:0.8098\n",
      "# TRAIN EP240, B:127, L:1.0152, AvgL:0.7036\n",
      "# TRAIN EP241, B:127, L:0.4582, AvgL:0.9399\n",
      "# TRAIN EP242, B:127, L:0.6554, AvgL:0.9356\n",
      "# TRAIN EP243, B:127, L:0.0226, AvgL:0.7475\n",
      "# TRAIN EP244, B:127, L:0.8190, AvgL:0.7837\n",
      "# TRAIN EP245, B:127, L:0.0000, AvgL:0.8918\n",
      "# TRAIN EP246, B:127, L:0.7956, AvgL:0.8456\n",
      "# TRAIN EP247, B:127, L:0.5980, AvgL:0.7418\n",
      "# TRAIN EP248, B:127, L:1.5613, AvgL:0.7916\n",
      "# TRAIN EP249, B:127, L:0.0000, AvgL:0.7091\n",
      "# TRAIN EP250, B:127, L:1.4147, AvgL:0.7510\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.25, b:0.00, w:68.00, 50p:2.00, 75p:5.00, 90p:10.90\n",
      "# TRAIN EP251, B:127, L:0.0000, AvgL:0.7924\n",
      "# TRAIN EP252, B:127, L:2.6818, AvgL:0.8794\n",
      "# TRAIN EP253, B:127, L:3.1030, AvgL:0.6697\n",
      "# TRAIN EP254, B:127, L:0.0000, AvgL:0.5984\n",
      "# TRAIN EP255, B:127, L:1.1093, AvgL:0.7735\n",
      "# TRAIN EP256, B:127, L:1.4902, AvgL:0.7890\n",
      "# TRAIN EP257, B:127, L:0.0000, AvgL:0.7656\n",
      "# TRAIN EP258, B:127, L:0.0001, AvgL:0.6561\n",
      "# TRAIN EP259, B:127, L:1.6369, AvgL:0.6805\n",
      "# TRAIN EP260, B:127, L:0.0000, AvgL:0.7863\n",
      "# TRAIN EP261, B:127, L:0.0000, AvgL:0.7585\n",
      "# TRAIN EP262, B:127, L:0.0000, AvgL:0.7842\n",
      "# TRAIN EP263, B:127, L:0.0340, AvgL:0.7483\n",
      "# TRAIN EP264, B:127, L:2.0022, AvgL:0.7840\n",
      "# TRAIN EP265, B:127, L:0.0000, AvgL:0.7180\n",
      "# TRAIN EP266, B:127, L:0.5043, AvgL:0.8279\n",
      "# TRAIN EP267, B:127, L:0.1343, AvgL:0.8457\n",
      "# TRAIN EP268, B:127, L:2.7138, AvgL:0.9056\n",
      "# TRAIN EP269, B:127, L:0.8954, AvgL:0.7760\n",
      "# TRAIN EP270, B:127, L:0.5249, AvgL:0.8024\n",
      "# TRAIN EP271, B:127, L:0.0977, AvgL:0.8144\n",
      "# TRAIN EP272, B:127, L:0.8004, AvgL:0.8020\n",
      "# TRAIN EP273, B:127, L:0.8248, AvgL:0.7842\n",
      "# TRAIN EP274, B:127, L:1.1185, AvgL:0.7076\n",
      "# TRAIN EP275, B:127, L:1.6108, AvgL:0.7835\n",
      "# TRAIN EP276, B:127, L:0.6946, AvgL:0.7728\n",
      "# TRAIN EP277, B:127, L:0.5147, AvgL:0.5919\n",
      "# TRAIN EP278, B:127, L:1.0072, AvgL:0.6367\n",
      "# TRAIN EP279, B:127, L:0.0559, AvgL:0.7579\n",
      "# TRAIN EP280, B:127, L:1.1608, AvgL:0.7851\n",
      "# TRAIN EP281, B:127, L:0.8717, AvgL:0.7467\n",
      "# TRAIN EP282, B:127, L:1.4193, AvgL:0.6375\n",
      "# TRAIN EP283, B:127, L:0.8777, AvgL:0.6777\n",
      "# TRAIN EP284, B:127, L:0.4953, AvgL:0.6886\n",
      "# TRAIN EP285, B:127, L:1.6788, AvgL:0.8384\n",
      "# TRAIN EP286, B:127, L:0.0000, AvgL:0.7696\n",
      "# TRAIN EP287, B:127, L:2.9312, AvgL:0.8139\n",
      "# TRAIN EP288, B:127, L:0.0072, AvgL:0.7075\n",
      "# TRAIN EP289, B:127, L:0.0046, AvgL:0.7750\n",
      "# TRAIN EP290, B:127, L:0.9416, AvgL:0.8895\n",
      "# TRAIN EP291, B:127, L:1.0005, AvgL:0.6983\n",
      "# TRAIN EP292, B:127, L:0.0000, AvgL:0.8157\n",
      "# TRAIN EP293, B:127, L:0.0101, AvgL:0.7847\n",
      "# TRAIN EP294, B:127, L:0.0000, AvgL:0.7252\n",
      "# TRAIN EP295, B:127, L:0.2272, AvgL:0.8636\n",
      "# TRAIN EP296, B:127, L:0.0000, AvgL:0.7301\n",
      "# TRAIN EP297, B:127, L:0.0000, AvgL:0.7398\n",
      "# TRAIN EP298, B:127, L:0.6479, AvgL:0.7513\n",
      "# TRAIN EP299, B:127, L:3.5435, AvgL:0.6289\n",
      "# TRAIN EP300, B:127, L:0.9761, AvgL:0.7321\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.18, b:0.00, w:107.00, 50p:2.00, 75p:6.00, 90p:11.00\n",
      "# TRAIN EP301, B:127, L:0.0437, AvgL:0.6933\n",
      "# TRAIN EP302, B:127, L:0.9961, AvgL:0.7453\n",
      "# TRAIN EP303, B:127, L:1.5522, AvgL:0.5700\n",
      "# TRAIN EP304, B:127, L:1.2569, AvgL:0.6347\n",
      "# TRAIN EP305, B:127, L:0.8966, AvgL:0.7061\n",
      "# TRAIN EP306, B:127, L:0.4298, AvgL:0.7724\n",
      "# TRAIN EP307, B:127, L:0.0000, AvgL:0.6347\n",
      "# TRAIN EP308, B:127, L:0.0015, AvgL:0.7145\n",
      "# TRAIN EP309, B:127, L:0.0000, AvgL:0.6995\n",
      "# TRAIN EP310, B:127, L:1.1815, AvgL:0.6781\n",
      "# TRAIN EP311, B:127, L:1.0088, AvgL:0.7424\n",
      "# TRAIN EP312, B:127, L:0.0000, AvgL:0.7741\n",
      "# TRAIN EP313, B:127, L:0.0191, AvgL:0.6649\n",
      "# TRAIN EP314, B:127, L:0.0294, AvgL:0.6796\n",
      "# TRAIN EP315, B:127, L:0.5342, AvgL:0.7359\n",
      "# TRAIN EP316, B:127, L:0.1603, AvgL:0.7010\n",
      "# TRAIN EP317, B:127, L:0.0000, AvgL:0.7568\n",
      "# TRAIN EP318, B:127, L:0.0014, AvgL:0.8033\n",
      "# TRAIN EP319, B:127, L:0.2792, AvgL:0.7370\n",
      "# TRAIN EP320, B:127, L:0.0223, AvgL:0.6508\n",
      "# TRAIN EP321, B:127, L:0.9641, AvgL:0.6503\n",
      "# TRAIN EP322, B:127, L:1.9060, AvgL:0.6189\n",
      "# TRAIN EP323, B:127, L:3.0634, AvgL:0.8653\n",
      "# TRAIN EP324, B:127, L:0.0124, AvgL:0.9228\n",
      "# TRAIN EP325, B:127, L:0.0000, AvgL:0.7631\n",
      "# TRAIN EP326, B:127, L:2.0316, AvgL:0.7168\n",
      "# TRAIN EP327, B:127, L:0.8950, AvgL:0.8539\n",
      "# TRAIN EP328, B:127, L:0.0000, AvgL:0.7485\n",
      "# TRAIN EP329, B:127, L:0.0000, AvgL:0.8056\n",
      "# TRAIN EP330, B:127, L:1.3361, AvgL:0.8145\n",
      "# TRAIN EP331, B:127, L:0.0000, AvgL:0.9093\n",
      "# TRAIN EP332, B:127, L:0.7410, AvgL:0.8518\n",
      "# TRAIN EP333, B:127, L:0.4176, AvgL:0.6896\n",
      "# TRAIN EP334, B:127, L:0.0011, AvgL:0.6639\n",
      "# TRAIN EP335, B:127, L:0.9250, AvgL:0.6937\n",
      "# TRAIN EP336, B:127, L:1.0155, AvgL:0.8300\n",
      "# TRAIN EP337, B:127, L:0.5451, AvgL:0.6935\n",
      "# TRAIN EP338, B:127, L:1.0979, AvgL:0.5886\n",
      "# TRAIN EP339, B:127, L:0.0000, AvgL:0.6881\n",
      "# TRAIN EP340, B:127, L:0.7339, AvgL:0.7005\n",
      "# TRAIN EP341, B:127, L:2.6046, AvgL:0.7878\n",
      "# TRAIN EP342, B:127, L:1.4118, AvgL:0.7852\n",
      "# TRAIN EP343, B:127, L:0.0000, AvgL:0.6924\n",
      "# TRAIN EP344, B:127, L:1.9756, AvgL:0.7399\n",
      "# TRAIN EP345, B:127, L:0.0000, AvgL:0.7734\n",
      "# TRAIN EP346, B:127, L:0.1121, AvgL:0.6835\n",
      "# TRAIN EP347, B:127, L:0.6545, AvgL:0.7287\n",
      "# TRAIN EP348, B:127, L:2.0082, AvgL:0.7577\n",
      "# TRAIN EP349, B:127, L:2.1646, AvgL:0.7067\n",
      "# TRAIN EP350, B:127, L:0.0000, AvgL:0.6201\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.17, b:0.00, w:58.00, 50p:3.00, 75p:5.00, 90p:11.00\n",
      "# TRAIN EP351, B:127, L:0.0000, AvgL:0.6617\n",
      "# TRAIN EP352, B:127, L:0.0411, AvgL:0.7662\n",
      "# TRAIN EP353, B:127, L:0.9596, AvgL:0.6893\n",
      "# TRAIN EP354, B:127, L:1.0188, AvgL:0.7326\n",
      "# TRAIN EP355, B:127, L:0.0000, AvgL:0.6853\n",
      "# TRAIN EP356, B:127, L:1.1554, AvgL:0.6327\n",
      "# TRAIN EP357, B:127, L:2.4777, AvgL:0.8544\n",
      "# TRAIN EP358, B:127, L:2.6638, AvgL:0.5639\n",
      "# TRAIN EP359, B:127, L:0.0030, AvgL:0.6275\n",
      "# TRAIN EP360, B:127, L:0.1964, AvgL:0.8293\n",
      "# TRAIN EP361, B:127, L:0.0499, AvgL:0.6773\n",
      "# TRAIN EP362, B:127, L:1.4052, AvgL:0.7286\n",
      "# TRAIN EP363, B:127, L:0.8228, AvgL:0.7777\n",
      "# TRAIN EP364, B:127, L:1.8108, AvgL:0.7740\n",
      "# TRAIN EP365, B:127, L:0.2846, AvgL:0.6996\n",
      "# TRAIN EP366, B:127, L:0.0000, AvgL:0.7509\n",
      "# TRAIN EP367, B:127, L:0.0063, AvgL:0.6703\n",
      "# TRAIN EP368, B:127, L:1.0585, AvgL:0.7950\n",
      "# TRAIN EP369, B:127, L:0.9009, AvgL:0.7135\n",
      "# TRAIN EP370, B:127, L:1.3186, AvgL:0.7519\n",
      "# TRAIN EP371, B:127, L:1.8990, AvgL:0.7538\n",
      "# TRAIN EP372, B:127, L:0.0122, AvgL:0.6461\n",
      "# TRAIN EP373, B:127, L:1.3685, AvgL:0.7521\n",
      "# TRAIN EP374, B:127, L:1.5466, AvgL:0.7607\n",
      "# TRAIN EP375, B:127, L:0.3078, AvgL:0.7239\n",
      "# TRAIN EP376, B:127, L:0.2240, AvgL:0.7089\n",
      "# TRAIN EP377, B:127, L:0.0000, AvgL:0.7254\n",
      "# TRAIN EP378, B:127, L:0.0000, AvgL:0.8082\n",
      "# TRAIN EP379, B:127, L:0.0000, AvgL:0.6646\n",
      "# TRAIN EP380, B:127, L:2.1835, AvgL:0.7566\n",
      "# TRAIN EP381, B:127, L:0.0014, AvgL:0.6538\n",
      "# TRAIN EP382, B:127, L:0.1933, AvgL:0.6982\n",
      "# TRAIN EP383, B:127, L:1.1439, AvgL:0.6878\n",
      "# TRAIN EP384, B:127, L:0.0000, AvgL:0.7377\n",
      "# TRAIN EP385, B:127, L:0.0000, AvgL:0.6436\n",
      "# TRAIN EP386, B:127, L:1.0971, AvgL:0.7017\n",
      "# TRAIN EP387, B:127, L:0.2654, AvgL:0.7777\n",
      "# TRAIN EP388, B:127, L:1.4790, AvgL:0.7760\n",
      "# TRAIN EP389, B:127, L:0.0000, AvgL:0.6623\n",
      "# TRAIN EP390, B:127, L:1.3501, AvgL:0.7372\n",
      "# TRAIN EP391, B:127, L:0.1526, AvgL:0.6561\n",
      "# TRAIN EP392, B:127, L:0.0000, AvgL:0.7676\n",
      "# TRAIN EP393, B:127, L:1.5413, AvgL:0.5891\n",
      "# TRAIN EP394, B:127, L:0.0000, AvgL:0.6081\n",
      "# TRAIN EP395, B:127, L:0.9007, AvgL:0.7411\n",
      "# TRAIN EP396, B:127, L:0.0000, AvgL:0.8128\n",
      "# TRAIN EP397, B:127, L:1.2540, AvgL:0.7486\n",
      "# TRAIN EP398, B:127, L:2.7338, AvgL:0.8189\n",
      "# TRAIN EP399, B:127, L:2.2161, AvgL:0.7067\n",
      "# TRAIN EP400, B:127, L:1.2727, AvgL:0.8497\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.34, b:0.00, w:40.00, 50p:2.00, 75p:6.00, 90p:11.00\n",
      "# TRAIN EP401, B:127, L:1.2485, AvgL:0.7524\n",
      "# TRAIN EP402, B:127, L:1.5708, AvgL:0.7575\n",
      "# TRAIN EP403, B:127, L:1.8906, AvgL:0.8171\n",
      "# TRAIN EP404, B:127, L:0.3917, AvgL:0.6776\n",
      "# TRAIN EP405, B:127, L:0.0804, AvgL:0.6244\n",
      "# TRAIN EP406, B:127, L:4.2155, AvgL:0.7044\n",
      "# TRAIN EP407, B:127, L:0.0343, AvgL:0.6967\n",
      "# TRAIN EP408, B:127, L:2.0557, AvgL:0.7161\n",
      "# TRAIN EP409, B:127, L:0.0262, AvgL:0.6498\n",
      "# TRAIN EP410, B:127, L:0.0000, AvgL:0.6767\n",
      "# TRAIN EP411, B:127, L:0.0320, AvgL:0.7223\n",
      "# TRAIN EP412, B:127, L:0.0000, AvgL:0.7384\n",
      "# TRAIN EP413, B:127, L:1.0179, AvgL:0.6210\n",
      "# TRAIN EP414, B:127, L:0.0000, AvgL:0.8392\n",
      "# TRAIN EP415, B:127, L:0.5915, AvgL:0.5643\n",
      "# TRAIN EP416, B:127, L:1.0382, AvgL:0.6987\n",
      "# TRAIN EP417, B:127, L:0.2165, AvgL:0.7098\n",
      "# TRAIN EP418, B:127, L:0.0081, AvgL:0.7250\n",
      "# TRAIN EP419, B:127, L:1.1382, AvgL:0.8527\n",
      "# TRAIN EP420, B:127, L:0.0305, AvgL:0.7223\n",
      "# TRAIN EP421, B:127, L:1.2713, AvgL:0.6837\n",
      "# TRAIN EP422, B:127, L:0.0000, AvgL:0.6719\n",
      "# TRAIN EP423, B:127, L:0.9079, AvgL:0.5893\n",
      "# TRAIN EP424, B:127, L:1.7006, AvgL:0.7342\n",
      "# TRAIN EP425, B:127, L:0.0000, AvgL:0.7023\n",
      "# TRAIN EP426, B:127, L:0.2400, AvgL:0.6272\n",
      "# TRAIN EP427, B:127, L:2.3146, AvgL:0.6413\n",
      "# TRAIN EP428, B:127, L:0.0123, AvgL:0.7081\n",
      "# TRAIN EP429, B:127, L:0.1315, AvgL:0.7451\n",
      "# TRAIN EP430, B:127, L:0.0000, AvgL:0.8900\n",
      "# TRAIN EP431, B:127, L:1.7693, AvgL:0.6614\n",
      "# TRAIN EP432, B:127, L:0.3048, AvgL:0.5544\n",
      "# TRAIN EP433, B:127, L:0.0000, AvgL:0.7547\n",
      "# TRAIN EP434, B:127, L:1.6937, AvgL:0.7742\n",
      "# TRAIN EP435, B:127, L:2.4373, AvgL:0.8119\n",
      "# TRAIN EP436, B:127, L:0.1410, AvgL:0.7473\n",
      "# TRAIN EP437, B:127, L:0.0000, AvgL:0.6940\n",
      "# TRAIN EP438, B:127, L:0.7581, AvgL:0.7910\n",
      "# TRAIN EP439, B:127, L:1.8451, AvgL:0.8103\n",
      "# TRAIN EP440, B:127, L:1.0663, AvgL:0.7554\n",
      "# TRAIN EP441, B:127, L:2.0063, AvgL:0.6512\n",
      "# TRAIN EP442, B:127, L:0.0000, AvgL:0.6699\n",
      "# TRAIN EP443, B:127, L:0.0338, AvgL:0.7047\n",
      "# TRAIN EP444, B:127, L:0.9252, AvgL:0.7589\n",
      "# TRAIN EP445, B:127, L:0.0000, AvgL:0.6755\n",
      "# TRAIN EP446, B:127, L:0.0000, AvgL:0.5423\n",
      "# TRAIN EP447, B:127, L:0.0000, AvgL:0.6100\n",
      "# TRAIN EP448, B:127, L:0.1297, AvgL:0.7765\n",
      "# TRAIN EP449, B:127, L:0.1840, AvgL:0.6824\n",
      "# TRAIN EP450, B:127, L:1.4191, AvgL:0.5605\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.18, b:0.00, w:85.00, 50p:2.00, 75p:6.00, 90p:10.00\n",
      "# TRAIN EP451, B:127, L:0.8728, AvgL:0.7333\n",
      "# TRAIN EP452, B:127, L:1.8878, AvgL:0.6926\n",
      "# TRAIN EP453, B:127, L:2.2172, AvgL:0.6806\n",
      "# TRAIN EP454, B:127, L:0.0000, AvgL:0.7456\n",
      "# TRAIN EP455, B:127, L:1.0222, AvgL:0.7696\n",
      "# TRAIN EP456, B:127, L:0.7227, AvgL:0.6802\n",
      "# TRAIN EP457, B:127, L:0.0000, AvgL:0.6033\n",
      "# TRAIN EP458, B:127, L:0.0090, AvgL:0.6896\n",
      "# TRAIN EP459, B:127, L:1.3978, AvgL:0.5250\n",
      "# TRAIN EP460, B:127, L:0.3231, AvgL:0.8707\n",
      "# TRAIN EP461, B:127, L:0.8563, AvgL:0.8549\n",
      "# TRAIN EP462, B:127, L:0.5479, AvgL:0.6310\n",
      "# TRAIN EP463, B:127, L:2.4420, AvgL:0.7785\n",
      "# TRAIN EP464, B:127, L:0.0542, AvgL:0.6281\n",
      "# TRAIN EP465, B:127, L:0.8179, AvgL:0.6249\n",
      "# TRAIN EP466, B:127, L:0.0000, AvgL:0.6550\n",
      "# TRAIN EP467, B:127, L:2.1740, AvgL:0.7896\n",
      "# TRAIN EP468, B:127, L:0.0000, AvgL:0.6524\n",
      "# TRAIN EP469, B:127, L:0.9485, AvgL:0.7943\n",
      "# TRAIN EP470, B:127, L:0.0000, AvgL:0.7622\n",
      "# TRAIN EP471, B:127, L:0.5174, AvgL:0.6848\n",
      "# TRAIN EP472, B:127, L:0.0000, AvgL:0.6762\n",
      "# TRAIN EP473, B:127, L:0.6726, AvgL:0.7084\n",
      "# TRAIN EP474, B:127, L:0.8734, AvgL:0.6948\n",
      "# TRAIN EP475, B:127, L:0.0807, AvgL:0.5988\n",
      "# TRAIN EP476, B:127, L:1.9614, AvgL:0.8038\n",
      "# TRAIN EP477, B:127, L:2.0621, AvgL:0.5944\n",
      "# TRAIN EP478, B:127, L:1.2382, AvgL:0.7859\n",
      "# TRAIN EP479, B:127, L:1.4628, AvgL:0.6997\n",
      "# TRAIN EP480, B:127, L:1.0850, AvgL:0.6566\n",
      "# TRAIN EP481, B:127, L:0.9170, AvgL:0.8913\n",
      "# TRAIN EP482, B:127, L:0.0299, AvgL:0.6310\n",
      "# TRAIN EP483, B:127, L:0.8923, AvgL:0.7275\n",
      "# TRAIN EP484, B:127, L:0.0000, AvgL:0.7030\n",
      "# TRAIN EP485, B:127, L:0.0366, AvgL:0.7128\n",
      "# TRAIN EP486, B:127, L:0.2431, AvgL:0.4873\n",
      "# TRAIN EP487, B:127, L:0.7454, AvgL:0.7379\n",
      "# TRAIN EP488, B:127, L:1.2632, AvgL:0.7643\n",
      "# TRAIN EP489, B:127, L:1.0878, AvgL:0.6324\n",
      "# TRAIN EP490, B:127, L:2.5193, AvgL:0.6740\n",
      "# TRAIN EP491, B:127, L:0.0000, AvgL:0.6997\n",
      "# TRAIN EP492, B:127, L:1.2628, AvgL:0.6603\n",
      "# TRAIN EP493, B:127, L:0.9889, AvgL:0.7341\n",
      "# TRAIN EP494, B:127, L:0.0363, AvgL:0.6145\n",
      "# TRAIN EP495, B:127, L:0.9995, AvgL:0.6551\n",
      "# TRAIN EP496, B:127, L:0.5473, AvgL:0.7209\n",
      "# TRAIN EP497, B:127, L:1.0940, AvgL:0.5959\n",
      "# TRAIN EP498, B:127, L:1.2548, AvgL:0.6309\n",
      "# TRAIN EP499, B:127, L:0.9736, AvgL:0.7937\n",
      "# TRAIN EP500, B:127, L:0.0006, AvgL:0.8304\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:3.70, b:0.00, w:21.00, 50p:2.00, 75p:6.00, 90p:9.00\n",
      "# TRAIN EP501, B:127, L:3.0538, AvgL:0.7124\n",
      "# TRAIN EP502, B:127, L:0.0000, AvgL:0.5308\n",
      "# TRAIN EP503, B:127, L:0.0410, AvgL:0.5649\n",
      "# TRAIN EP504, B:127, L:0.2010, AvgL:0.5944\n",
      "# TRAIN EP505, B:127, L:1.6004, AvgL:0.5966\n",
      "# TRAIN EP506, B:127, L:0.0000, AvgL:0.6758\n",
      "# TRAIN EP507, B:127, L:0.4089, AvgL:0.8802\n",
      "# TRAIN EP508, B:127, L:0.0003, AvgL:0.6588\n",
      "# TRAIN EP509, B:127, L:1.2469, AvgL:0.6381\n",
      "# TRAIN EP510, B:127, L:1.1176, AvgL:0.6345\n",
      "# TRAIN EP511, B:127, L:0.0000, AvgL:0.7303\n",
      "# TRAIN EP512, B:127, L:0.0000, AvgL:0.7074\n",
      "# TRAIN EP513, B:127, L:0.7186, AvgL:0.6304\n",
      "# TRAIN EP514, B:127, L:0.2215, AvgL:0.7324\n",
      "# TRAIN EP515, B:127, L:0.0057, AvgL:0.6576\n",
      "# TRAIN EP516, B:127, L:0.8849, AvgL:0.6362\n",
      "# TRAIN EP517, B:127, L:1.8029, AvgL:0.5854\n",
      "# TRAIN EP518, B:127, L:1.7096, AvgL:0.6566\n",
      "# TRAIN EP519, B:127, L:0.9823, AvgL:0.7836\n",
      "# TRAIN EP520, B:127, L:0.3498, AvgL:0.7258\n",
      "# TRAIN EP521, B:127, L:0.9283, AvgL:0.5933\n",
      "# TRAIN EP522, B:127, L:0.0000, AvgL:0.6849\n",
      "# TRAIN EP523, B:127, L:0.0000, AvgL:0.5855\n",
      "# TRAIN EP524, B:127, L:0.0000, AvgL:0.5792\n",
      "# TRAIN EP525, B:127, L:2.1325, AvgL:0.8347\n",
      "# TRAIN EP526, B:127, L:0.0807, AvgL:0.5809\n",
      "# TRAIN EP527, B:127, L:0.3132, AvgL:0.5955\n",
      "# TRAIN EP528, B:127, L:0.8092, AvgL:0.6873\n",
      "# TRAIN EP529, B:127, L:1.0856, AvgL:0.6140\n",
      "# TRAIN EP530, B:127, L:0.0007, AvgL:0.7039\n",
      "# TRAIN EP531, B:127, L:0.6622, AvgL:0.7154\n",
      "# TRAIN EP532, B:127, L:1.1898, AvgL:0.5794\n",
      "# TRAIN EP533, B:127, L:1.3978, AvgL:0.6686\n",
      "# TRAIN EP534, B:127, L:0.0000, AvgL:0.6312\n",
      "# TRAIN EP535, B:127, L:0.0000, AvgL:0.6742\n",
      "# TRAIN EP536, B:127, L:1.4851, AvgL:0.6453\n",
      "# TRAIN EP537, B:127, L:0.0374, AvgL:0.6981\n",
      "# TRAIN EP538, B:127, L:0.2233, AvgL:0.7434\n",
      "# TRAIN EP539, B:127, L:0.3227, AvgL:0.7164\n",
      "# TRAIN EP540, B:127, L:0.0213, AvgL:0.5299\n",
      "# TRAIN EP541, B:127, L:0.0000, AvgL:0.5763\n",
      "# TRAIN EP542, B:127, L:0.0020, AvgL:0.6673\n",
      "# TRAIN EP543, B:127, L:0.0000, AvgL:0.6739\n",
      "# TRAIN EP544, B:127, L:0.0414, AvgL:0.6524\n",
      "# TRAIN EP545, B:127, L:0.0659, AvgL:0.5495\n",
      "# TRAIN EP546, B:127, L:0.9324, AvgL:0.6663\n",
      "# TRAIN EP547, B:127, L:0.0343, AvgL:0.6191\n",
      "# TRAIN EP548, B:127, L:0.0764, AvgL:0.6645\n",
      "# TRAIN EP549, B:127, L:0.0035, AvgL:0.5596\n",
      "# TRAIN EP550, B:127, L:0.3434, AvgL:0.6356\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:3.89, b:0.00, w:87.00, 50p:2.00, 75p:5.00, 90p:9.90\n",
      "# TRAIN EP551, B:127, L:1.0107, AvgL:0.6661\n",
      "# TRAIN EP552, B:127, L:0.8869, AvgL:0.6606\n",
      "# TRAIN EP553, B:127, L:0.0272, AvgL:0.6046\n",
      "# TRAIN EP554, B:127, L:0.0618, AvgL:0.6798\n",
      "# TRAIN EP555, B:127, L:0.8960, AvgL:0.8019\n",
      "# TRAIN EP556, B:127, L:0.9351, AvgL:0.6296\n",
      "# TRAIN EP557, B:127, L:0.0000, AvgL:0.7312\n",
      "# TRAIN EP558, B:127, L:0.0000, AvgL:0.6045\n",
      "# TRAIN EP559, B:127, L:0.0000, AvgL:0.7002\n",
      "# TRAIN EP560, B:37, L:0.0000, AvgL:0.8171"
     ]
    }
   ],
   "source": [
    "MTETrainer(1000000, mte, ld_mg_train, ld_mg_test, optimizer, m_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
