{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MetaNeo Ranker\n",
    "- tell which pair of tables are functionally/logically closer on execution chain\n",
    "- Stage: Cambrian\n",
    "- Version: Yorgia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tyrell.spec as S\n",
    "from tyrell.decider import Example\n",
    "\n",
    "# Morpheus Version\n",
    "from MorpheusInterpreter import *\n",
    "from ProgramSpace import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dataset for training: load random positive/negative examples every time\n",
    "'''\n",
    "class RankerDataset(Dataset):\n",
    "    def __init__(self, p_config=None, p_dataset=None, p_interpreter=None, p_spec=None):\n",
    "        self.n_sample = None # should manually assign\n",
    "        self.interpreter = p_interpreter\n",
    "        self.spec = p_spec\n",
    "        self.dataset = p_dataset\n",
    "        self.config = p_config\n",
    "        \n",
    "        # record all possible programs\n",
    "        self.progs = [\n",
    "            self.dataset[dkey][0][0]\n",
    "            for dkey in self.dataset.keys()\n",
    "        ]\n",
    "        \n",
    "        self.n_exp = len(self.progs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_sample\n",
    "    \n",
    "    '''\n",
    "    return a single triangle sample\n",
    "    (input, output1, output2, random)\n",
    "    A->B, A->C, D\n",
    "    pos: (A,B), (A,C)\n",
    "    neg: (B,A), (C,A)\n",
    "    neg: (B,C), (C,B)\n",
    "    neg: (A,D), (B,D), (C,D)\n",
    "    neg: (D,A), (D,B), (D,C)\n",
    "    '''\n",
    "    def get_triangle(self):\n",
    "        # sample (A,B)\n",
    "        while True:\n",
    "            d_progAB = random.choice(self.progs)\n",
    "            d_inputA = self.interpreter.random_table()\n",
    "            try:\n",
    "                d_evalB = self.interpreter.eval(\n",
    "                    d_progAB,\n",
    "                    [d_inputA],\n",
    "                )\n",
    "            except Exception:\n",
    "                continue\n",
    "            d_exampleAB = Example(\n",
    "                input=[d_inputA],\n",
    "                output=d_evalB,\n",
    "            )\n",
    "            d_psAB = ProgramSpace(\n",
    "                self.spec, self.interpreter,\n",
    "                d_exampleAB.input, d_exampleAB.output,\n",
    "            )\n",
    "            d_psAB.init_by_prog(d_progAB)\n",
    "            d_checkAB = self.interpreter.sanity_check(d_psAB)\n",
    "            if d_checkAB[0]:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # sample (A,C)\n",
    "        while True:\n",
    "            d_progAC = random.choice(self.progs)\n",
    "            if d_progAC.name==d_progAB.name:\n",
    "                continue\n",
    "            try:\n",
    "                d_evalC = self.interpreter.eval(\n",
    "                    d_progAC,\n",
    "                    [d_inputA],\n",
    "                )\n",
    "            except Exception:\n",
    "                continue\n",
    "            d_exampleAC = Example(\n",
    "                input=[d_inputA],\n",
    "                output=d_evalC,\n",
    "            )\n",
    "            d_psAC = ProgramSpace(\n",
    "                self.spec, self.interpreter,\n",
    "                d_exampleAC.input, d_exampleAC.output,\n",
    "            )\n",
    "            d_psAC.init_by_prog(d_progAC)\n",
    "            d_checkAC = self.interpreter.sanity_check(d_psAC)\n",
    "            if d_checkAC[0]:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # then randomly generate a table D\n",
    "        d_randD = self.interpreter.random_table()\n",
    "        \n",
    "        # all (map_r, map_c)\n",
    "        map_inputA = numpy.array([self.interpreter.camb_get_yorgia(d_inputA,modn=self.config[\"vocab_size\"])])\n",
    "        map_evalB = numpy.array([self.interpreter.camb_get_yorgia(d_evalB,modn=self.config[\"vocab_size\"])])\n",
    "        map_evalC = numpy.array([self.interpreter.camb_get_yorgia(d_evalC,modn=self.config[\"vocab_size\"])])\n",
    "        map_randD = numpy.array([self.interpreter.camb_get_yorgia(d_randD,modn=self.config[\"vocab_size\"])])\n",
    "        \n",
    "        return (map_inputA, map_evalB, map_evalC, map_randD)\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    should always use batch_size=1 so as to ensure the ratio of negative examples\n",
    "    '''\n",
    "    def __getitem__(self, p_ind):\n",
    "        return self.get_triangle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ranker(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(Ranker, self).__init__()\n",
    "        self.config = p_config\n",
    "        self.value_embedding = nn.Embedding(\n",
    "            self.config[\"vocab_size\"],\n",
    "            self.config[\"embd_dim\"],\n",
    "        )\n",
    "        self.fc0 = nn.Linear(\n",
    "            self.config[\"embd_dim\"] * self.config[\"max_length\"] * 2,\n",
    "            4096,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(\n",
    "            4096,\n",
    "            2048,\n",
    "        )\n",
    "        self.fc2 = nn.Linear(\n",
    "            2048,\n",
    "            128\n",
    "        )\n",
    "        self.fc3 = nn.Linear(\n",
    "            128,\n",
    "            2\n",
    "        )\n",
    "        \n",
    "    def forward(self, pA, pB, pC, pD):\n",
    "        # pA/pB/pC/pD: (B=1, seq)\n",
    "        B = pA.shape[0]\n",
    "        \n",
    "        # (B, seq, embd_dim)\n",
    "        vA = self.value_embedding(pA).view(B, self.config[\"max_length\"]*self.config[\"embd_dim\"])\n",
    "        vB = self.value_embedding(pB).view(B, self.config[\"max_length\"]*self.config[\"embd_dim\"])\n",
    "        vC = self.value_embedding(pC).view(B, self.config[\"max_length\"]*self.config[\"embd_dim\"])\n",
    "        vD = self.value_embedding(pD).view(B, self.config[\"max_length\"]*self.config[\"embd_dim\"])\n",
    "        \n",
    "        # (B=2, embd_dim * 2)\n",
    "        vPOS = torch.cat(\n",
    "            [\n",
    "                torch.cat([vA,vB],dim=1),\n",
    "                torch.cat([vA,vC],dim=1),\n",
    "            ],dim=0\n",
    "        )\n",
    "        \n",
    "        # (B=10, embd_dim * 2)\n",
    "        vNEG = torch.cat(\n",
    "            [\n",
    "                torch.cat([vB,vA],dim=1),\n",
    "                torch.cat([vB,vC],dim=1),\n",
    "                torch.cat([vC,vA],dim=1),\n",
    "                torch.cat([vC,vB],dim=1),\n",
    "            ],dim=0\n",
    "        )\n",
    "        \n",
    "        # (B=4*3=12, embd_dim * 2)\n",
    "        vII = torch.cat([vPOS,vNEG],dim=0)\n",
    "        \n",
    "        v00 = F.relu(self.fc0(vII))\n",
    "        v01 = F.relu(self.fc1(v00))\n",
    "        v02 = F.relu(self.fc2(v01))\n",
    "        v03 = self.fc3(v02)\n",
    "        # == Notice: don't do any activation at the last layer ==\n",
    "        # (B=4*3=12, 2)\n",
    "        \n",
    "        return v03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RankerTester(p_config, p_model, pld_test, p_lossfn):\n",
    "    test_loss_list = []\n",
    "    test_pPOS_list = []\n",
    "    test_pNEG_list = []\n",
    "    test_aPOS_list = []\n",
    "    test_aNEG_list = []\n",
    "    test_sPOS_list = []\n",
    "    test_sNEG_list = []\n",
    "    for batch_idx, (dA, dB, dC, dD) in enumerate(pld_test):\n",
    "        p_model.eval()\n",
    "        if use_cuda:\n",
    "            tdA = Variable(dA).cuda() # (B=1, map_r, map_c)\n",
    "            tdB = Variable(dB).cuda() # (B=1, map_r, map_c)\n",
    "            tdC = Variable(dC).cuda() # (B=1, map_r, map_c)\n",
    "            tdD = Variable(dD).cuda() # (B=1, map_r, map_c)\n",
    "            td_label = Variable(torch.tensor(\n",
    "                [1 for _ in range(2)]+\\\n",
    "                [0 for _ in range(4)]\n",
    "            )).cuda()\n",
    "        else:\n",
    "            tdA = Variable(dA) # (B=1, map_r, map_c)\n",
    "            tdB = Variable(dB) # (B=1, map_r, map_c)\n",
    "            tdC = Variable(dC) # (B=1, map_r, map_c)\n",
    "            tdD = Variable(dD) # (B=1, map_r, map_c)\n",
    "            td_label = Variable(torch.tensor(\n",
    "                [1 for _ in range(2)]+\\\n",
    "                [0 for _ in range(4)]\n",
    "            ))\n",
    "            \n",
    "        d_output = p_model(tdA, tdB, tdC, tdD) # (B, 2)\n",
    "        d_loss = p_lossfn(\n",
    "            F.log_softmax(d_output, dim=1),\n",
    "            td_label,\n",
    "        )\n",
    "        \n",
    "        test_loss_list.append(d_loss.cpu().data.numpy())\n",
    "        test_pPOS_list += F.softmax(d_output,dim=1)[:2,1].cpu().data.tolist()\n",
    "        test_pNEG_list += F.softmax(d_output,dim=1)[2:,0].cpu().data.tolist()\n",
    "        test_aPOS_list += (torch.argmax(d_output,dim=1)[:2]==td_label[:2]).cpu().data.tolist()\n",
    "        test_aNEG_list += (torch.argmax(d_output,dim=1)[2:]==td_label[2:]).cpu().data.tolist()\n",
    "        test_sPOS_list += F.softmax(d_output,dim=1)[:2,1].cpu().data.tolist()\n",
    "        test_sNEG_list += F.softmax(d_output,dim=1)[2:,1].cpu().data.tolist()\n",
    "        \n",
    "    print(\"# Test avg.loss:{:.2f}, avg.prob.:{:.2f}/{:.2f}, avg.acc.:{:.2f}/{:.2f}, avg.score:{:.2f}/{:.2f}\".format(\n",
    "        sum(test_loss_list)/len(test_loss_list),\n",
    "        sum(test_pNEG_list)/len(test_pNEG_list),\n",
    "        sum(test_pPOS_list)/len(test_pPOS_list),\n",
    "        sum(test_aNEG_list)/len(test_aNEG_list),\n",
    "        sum(test_aPOS_list)/len(test_aPOS_list),\n",
    "        sum(test_sNEG_list)/len(test_sNEG_list), # score is the similarity score\n",
    "        sum(test_sPOS_list)/len(test_sPOS_list),\n",
    "    ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RankerTrainer(p_config, p_model, pld_train, pld_test, p_optim, p_lossfn):\n",
    "    RankerTester(p_config, p_model, pld_test, p_lossfn)\n",
    "    for d_ep in range(p_config[\"ranker\"][\"n_ep\"]):\n",
    "        epoch_loss_list = []\n",
    "        for batch_idx, (dA, dB, dC, dD) in enumerate(pld_train):\n",
    "            p_model.train()\n",
    "            \n",
    "            if use_cuda:\n",
    "                tdA = Variable(dA).cuda() # (B=1, map_r, map_c)\n",
    "                tdB = Variable(dB).cuda() # (B=1, map_r, map_c)\n",
    "                tdC = Variable(dC).cuda() # (B=1, map_r, map_c)\n",
    "                tdD = Variable(dD).cuda() # (B=1, map_r, map_c)\n",
    "                td_label = Variable(torch.tensor(\n",
    "                    [1 for _ in range(2)]+\\\n",
    "                    [0 for _ in range(4)]\n",
    "                )).cuda()\n",
    "            else:\n",
    "                tdA = Variable(dA) # (B=1, map_r, map_c)\n",
    "                tdB = Variable(dB) # (B=1, map_r, map_c)\n",
    "                tdC = Variable(dC) # (B=1, map_r, map_c)\n",
    "                tdD = Variable(dD) # (B=1, map_r, map_c)\n",
    "                td_label = Variable(torch.tensor(\n",
    "                    [1 for _ in range(2)]+\\\n",
    "                    [0 for _ in range(4)]\n",
    "                ))\n",
    "                \n",
    "            # (B=12, 2)\n",
    "            d_output = p_model(tdA, tdB, tdC, tdD)\n",
    "            p_optim.zero_grad()\n",
    "            d_loss = p_lossfn(\n",
    "                F.log_softmax(d_output, dim=1),\n",
    "                td_label,\n",
    "            )\n",
    "            epoch_loss_list.append(d_loss.cpu().data.numpy())\n",
    "            d_loss.backward()\n",
    "            p_optim.step()\n",
    "            \n",
    "            print(\"\\r# Training EP:{}, B:{}, ep.loss:{:.2f}\".format(\n",
    "                d_ep, batch_idx, sum(epoch_loss_list),\n",
    "            ),end=\"\")\n",
    "        \n",
    "        # end of epoch print a new line\n",
    "        print()\n",
    "        RankerTester(p_config, p_model, pld_test, p_lossfn)\n",
    "        \n",
    "        # save the model\n",
    "        if d_ep%10==0:\n",
    "            torch.save(p_model.state_dict(), \"./saved_models/0730Ranker_Zeke_ep{}.pt\".format(d_ep))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_interpreter = MorpheusInterpreter()\n",
    "m_spec = S.parse_file('./example/camb3.tyrell')\n",
    "\n",
    "m_config = {\n",
    "    \"vocab_size\": 2048,\n",
    "    \"embd_dim\": 16,\n",
    "    \"max_length\": m_interpreter.CAMB_NROW + m_interpreter.CAMB_NCOL,\n",
    "    \"ranker\":{\n",
    "        \"data_path\": \"./0716MDsize1.pkl\",\n",
    "        \"train_size\": 1000, # how many samples in every epoch\n",
    "        \"test_size\": 100,\n",
    "        \"n_ep\": 1000000,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# load the data and dataset\n",
    "with open(m_config[\"ranker\"][\"data_path\"],\"rb\") as f:\n",
    "    m_data = pickle.load(f)\n",
    "    \n",
    "dt_train = RankerDataset(\n",
    "    p_config=m_config, \n",
    "    p_dataset=m_data, \n",
    "    p_interpreter=m_interpreter,\n",
    "    p_spec=m_spec,\n",
    ")\n",
    "dt_train.n_sample = m_config[\"ranker\"][\"train_size\"]\n",
    "ld_train = DataLoader(dataset=dt_train, batch_size=1, shuffle=True)\n",
    "\n",
    "dt_test = RankerDataset(\n",
    "    p_config=m_config, \n",
    "    p_dataset=m_data, \n",
    "    p_interpreter=m_interpreter,\n",
    "    p_spec=m_spec,\n",
    ")\n",
    "dt_test.n_sample = m_config[\"ranker\"][\"test_size\"]\n",
    "ld_test = DataLoader(dataset=dt_test, batch_size=1, shuffle=True)\n",
    "\n",
    "m_ranker = Ranker(p_config=m_config)\n",
    "if use_cuda:\n",
    "    m_ranker = m_ranker.cuda()\n",
    "optimizer = torch.optim.Adam(list(m_ranker.parameters()))\n",
    "lossfn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Test avg.loss:0.69, avg.prob.:0.51/0.50, avg.acc.:0.77/0.30, avg.score:0.49/0.50\n",
      "# Training EP:0, B:999, ep.loss:508.54\n",
      "# Test avg.loss:0.48, avg.prob.:0.68/0.72, avg.acc.:0.61/0.93, avg.score:0.32/0.72\n",
      "# Training EP:1, B:999, ep.loss:446.19\n",
      "# Test avg.loss:0.39, avg.prob.:0.80/0.56, avg.acc.:0.83/0.71, avg.score:0.20/0.56\n",
      "# Training EP:2, B:999, ep.loss:420.04\n",
      "# Test avg.loss:0.40, avg.prob.:0.82/0.50, avg.acc.:0.82/0.70, avg.score:0.18/0.50\n",
      "# Training EP:3, B:999, ep.loss:404.09\n",
      "# Test avg.loss:0.39, avg.prob.:0.79/0.62, avg.acc.:0.83/0.81, avg.score:0.21/0.62\n",
      "# Training EP:4, B:999, ep.loss:380.59\n",
      "# Test avg.loss:0.37, avg.prob.:0.85/0.56, avg.acc.:0.82/0.79, avg.score:0.15/0.56\n",
      "# Training EP:5, B:999, ep.loss:363.18\n",
      "# Test avg.loss:0.46, avg.prob.:0.81/0.66, avg.acc.:0.81/0.71, avg.score:0.19/0.66\n",
      "# Training EP:6, B:999, ep.loss:367.22\n",
      "# Test avg.loss:0.35, avg.prob.:0.82/0.60, avg.acc.:0.81/0.86, avg.score:0.18/0.60\n",
      "# Training EP:7, B:999, ep.loss:345.99\n",
      "# Test avg.loss:0.33, avg.prob.:0.82/0.73, avg.acc.:0.81/0.90, avg.score:0.18/0.73\n",
      "# Training EP:8, B:999, ep.loss:345.52\n",
      "# Test avg.loss:0.40, avg.prob.:0.80/0.53, avg.acc.:0.80/0.84, avg.score:0.20/0.53\n",
      "# Training EP:9, B:999, ep.loss:356.57\n",
      "# Test avg.loss:0.38, avg.prob.:0.83/0.57, avg.acc.:0.90/0.55, avg.score:0.17/0.57\n",
      "# Training EP:10, B:999, ep.loss:350.46\n",
      "# Test avg.loss:0.38, avg.prob.:0.85/0.57, avg.acc.:0.83/0.71, avg.score:0.15/0.57\n",
      "# Training EP:11, B:999, ep.loss:319.12\n",
      "# Test avg.loss:0.33, avg.prob.:0.82/0.72, avg.acc.:0.82/0.88, avg.score:0.18/0.72\n",
      "# Training EP:12, B:999, ep.loss:338.10\n",
      "# Test avg.loss:0.35, avg.prob.:0.87/0.54, avg.acc.:0.87/0.73, avg.score:0.13/0.54\n",
      "# Training EP:13, B:999, ep.loss:299.90\n",
      "# Test avg.loss:0.31, avg.prob.:0.88/0.65, avg.acc.:0.94/0.66, avg.score:0.12/0.65\n",
      "# Training EP:14, B:999, ep.loss:324.86\n",
      "# Test avg.loss:0.30, avg.prob.:0.83/0.69, avg.acc.:0.81/0.88, avg.score:0.17/0.69\n",
      "# Training EP:15, B:999, ep.loss:318.78\n",
      "# Test avg.loss:0.28, avg.prob.:0.85/0.71, avg.acc.:0.89/0.81, avg.score:0.15/0.71\n",
      "# Training EP:16, B:999, ep.loss:327.61\n",
      "# Test avg.loss:0.27, avg.prob.:0.88/0.70, avg.acc.:0.94/0.77, avg.score:0.12/0.70\n",
      "# Training EP:17, B:999, ep.loss:321.59\n",
      "# Test avg.loss:0.29, avg.prob.:0.84/0.71, avg.acc.:0.90/0.79, avg.score:0.16/0.71\n",
      "# Training EP:18, B:999, ep.loss:322.70\n",
      "# Test avg.loss:0.32, avg.prob.:0.83/0.64, avg.acc.:0.82/0.87, avg.score:0.17/0.64\n",
      "# Training EP:19, B:999, ep.loss:314.93\n",
      "# Test avg.loss:0.29, avg.prob.:0.86/0.68, avg.acc.:0.84/0.88, avg.score:0.14/0.68\n",
      "# Training EP:20, B:999, ep.loss:315.17\n",
      "# Test avg.loss:0.29, avg.prob.:0.85/0.71, avg.acc.:0.91/0.77, avg.score:0.15/0.71\n",
      "# Training EP:21, B:999, ep.loss:312.40\n",
      "# Test avg.loss:0.27, avg.prob.:0.86/0.76, avg.acc.:0.88/0.82, avg.score:0.14/0.76\n",
      "# Training EP:22, B:999, ep.loss:312.41\n",
      "# Test avg.loss:0.28, avg.prob.:0.85/0.69, avg.acc.:0.90/0.81, avg.score:0.15/0.69\n",
      "# Training EP:23, B:999, ep.loss:293.52\n",
      "# Test avg.loss:0.30, avg.prob.:0.82/0.77, avg.acc.:0.81/0.88, avg.score:0.18/0.77\n",
      "# Training EP:24, B:999, ep.loss:296.80\n",
      "# Test avg.loss:0.25, avg.prob.:0.89/0.71, avg.acc.:0.95/0.69, avg.score:0.11/0.71\n",
      "# Training EP:25, B:999, ep.loss:304.99\n",
      "# Test avg.loss:0.29, avg.prob.:0.87/0.71, avg.acc.:0.91/0.71, avg.score:0.13/0.71\n",
      "# Training EP:26, B:999, ep.loss:287.47\n",
      "# Test avg.loss:0.29, avg.prob.:0.86/0.67, avg.acc.:0.94/0.68, avg.score:0.14/0.67\n",
      "# Training EP:27, B:999, ep.loss:310.96\n",
      "# Test avg.loss:0.30, avg.prob.:0.87/0.70, avg.acc.:0.92/0.71, avg.score:0.13/0.70\n",
      "# Training EP:28, B:999, ep.loss:298.63\n",
      "# Test avg.loss:0.25, avg.prob.:0.86/0.72, avg.acc.:0.95/0.74, avg.score:0.14/0.72\n",
      "# Training EP:29, B:999, ep.loss:286.52\n",
      "# Test avg.loss:0.29, avg.prob.:0.84/0.73, avg.acc.:0.91/0.74, avg.score:0.16/0.73\n",
      "# Training EP:30, B:999, ep.loss:295.47\n",
      "# Test avg.loss:0.29, avg.prob.:0.84/0.73, avg.acc.:0.87/0.79, avg.score:0.16/0.73\n",
      "# Training EP:31, B:999, ep.loss:293.81\n",
      "# Test avg.loss:0.28, avg.prob.:0.86/0.77, avg.acc.:0.92/0.81, avg.score:0.14/0.77\n",
      "# Training EP:32, B:999, ep.loss:294.47\n",
      "# Test avg.loss:0.30, avg.prob.:0.86/0.73, avg.acc.:0.92/0.71, avg.score:0.14/0.73\n",
      "# Training EP:33, B:999, ep.loss:285.01\n",
      "# Test avg.loss:0.32, avg.prob.:0.86/0.71, avg.acc.:0.93/0.66, avg.score:0.14/0.71\n",
      "# Training EP:34, B:999, ep.loss:279.04\n",
      "# Test avg.loss:0.28, avg.prob.:0.87/0.66, avg.acc.:0.94/0.66, avg.score:0.13/0.66\n",
      "# Training EP:35, B:999, ep.loss:274.88\n",
      "# Test avg.loss:0.31, avg.prob.:0.85/0.70, avg.acc.:0.91/0.68, avg.score:0.15/0.70\n",
      "# Training EP:36, B:999, ep.loss:279.70\n",
      "# Test avg.loss:0.23, avg.prob.:0.88/0.77, avg.acc.:0.95/0.74, avg.score:0.12/0.77\n",
      "# Training EP:37, B:999, ep.loss:289.28\n",
      "# Test avg.loss:0.26, avg.prob.:0.88/0.73, avg.acc.:0.93/0.74, avg.score:0.12/0.73\n",
      "# Training EP:38, B:999, ep.loss:285.13\n",
      "# Test avg.loss:0.19, avg.prob.:0.90/0.76, avg.acc.:0.98/0.77, avg.score:0.10/0.76\n",
      "# Training EP:39, B:999, ep.loss:282.72\n",
      "# Test avg.loss:0.22, avg.prob.:0.91/0.73, avg.acc.:0.96/0.77, avg.score:0.09/0.73\n",
      "# Training EP:40, B:999, ep.loss:267.09\n",
      "# Test avg.loss:0.31, avg.prob.:0.80/0.81, avg.acc.:0.80/0.95, avg.score:0.20/0.81\n",
      "# Training EP:41, B:999, ep.loss:262.19\n",
      "# Test avg.loss:0.22, avg.prob.:0.87/0.79, avg.acc.:0.88/0.92, avg.score:0.13/0.79\n",
      "# Training EP:42, B:999, ep.loss:273.82\n",
      "# Test avg.loss:0.37, avg.prob.:0.89/0.63, avg.acc.:0.93/0.59, avg.score:0.11/0.63\n",
      "# Training EP:43, B:999, ep.loss:262.38\n",
      "# Test avg.loss:0.31, avg.prob.:0.87/0.74, avg.acc.:0.85/0.88, avg.score:0.13/0.74\n",
      "# Training EP:44, B:999, ep.loss:256.35\n",
      "# Test avg.loss:0.29, avg.prob.:0.91/0.69, avg.acc.:0.93/0.76, avg.score:0.09/0.69\n",
      "# Training EP:45, B:999, ep.loss:287.00\n",
      "# Test avg.loss:0.30, avg.prob.:0.87/0.69, avg.acc.:0.88/0.72, avg.score:0.13/0.69\n",
      "# Training EP:46, B:999, ep.loss:262.23\n",
      "# Test avg.loss:0.22, avg.prob.:0.90/0.81, avg.acc.:0.94/0.84, avg.score:0.10/0.81\n",
      "# Training EP:47, B:999, ep.loss:262.80\n",
      "# Test avg.loss:0.32, avg.prob.:0.79/0.77, avg.acc.:0.76/0.92, avg.score:0.21/0.77\n",
      "# Training EP:48, B:999, ep.loss:270.32\n",
      "# Test avg.loss:0.25, avg.prob.:0.84/0.87, avg.acc.:0.81/0.95, avg.score:0.16/0.87\n",
      "# Training EP:49, B:999, ep.loss:268.66\n",
      "# Test avg.loss:0.27, avg.prob.:0.90/0.67, avg.acc.:0.92/0.76, avg.score:0.10/0.67\n",
      "# Training EP:50, B:999, ep.loss:253.90\n",
      "# Test avg.loss:0.26, avg.prob.:0.85/0.78, avg.acc.:0.88/0.81, avg.score:0.15/0.78\n",
      "# Training EP:51, B:999, ep.loss:265.35\n",
      "# Test avg.loss:0.25, avg.prob.:0.88/0.76, avg.acc.:0.90/0.85, avg.score:0.12/0.76\n",
      "# Training EP:52, B:999, ep.loss:258.66\n",
      "# Test avg.loss:0.25, avg.prob.:0.88/0.80, avg.acc.:0.89/0.82, avg.score:0.12/0.80\n",
      "# Training EP:53, B:999, ep.loss:250.80\n",
      "# Test avg.loss:0.25, avg.prob.:0.87/0.78, avg.acc.:0.87/0.87, avg.score:0.13/0.78\n",
      "# Training EP:54, B:999, ep.loss:259.12\n",
      "# Test avg.loss:0.32, avg.prob.:0.86/0.69, avg.acc.:0.86/0.79, avg.score:0.14/0.69\n",
      "# Training EP:55, B:999, ep.loss:271.57\n",
      "# Test avg.loss:0.29, avg.prob.:0.84/0.74, avg.acc.:0.83/0.89, avg.score:0.16/0.74\n",
      "# Training EP:56, B:999, ep.loss:251.66\n",
      "# Test avg.loss:0.24, avg.prob.:0.88/0.77, avg.acc.:0.89/0.86, avg.score:0.12/0.77\n",
      "# Training EP:57, B:999, ep.loss:255.22\n",
      "# Test avg.loss:0.23, avg.prob.:0.90/0.74, avg.acc.:0.91/0.84, avg.score:0.10/0.74\n",
      "# Training EP:58, B:999, ep.loss:252.47\n",
      "# Test avg.loss:0.28, avg.prob.:0.88/0.72, avg.acc.:0.90/0.76, avg.score:0.12/0.72\n",
      "# Training EP:59, B:999, ep.loss:254.67\n",
      "# Test avg.loss:0.27, avg.prob.:0.84/0.73, avg.acc.:0.85/0.88, avg.score:0.16/0.73\n",
      "# Training EP:60, B:999, ep.loss:269.76\n",
      "# Test avg.loss:0.23, avg.prob.:0.88/0.79, avg.acc.:0.89/0.83, avg.score:0.12/0.79\n",
      "# Training EP:61, B:999, ep.loss:251.11\n",
      "# Test avg.loss:0.23, avg.prob.:0.93/0.72, avg.acc.:0.94/0.79, avg.score:0.07/0.72\n",
      "# Training EP:62, B:999, ep.loss:250.69\n",
      "# Test avg.loss:0.26, avg.prob.:0.88/0.78, avg.acc.:0.83/0.90, avg.score:0.12/0.78\n",
      "# Training EP:63, B:999, ep.loss:257.64\n",
      "# Test avg.loss:0.28, avg.prob.:0.87/0.67, avg.acc.:0.98/0.56, avg.score:0.13/0.67\n",
      "# Training EP:64, B:999, ep.loss:246.21\n",
      "# Test avg.loss:0.26, avg.prob.:0.85/0.76, avg.acc.:0.93/0.73, avg.score:0.15/0.76\n",
      "# Training EP:65, B:999, ep.loss:250.08\n",
      "# Test avg.loss:0.25, avg.prob.:0.88/0.74, avg.acc.:0.88/0.86, avg.score:0.12/0.74\n",
      "# Training EP:66, B:999, ep.loss:248.25\n",
      "# Test avg.loss:0.29, avg.prob.:0.85/0.73, avg.acc.:0.83/0.87, avg.score:0.15/0.73\n",
      "# Training EP:67, B:999, ep.loss:252.18\n",
      "# Test avg.loss:0.28, avg.prob.:0.86/0.70, avg.acc.:0.83/0.89, avg.score:0.14/0.70\n",
      "# Training EP:68, B:999, ep.loss:260.74\n",
      "# Test avg.loss:0.25, avg.prob.:0.91/0.69, avg.acc.:0.92/0.77, avg.score:0.09/0.69\n",
      "# Training EP:69, B:999, ep.loss:253.13\n",
      "# Test avg.loss:0.25, avg.prob.:0.84/0.84, avg.acc.:0.82/0.96, avg.score:0.16/0.84\n",
      "# Training EP:70, B:999, ep.loss:262.56\n",
      "# Test avg.loss:0.28, avg.prob.:0.89/0.68, avg.acc.:0.86/0.80, avg.score:0.11/0.68\n",
      "# Training EP:71, B:999, ep.loss:252.50\n",
      "# Test avg.loss:0.26, avg.prob.:0.88/0.75, avg.acc.:0.87/0.84, avg.score:0.12/0.75\n",
      "# Training EP:72, B:999, ep.loss:233.57\n",
      "# Test avg.loss:0.27, avg.prob.:0.88/0.79, avg.acc.:0.87/0.86, avg.score:0.12/0.79\n",
      "# Training EP:73, B:999, ep.loss:256.95\n",
      "# Test avg.loss:0.24, avg.prob.:0.88/0.76, avg.acc.:0.90/0.83, avg.score:0.12/0.76\n",
      "# Training EP:74, B:999, ep.loss:265.46\n",
      "# Test avg.loss:0.23, avg.prob.:0.90/0.75, avg.acc.:0.97/0.70, avg.score:0.10/0.75\n",
      "# Training EP:75, B:999, ep.loss:225.66\n",
      "# Test avg.loss:0.21, avg.prob.:0.89/0.79, avg.acc.:0.95/0.77, avg.score:0.11/0.79\n",
      "# Training EP:76, B:999, ep.loss:237.09\n",
      "# Test avg.loss:0.21, avg.prob.:0.91/0.78, avg.acc.:0.92/0.86, avg.score:0.09/0.78\n",
      "# Training EP:77, B:999, ep.loss:238.41\n",
      "# Test avg.loss:0.30, avg.prob.:0.90/0.74, avg.acc.:0.90/0.79, avg.score:0.10/0.74\n",
      "# Training EP:78, B:999, ep.loss:245.24\n",
      "# Test avg.loss:0.25, avg.prob.:0.89/0.70, avg.acc.:0.96/0.69, avg.score:0.11/0.70\n",
      "# Training EP:79, B:999, ep.loss:245.82\n",
      "# Test avg.loss:0.23, avg.prob.:0.88/0.78, avg.acc.:0.90/0.86, avg.score:0.12/0.78\n",
      "# Training EP:80, B:999, ep.loss:230.65\n",
      "# Test avg.loss:0.21, avg.prob.:0.90/0.76, avg.acc.:0.93/0.82, avg.score:0.10/0.76\n",
      "# Training EP:81, B:999, ep.loss:229.03\n",
      "# Test avg.loss:0.22, avg.prob.:0.89/0.82, avg.acc.:0.86/0.92, avg.score:0.11/0.82\n",
      "# Training EP:82, B:288, ep.loss:75.70"
     ]
    }
   ],
   "source": [
    "RankerTrainer(m_config, m_ranker, ld_train, ld_test, optimizer, lossfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
