{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MetaNeo Ranker\n",
    "- tell which pair of tables are functionally/logically closer on execution chain\n",
    "- Stage: Cambrian\n",
    "- Version: Yorgia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tyrell.spec as S\n",
    "from tyrell.decider import Example\n",
    "\n",
    "# Morpheus Version\n",
    "from MorpheusInterpreter import *\n",
    "from ProgramSpace import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dataset for training: load random positive/negative examples every time\n",
    "'''\n",
    "class RankerDataset(Dataset):\n",
    "    def __init__(self, p_config=None, p_dataset=None, p_interpreter=None, p_spec=None):\n",
    "        self.n_sample = None # should manually assign\n",
    "        self.interpreter = p_interpreter\n",
    "        self.spec = p_spec\n",
    "        self.dataset = p_dataset\n",
    "        self.config = p_config\n",
    "        \n",
    "        # record all possible programs\n",
    "        self.progs = [\n",
    "            self.dataset[dkey][0][0]\n",
    "            for dkey in self.dataset.keys()\n",
    "        ]\n",
    "        \n",
    "        self.n_exp = len(self.progs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_sample\n",
    "    \n",
    "    '''\n",
    "    return a single triangle sample\n",
    "    (input, output1, output2, random)\n",
    "    A->B, A->C, D\n",
    "    pos: (A,B), (A,C)\n",
    "    neg: (B,A), (C,A)\n",
    "    neg: (B,C), (C,B)\n",
    "    neg: (A,D), (B,D), (C,D)\n",
    "    neg: (D,A), (D,B), (D,C)\n",
    "    '''\n",
    "    def get_triangle(self):\n",
    "        # sample (A,B)\n",
    "        while True:\n",
    "            d_progAB = random.choice(self.progs)\n",
    "            d_inputA = self.interpreter.random_table()\n",
    "            try:\n",
    "                d_evalB = self.interpreter.eval(\n",
    "                    d_progAB,\n",
    "                    [d_inputA],\n",
    "                )\n",
    "            except Exception:\n",
    "                continue\n",
    "            d_exampleAB = Example(\n",
    "                input=[d_inputA],\n",
    "                output=d_evalB,\n",
    "            )\n",
    "            d_psAB = ProgramSpace(\n",
    "                self.spec, self.interpreter,\n",
    "                d_exampleAB.input, d_exampleAB.output,\n",
    "            )\n",
    "            d_psAB.init_by_prog(d_progAB)\n",
    "            d_checkAB = self.interpreter.sanity_check(d_psAB)\n",
    "            if d_checkAB[0]:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # sample (A,C)\n",
    "        while True:\n",
    "            d_progAC = random.choice(self.progs)\n",
    "            if d_progAC.name==d_progAB.name:\n",
    "                continue\n",
    "            try:\n",
    "                d_evalC = self.interpreter.eval(\n",
    "                    d_progAC,\n",
    "                    [d_inputA],\n",
    "                )\n",
    "            except Exception:\n",
    "                continue\n",
    "            d_exampleAC = Example(\n",
    "                input=[d_inputA],\n",
    "                output=d_evalC,\n",
    "            )\n",
    "            d_psAC = ProgramSpace(\n",
    "                self.spec, self.interpreter,\n",
    "                d_exampleAC.input, d_exampleAC.output,\n",
    "            )\n",
    "            d_psAC.init_by_prog(d_progAC)\n",
    "            d_checkAC = self.interpreter.sanity_check(d_psAC)\n",
    "            if d_checkAC[0]:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # then randomly generate a table D\n",
    "        d_randD = self.interpreter.random_table()\n",
    "        \n",
    "        # all (map_r, map_c)\n",
    "        map_inputA = numpy.array([self.interpreter.camb_get_yorgia(d_inputA)])\n",
    "        map_evalB = numpy.array([self.interpreter.camb_get_yorgia(d_evalB)])\n",
    "        map_evalC = numpy.array([self.interpreter.camb_get_yorgia(d_evalC)])\n",
    "        map_randD = numpy.array([self.interpreter.camb_get_yorgia(d_randD)])\n",
    "        \n",
    "        return (map_inputA, map_evalB, map_evalC, map_randD)\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    should always use batch_size=1 so as to ensure the ratio of negative examples\n",
    "    '''\n",
    "    def __getitem__(self, p_ind):\n",
    "        return self.get_triangle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ranker(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(Ranker, self).__init__()\n",
    "        self.config = p_config\n",
    "        self.value_embedding = nn.Embedding(\n",
    "            self.config[\"vocab_size\"],\n",
    "            self.config[\"embd_dim\"],\n",
    "        )\n",
    "        self.fc0 = nn.Linear(\n",
    "            self.config[\"embd_dim\"] * self.config[\"max_length\"] * 2,\n",
    "            2048,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(\n",
    "            2048,\n",
    "            2,\n",
    "        )\n",
    "        \n",
    "    def forward(self, pA, pB, pC, pD):\n",
    "        # pA/pB/pC/pD: (B=1, seq)\n",
    "        B = pA.shape[0]\n",
    "        \n",
    "        # (B, seq, embd_dim)\n",
    "        vA = self.value_embedding(pA).view(B, self.config[\"max_length\"]*self.config[\"embd_dim\"])\n",
    "        vB = self.value_embedding(pB).view(B, self.config[\"max_length\"]*self.config[\"embd_dim\"])\n",
    "        vC = self.value_embedding(pC).view(B, self.config[\"max_length\"]*self.config[\"embd_dim\"])\n",
    "        vD = self.value_embedding(pD).view(B, self.config[\"max_length\"]*self.config[\"embd_dim\"])\n",
    "        \n",
    "        # (B=2, embd_dim * 2)\n",
    "        vPOS = torch.cat(\n",
    "            [\n",
    "                torch.cat([vA,vB],dim=1),\n",
    "                torch.cat([vA,vC],dim=1),\n",
    "            ],dim=0\n",
    "        )\n",
    "        \n",
    "        # (B=10, embd_dim * 2)\n",
    "        vNEG = torch.cat(\n",
    "            [\n",
    "                torch.cat([vB,vA],dim=1),\n",
    "                torch.cat([vB,vC],dim=1),\n",
    "                torch.cat([vC,vA],dim=1),\n",
    "                torch.cat([vC,vB],dim=1),\n",
    "            ],dim=0\n",
    "        )\n",
    "        \n",
    "        # (B=4*3=12, embd_dim * 2)\n",
    "        vII = torch.cat([vPOS,vNEG],dim=0)\n",
    "        \n",
    "        # == Notice: don't do any activation at the last layer ==\n",
    "        vOO = self.fc1(\n",
    "            F.relu(\n",
    "                self.fc0(\n",
    "                    vII\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        # (B=4*3=12, 2)\n",
    "        \n",
    "        return vOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RankerTester(p_config, p_model, pld_test, p_lossfn):\n",
    "    test_loss_list = []\n",
    "    test_pPOS_list = []\n",
    "    test_pNEG_list = []\n",
    "    test_aPOS_list = []\n",
    "    test_aNEG_list = []\n",
    "    test_sPOS_list = []\n",
    "    test_sNEG_list = []\n",
    "    for batch_idx, (dA, dB, dC, dD) in enumerate(pld_test):\n",
    "        p_model.eval()\n",
    "        if use_cuda:\n",
    "            tdA = Variable(dA).cuda() # (B=1, map_r, map_c)\n",
    "            tdB = Variable(dB).cuda() # (B=1, map_r, map_c)\n",
    "            tdC = Variable(dC).cuda() # (B=1, map_r, map_c)\n",
    "            tdD = Variable(dD).cuda() # (B=1, map_r, map_c)\n",
    "            td_label = Variable(torch.tensor(\n",
    "                [1 for _ in range(2)]+\\\n",
    "                [0 for _ in range(4)]\n",
    "            )).cuda()\n",
    "        else:\n",
    "            tdA = Variable(dA) # (B=1, map_r, map_c)\n",
    "            tdB = Variable(dB) # (B=1, map_r, map_c)\n",
    "            tdC = Variable(dC) # (B=1, map_r, map_c)\n",
    "            tdD = Variable(dD) # (B=1, map_r, map_c)\n",
    "            td_label = Variable(torch.tensor(\n",
    "                [1 for _ in range(2)]+\\\n",
    "                [0 for _ in range(4)]\n",
    "            ))\n",
    "            \n",
    "        d_output = p_model(tdA, tdB, tdC, tdD) # (B, 2)\n",
    "        d_loss = p_lossfn(\n",
    "            F.log_softmax(d_output, dim=1),\n",
    "            td_label,\n",
    "        )\n",
    "        \n",
    "        test_loss_list.append(d_loss.cpu().data.numpy())\n",
    "        test_pPOS_list += F.softmax(d_output,dim=1)[:2,1].cpu().data.tolist()\n",
    "        test_pNEG_list += F.softmax(d_output,dim=1)[2:,0].cpu().data.tolist()\n",
    "        test_aPOS_list += (torch.argmax(d_output,dim=1)[:2]==td_label[:2]).cpu().data.tolist()\n",
    "        test_aNEG_list += (torch.argmax(d_output,dim=1)[2:]==td_label[2:]).cpu().data.tolist()\n",
    "        test_sPOS_list += F.softmax(d_output,dim=1)[:2,1].cpu().data.tolist()\n",
    "        test_sNEG_list += F.softmax(d_output,dim=1)[2:,1].cpu().data.tolist()\n",
    "        \n",
    "    print(\"# Test avg.loss:{:.2f}, avg.prob.:{:.2f}/{:.2f}, avg.acc.:{:.2f}/{:.2f}, avg.score:{:.2f}/{:.2f}\".format(\n",
    "        sum(test_loss_list)/len(test_loss_list),\n",
    "        sum(test_pNEG_list)/len(test_pNEG_list),\n",
    "        sum(test_pPOS_list)/len(test_pPOS_list),\n",
    "        sum(test_aNEG_list)/len(test_aNEG_list),\n",
    "        sum(test_aPOS_list)/len(test_aPOS_list),\n",
    "        sum(test_sNEG_list)/len(test_sNEG_list), # score is the similarity score\n",
    "        sum(test_sPOS_list)/len(test_sPOS_list),\n",
    "    ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RankerTrainer(p_config, p_model, pld_train, pld_test, p_optim, p_lossfn):\n",
    "    RankerTester(p_config, p_model, pld_test, p_lossfn)\n",
    "    for d_ep in range(p_config[\"ranker\"][\"n_ep\"]):\n",
    "        epoch_loss_list = []\n",
    "        for batch_idx, (dA, dB, dC, dD) in enumerate(pld_train):\n",
    "            p_model.train()\n",
    "            \n",
    "            if use_cuda:\n",
    "                tdA = Variable(dA).cuda() # (B=1, map_r, map_c)\n",
    "                tdB = Variable(dB).cuda() # (B=1, map_r, map_c)\n",
    "                tdC = Variable(dC).cuda() # (B=1, map_r, map_c)\n",
    "                tdD = Variable(dD).cuda() # (B=1, map_r, map_c)\n",
    "                td_label = Variable(torch.tensor(\n",
    "                    [1 for _ in range(2)]+\\\n",
    "                    [0 for _ in range(4)]\n",
    "                )).cuda()\n",
    "            else:\n",
    "                tdA = Variable(dA) # (B=1, map_r, map_c)\n",
    "                tdB = Variable(dB) # (B=1, map_r, map_c)\n",
    "                tdC = Variable(dC) # (B=1, map_r, map_c)\n",
    "                tdD = Variable(dD) # (B=1, map_r, map_c)\n",
    "                td_label = Variable(torch.tensor(\n",
    "                    [1 for _ in range(2)]+\\\n",
    "                    [0 for _ in range(4)]\n",
    "                ))\n",
    "                \n",
    "            # (B=12, 2)\n",
    "            d_output = p_model(tdA, tdB, tdC, tdD)\n",
    "            p_optim.zero_grad()\n",
    "            d_loss = p_lossfn(\n",
    "                F.log_softmax(d_output, dim=1),\n",
    "                td_label,\n",
    "            )\n",
    "            epoch_loss_list.append(d_loss.cpu().data.numpy())\n",
    "            d_loss.backward()\n",
    "            p_optim.step()\n",
    "            \n",
    "            print(\"\\r# Training EP:{}, B:{}, ep.loss:{:.2f}\".format(\n",
    "                d_ep, batch_idx, sum(epoch_loss_list),\n",
    "            ),end=\"\")\n",
    "        \n",
    "        # end of epoch print a new line\n",
    "        print()\n",
    "        RankerTester(p_config, p_model, pld_test, p_lossfn)\n",
    "        \n",
    "        # save the model\n",
    "        # if d_ep%10==0:\n",
    "        # torch.save(p_model.state_dict(), \"./saved_models/0722CambRanker_Zion_ep{}.pt\".format(d_ep))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_interpreter = MorpheusInterpreter()\n",
    "m_spec = S.parse_file('./example/camb3.tyrell')\n",
    "\n",
    "m_config = {\n",
    "    \"vocab_size\": 1024,\n",
    "    \"embd_dim\": 16,\n",
    "    \"max_length\": m_interpreter.CAMB_NROW + m_interpreter.CAMB_NCOL,\n",
    "    \"ranker\":{\n",
    "        \"data_path\": \"./0716MDsize1.pkl\",\n",
    "        \"train_size\": 500, # how many samples in every epoch\n",
    "        \"test_size\": 100,\n",
    "        \"n_ep\": 1000000,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# load the data and dataset\n",
    "with open(m_config[\"ranker\"][\"data_path\"],\"rb\") as f:\n",
    "    m_data = pickle.load(f)\n",
    "    \n",
    "dt_train = RankerDataset(\n",
    "    p_config=m_config, \n",
    "    p_dataset=m_data, \n",
    "    p_interpreter=m_interpreter,\n",
    "    p_spec=m_spec,\n",
    ")\n",
    "dt_train.n_sample = m_config[\"ranker\"][\"train_size\"]\n",
    "ld_train = DataLoader(dataset=dt_train, batch_size=1, shuffle=True)\n",
    "\n",
    "dt_test = RankerDataset(\n",
    "    p_config=m_config, \n",
    "    p_dataset=m_data, \n",
    "    p_interpreter=m_interpreter,\n",
    "    p_spec=m_spec,\n",
    ")\n",
    "dt_test.n_sample = m_config[\"ranker\"][\"test_size\"]\n",
    "ld_test = DataLoader(dataset=dt_test, batch_size=1, shuffle=True)\n",
    "\n",
    "m_ranker = Ranker(p_config=m_config)\n",
    "if use_cuda:\n",
    "    m_ranker = m_ranker.cuda()\n",
    "optimizer = torch.optim.Adam(list(m_ranker.parameters()))\n",
    "lossfn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Test avg.loss:0.69, avg.prob.:0.50/0.51, avg.acc.:0.52/0.57, avg.score:0.50/0.51\n",
      "# Training EP:0, B:499, ep.loss:259.85\n",
      "# Test avg.loss:0.44, avg.prob.:0.77/0.55, avg.acc.:0.85/0.62, avg.score:0.23/0.55\n",
      "# Training EP:1, B:499, ep.loss:230.44\n",
      "# Test avg.loss:0.42, avg.prob.:0.78/0.54, avg.acc.:0.85/0.62, avg.score:0.22/0.54\n",
      "# Training EP:2, B:499, ep.loss:205.28\n",
      "# Test avg.loss:0.42, avg.prob.:0.75/0.71, avg.acc.:0.77/0.86, avg.score:0.25/0.71\n",
      "# Training EP:3, B:499, ep.loss:206.25\n",
      "# Test avg.loss:0.44, avg.prob.:0.83/0.48, avg.acc.:0.92/0.47, avg.score:0.17/0.48\n",
      "# Training EP:4, B:499, ep.loss:189.34\n",
      "# Test avg.loss:0.38, avg.prob.:0.81/0.59, avg.acc.:0.83/0.74, avg.score:0.19/0.59\n",
      "# Training EP:5, B:499, ep.loss:204.46\n",
      "# Test avg.loss:0.36, avg.prob.:0.82/0.60, avg.acc.:0.87/0.77, avg.score:0.18/0.60\n",
      "# Training EP:6, B:499, ep.loss:192.93\n",
      "# Test avg.loss:0.38, avg.prob.:0.76/0.66, avg.acc.:0.80/0.88, avg.score:0.24/0.66\n",
      "# Training EP:7, B:499, ep.loss:190.71\n",
      "# Test avg.loss:0.42, avg.prob.:0.85/0.53, avg.acc.:0.89/0.61, avg.score:0.15/0.53\n",
      "# Training EP:8, B:499, ep.loss:188.56\n",
      "# Test avg.loss:0.36, avg.prob.:0.80/0.68, avg.acc.:0.81/0.80, avg.score:0.20/0.68\n",
      "# Training EP:9, B:499, ep.loss:184.03\n",
      "# Test avg.loss:0.33, avg.prob.:0.81/0.71, avg.acc.:0.82/0.88, avg.score:0.19/0.71\n",
      "# Training EP:10, B:499, ep.loss:182.56\n",
      "# Test avg.loss:0.33, avg.prob.:0.85/0.61, avg.acc.:0.90/0.76, avg.score:0.15/0.61\n",
      "# Training EP:11, B:499, ep.loss:192.25\n",
      "# Test avg.loss:0.35, avg.prob.:0.79/0.71, avg.acc.:0.79/0.89, avg.score:0.21/0.71\n",
      "# Training EP:12, B:499, ep.loss:177.63\n",
      "# Test avg.loss:0.45, avg.prob.:0.84/0.63, avg.acc.:0.83/0.75, avg.score:0.16/0.63\n",
      "# Training EP:13, B:499, ep.loss:182.56\n",
      "# Test avg.loss:0.37, avg.prob.:0.84/0.55, avg.acc.:0.89/0.57, avg.score:0.16/0.55\n",
      "# Training EP:14, B:499, ep.loss:173.57\n",
      "# Test avg.loss:0.31, avg.prob.:0.86/0.69, avg.acc.:0.86/0.82, avg.score:0.14/0.69\n",
      "# Training EP:15, B:499, ep.loss:163.39\n",
      "# Test avg.loss:0.35, avg.prob.:0.91/0.53, avg.acc.:0.95/0.56, avg.score:0.09/0.53\n",
      "# Training EP:16, B:499, ep.loss:178.22\n",
      "# Test avg.loss:0.33, avg.prob.:0.83/0.66, avg.acc.:0.85/0.79, avg.score:0.17/0.66\n",
      "# Training EP:17, B:499, ep.loss:163.30\n",
      "# Test avg.loss:0.34, avg.prob.:0.81/0.72, avg.acc.:0.82/0.91, avg.score:0.19/0.72\n",
      "# Training EP:18, B:499, ep.loss:173.53\n",
      "# Test avg.loss:0.35, avg.prob.:0.85/0.71, avg.acc.:0.83/0.87, avg.score:0.15/0.71\n",
      "# Training EP:19, B:499, ep.loss:169.45\n",
      "# Test avg.loss:0.33, avg.prob.:0.82/0.64, avg.acc.:0.80/0.88, avg.score:0.18/0.64\n",
      "# Training EP:20, B:499, ep.loss:164.10\n",
      "# Test avg.loss:0.28, avg.prob.:0.84/0.71, avg.acc.:0.89/0.86, avg.score:0.16/0.71\n",
      "# Training EP:21, B:499, ep.loss:172.23\n",
      "# Test avg.loss:0.38, avg.prob.:0.85/0.61, avg.acc.:0.85/0.76, avg.score:0.15/0.61\n",
      "# Training EP:22, B:499, ep.loss:168.06\n",
      "# Test avg.loss:0.32, avg.prob.:0.83/0.67, avg.acc.:0.85/0.80, avg.score:0.17/0.67\n",
      "# Training EP:23, B:499, ep.loss:173.91\n",
      "# Test avg.loss:0.33, avg.prob.:0.82/0.69, avg.acc.:0.86/0.77, avg.score:0.18/0.69\n",
      "# Training EP:24, B:499, ep.loss:166.97\n",
      "# Test avg.loss:0.37, avg.prob.:0.83/0.59, avg.acc.:0.82/0.72, avg.score:0.17/0.59\n",
      "# Training EP:25, B:499, ep.loss:171.09\n",
      "# Test avg.loss:0.31, avg.prob.:0.84/0.68, avg.acc.:0.84/0.83, avg.score:0.16/0.68\n",
      "# Training EP:26, B:499, ep.loss:163.90\n",
      "# Test avg.loss:0.37, avg.prob.:0.84/0.56, avg.acc.:0.90/0.70, avg.score:0.16/0.56\n",
      "# Training EP:27, B:499, ep.loss:167.70\n",
      "# Test avg.loss:0.32, avg.prob.:0.83/0.67, avg.acc.:0.85/0.87, avg.score:0.17/0.67\n",
      "# Training EP:28, B:499, ep.loss:170.02\n",
      "# Test avg.loss:0.33, avg.prob.:0.80/0.71, avg.acc.:0.81/0.87, avg.score:0.20/0.71\n",
      "# Training EP:29, B:499, ep.loss:159.17\n",
      "# Test avg.loss:0.30, avg.prob.:0.86/0.64, avg.acc.:0.92/0.72, avg.score:0.14/0.64\n",
      "# Training EP:30, B:499, ep.loss:156.76\n",
      "# Test avg.loss:0.32, avg.prob.:0.85/0.69, avg.acc.:0.86/0.83, avg.score:0.15/0.69\n",
      "# Training EP:31, B:499, ep.loss:153.54\n",
      "# Test avg.loss:0.35, avg.prob.:0.83/0.69, avg.acc.:0.80/0.85, avg.score:0.17/0.69\n",
      "# Training EP:32, B:499, ep.loss:154.92\n",
      "# Test avg.loss:0.37, avg.prob.:0.83/0.60, avg.acc.:0.88/0.56, avg.score:0.17/0.60\n",
      "# Training EP:33, B:499, ep.loss:165.76\n",
      "# Test avg.loss:0.31, avg.prob.:0.85/0.70, avg.acc.:0.87/0.78, avg.score:0.15/0.70\n",
      "# Training EP:34, B:499, ep.loss:168.39\n",
      "# Test avg.loss:0.30, avg.prob.:0.83/0.73, avg.acc.:0.82/0.90, avg.score:0.17/0.73\n",
      "# Training EP:35, B:499, ep.loss:147.40\n",
      "# Test avg.loss:0.30, avg.prob.:0.84/0.72, avg.acc.:0.84/0.85, avg.score:0.16/0.72\n",
      "# Training EP:36, B:499, ep.loss:156.98\n",
      "# Test avg.loss:0.27, avg.prob.:0.85/0.75, avg.acc.:0.86/0.89, avg.score:0.15/0.75\n",
      "# Training EP:37, B:499, ep.loss:147.89\n",
      "# Test avg.loss:0.30, avg.prob.:0.84/0.66, avg.acc.:0.89/0.71, avg.score:0.16/0.66\n",
      "# Training EP:38, B:499, ep.loss:147.43\n",
      "# Test avg.loss:0.27, avg.prob.:0.87/0.71, avg.acc.:0.92/0.79, avg.score:0.13/0.71\n",
      "# Training EP:39, B:499, ep.loss:142.21\n",
      "# Test avg.loss:0.31, avg.prob.:0.86/0.68, avg.acc.:0.90/0.70, avg.score:0.14/0.68\n",
      "# Training EP:40, B:499, ep.loss:146.33\n",
      "# Test avg.loss:0.28, avg.prob.:0.83/0.79, avg.acc.:0.84/0.89, avg.score:0.17/0.79\n",
      "# Training EP:41, B:499, ep.loss:148.61\n",
      "# Test avg.loss:0.33, avg.prob.:0.84/0.67, avg.acc.:0.87/0.72, avg.score:0.16/0.67\n",
      "# Training EP:42, B:499, ep.loss:150.72\n",
      "# Test avg.loss:0.32, avg.prob.:0.85/0.63, avg.acc.:0.89/0.74, avg.score:0.15/0.63\n",
      "# Training EP:43, B:499, ep.loss:149.12\n",
      "# Test avg.loss:0.27, avg.prob.:0.86/0.72, avg.acc.:0.90/0.81, avg.score:0.14/0.72\n",
      "# Training EP:44, B:499, ep.loss:152.56\n",
      "# Test avg.loss:0.29, avg.prob.:0.84/0.78, avg.acc.:0.85/0.88, avg.score:0.16/0.78\n",
      "# Training EP:45, B:499, ep.loss:147.03\n",
      "# Test avg.loss:0.27, avg.prob.:0.87/0.70, avg.acc.:0.91/0.81, avg.score:0.13/0.70\n",
      "# Training EP:46, B:499, ep.loss:141.18\n",
      "# Test avg.loss:0.31, avg.prob.:0.86/0.69, avg.acc.:0.92/0.73, avg.score:0.14/0.69\n",
      "# Training EP:47, B:499, ep.loss:157.36\n",
      "# Test avg.loss:0.35, avg.prob.:0.83/0.67, avg.acc.:0.84/0.80, avg.score:0.17/0.67\n",
      "# Training EP:48, B:499, ep.loss:145.99\n",
      "# Test avg.loss:0.36, avg.prob.:0.83/0.62, avg.acc.:0.86/0.67, avg.score:0.17/0.62\n",
      "# Training EP:49, B:499, ep.loss:148.19\n",
      "# Test avg.loss:0.26, avg.prob.:0.86/0.74, avg.acc.:0.89/0.82, avg.score:0.14/0.74\n",
      "# Training EP:50, B:499, ep.loss:156.05\n",
      "# Test avg.loss:0.26, avg.prob.:0.87/0.71, avg.acc.:0.90/0.81, avg.score:0.13/0.71\n",
      "# Training EP:51, B:499, ep.loss:150.97\n",
      "# Test avg.loss:0.27, avg.prob.:0.83/0.77, avg.acc.:0.84/0.93, avg.score:0.17/0.77\n",
      "# Training EP:52, B:499, ep.loss:141.17\n",
      "# Test avg.loss:0.33, avg.prob.:0.86/0.66, avg.acc.:0.89/0.71, avg.score:0.14/0.66\n",
      "# Training EP:53, B:499, ep.loss:146.00\n",
      "# Test avg.loss:0.27, avg.prob.:0.86/0.76, avg.acc.:0.88/0.87, avg.score:0.14/0.76\n",
      "# Training EP:54, B:499, ep.loss:137.71\n",
      "# Test avg.loss:0.32, avg.prob.:0.86/0.68, avg.acc.:0.89/0.76, avg.score:0.14/0.68\n",
      "# Training EP:55, B:499, ep.loss:140.37\n",
      "# Test avg.loss:0.27, avg.prob.:0.87/0.72, avg.acc.:0.93/0.76, avg.score:0.13/0.72\n",
      "# Training EP:56, B:499, ep.loss:153.52\n",
      "# Test avg.loss:0.32, avg.prob.:0.84/0.75, avg.acc.:0.84/0.83, avg.score:0.16/0.75\n",
      "# Training EP:57, B:499, ep.loss:156.85\n",
      "# Test avg.loss:0.29, avg.prob.:0.84/0.72, avg.acc.:0.85/0.83, avg.score:0.16/0.72\n",
      "# Training EP:58, B:499, ep.loss:140.45\n",
      "# Test avg.loss:0.33, avg.prob.:0.84/0.68, avg.acc.:0.86/0.73, avg.score:0.16/0.68\n",
      "# Training EP:59, B:499, ep.loss:139.87\n",
      "# Test avg.loss:0.28, avg.prob.:0.87/0.68, avg.acc.:0.92/0.72, avg.score:0.13/0.68\n",
      "# Training EP:60, B:499, ep.loss:148.58\n",
      "# Test avg.loss:0.31, avg.prob.:0.86/0.71, avg.acc.:0.86/0.79, avg.score:0.14/0.71\n",
      "# Training EP:61, B:499, ep.loss:139.54\n",
      "# Test avg.loss:0.31, avg.prob.:0.84/0.72, avg.acc.:0.86/0.84, avg.score:0.16/0.72\n",
      "# Training EP:62, B:499, ep.loss:147.92\n",
      "# Test avg.loss:0.33, avg.prob.:0.86/0.65, avg.acc.:0.88/0.68, avg.score:0.14/0.65\n",
      "# Training EP:63, B:499, ep.loss:153.00\n",
      "# Test avg.loss:0.27, avg.prob.:0.85/0.74, avg.acc.:0.87/0.85, avg.score:0.15/0.74\n",
      "# Training EP:64, B:499, ep.loss:142.67\n",
      "# Test avg.loss:0.27, avg.prob.:0.90/0.72, avg.acc.:0.93/0.77, avg.score:0.10/0.72\n",
      "# Training EP:65, B:499, ep.loss:154.44\n",
      "# Test avg.loss:0.30, avg.prob.:0.80/0.80, avg.acc.:0.78/0.94, avg.score:0.20/0.80\n",
      "# Training EP:66, B:499, ep.loss:148.39\n",
      "# Test avg.loss:0.32, avg.prob.:0.86/0.67, avg.acc.:0.91/0.76, avg.score:0.14/0.67\n",
      "# Training EP:67, B:499, ep.loss:138.25\n",
      "# Test avg.loss:0.29, avg.prob.:0.86/0.74, avg.acc.:0.88/0.84, avg.score:0.14/0.74\n",
      "# Training EP:68, B:499, ep.loss:136.77\n",
      "# Test avg.loss:0.28, avg.prob.:0.86/0.76, avg.acc.:0.88/0.87, avg.score:0.14/0.76\n",
      "# Training EP:69, B:499, ep.loss:140.71\n",
      "# Test avg.loss:0.28, avg.prob.:0.85/0.77, avg.acc.:0.85/0.85, avg.score:0.15/0.77\n",
      "# Training EP:70, B:499, ep.loss:148.72\n",
      "# Test avg.loss:0.25, avg.prob.:0.86/0.75, avg.acc.:0.89/0.86, avg.score:0.14/0.75\n",
      "# Training EP:71, B:499, ep.loss:141.39\n",
      "# Test avg.loss:0.32, avg.prob.:0.85/0.70, avg.acc.:0.88/0.75, avg.score:0.15/0.70\n",
      "# Training EP:72, B:499, ep.loss:144.93\n",
      "# Test avg.loss:0.33, avg.prob.:0.83/0.73, avg.acc.:0.85/0.83, avg.score:0.17/0.73\n",
      "# Training EP:73, B:499, ep.loss:143.53\n",
      "# Test avg.loss:0.25, avg.prob.:0.88/0.76, avg.acc.:0.91/0.81, avg.score:0.12/0.76\n",
      "# Training EP:74, B:499, ep.loss:141.14\n",
      "# Test avg.loss:0.29, avg.prob.:0.87/0.76, avg.acc.:0.90/0.82, avg.score:0.13/0.76\n",
      "# Training EP:75, B:499, ep.loss:139.30\n",
      "# Test avg.loss:0.28, avg.prob.:0.86/0.71, avg.acc.:0.89/0.81, avg.score:0.14/0.71\n",
      "# Training EP:76, B:499, ep.loss:129.07\n",
      "# Test avg.loss:0.31, avg.prob.:0.85/0.74, avg.acc.:0.86/0.83, avg.score:0.15/0.74\n",
      "# Training EP:77, B:70, ep.loss:18.58Traceback (most recent call last):\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-10-d2e73a8cc945>\", line 1, in <module>\n",
      "    RankerTrainer(m_config, m_ranker, ld_train, ld_test, optimizer, lossfn)\n",
      "  File \"<ipython-input-8-b651dc14c7ee>\", line 28, in RankerTrainer\n",
      "    d_output = p_model(tdA, tdB, tdC, tdD)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 486, in __call__\n",
      "    if torch._C._get_tracing_state():\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2018, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/inspect.py\", line 1500, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/inspect.py\", line 1462, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 167, in findsource\n",
      "    file = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "RankerTrainer(m_config, m_ranker, ld_train, ld_test, optimizer, lossfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
