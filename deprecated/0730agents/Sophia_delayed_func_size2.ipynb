{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransNeo/AlphaNeo\n",
    "- AlphaNeo using pre-trained TransE embeddings (optional)\n",
    "- Stage: Cambrian\n",
    "- Version: Spriggina\n",
    "- Update Logs\n",
    "    - 0713: with DeepPath style rollback at training\n",
    "    - **0716: new learning paradigm, see memo for details**\n",
    "\n",
    "#### Related Commands\n",
    "- tensorboard --logdir runs\n",
    "- nohup jupyter lab > jupyter.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tyrell.spec as S\n",
    "from tyrell.decider import Example\n",
    "\n",
    "# Morpheus Version\n",
    "from MorpheusInterpreter import *\n",
    "from ProgramSpace import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransNeo(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(TransNeo, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        # predict a fixed number of shells\n",
    "#         self.policy = nn.Linear(\n",
    "#             self.config[\"embd_dim\"],\n",
    "#             self.config[\"fn\"][\"vocab_size\"],\n",
    "#         )\n",
    "        \n",
    "        # deeper\n",
    "        self.policy0 = nn.Linear(\n",
    "            self.config[\"embd_dim\"],\n",
    "            128,\n",
    "        )\n",
    "        self.policy1 = nn.Linear(\n",
    "            128,\n",
    "            self.config[\"fn\"][\"vocab_size\"],\n",
    "        )\n",
    "        \n",
    "    def forward(self, p_mapin, p_mapout):\n",
    "        # p_mapin/p_mapout: (B, 15*3)\n",
    "        v_delta = p_mapout-p_mapin\n",
    "#         tmp_out = torch.log_softmax(\n",
    "#             self.policy(v_delta),dim=1\n",
    "#         )\n",
    "        tmp_out = torch.log_softmax(\n",
    "            self.policy1(\n",
    "                F.relu(\n",
    "                    self.policy0(\n",
    "                        v_delta\n",
    "                    )\n",
    "                )\n",
    "            ),dim=1\n",
    "        )\n",
    "        \n",
    "        return tmp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace certain node id with certain value\n",
    "def modify_shell(p_shell, p_id_from, p_id_to):\n",
    "    d_prod = p_shell[0]\n",
    "    d_rhs = p_shell[1]\n",
    "    ld_rhs = [p_id_to if d_rhs[i]==p_id_from else d_rhs[i]\n",
    "             for i in range(len(d_rhs))]\n",
    "    return (d_prod, tuple(ld_rhs))\n",
    "\n",
    "\n",
    "'''\n",
    "meta-train the agent in a supervised way\n",
    "epoch -> episode, one attempt with hint\n",
    "NOTICE: only valid for size 1 training\n",
    "'''\n",
    "def MetaTrain(p_config, p_spec, p_interpreter, p_model, p_data, p_optim, p_writer):\n",
    "    print(\"# Start Meta-Train...\")\n",
    "    for d_epoch in range(p_config[\"meta_train\"][\"n_epoch\"]):\n",
    "        p_model.train()\n",
    "        \n",
    "        epoch_loss_list = []\n",
    "        batch_loss_list = []\n",
    "        random.shuffle(p_data)\n",
    "        train_data = p_data[:p_config[\"meta_train\"][\"n_truncated\"]]\n",
    "        \n",
    "        for d_ind in range(len(train_data)):\n",
    "            print(\"\\r# epoch:{}, index:{}/{}, avg.loss:{:.2f}\".format(\n",
    "                d_epoch, d_ind, len(train_data),\n",
    "                sum(epoch_loss_list)/len(epoch_loss_list)\n",
    "                if len(epoch_loss_list)>0 else 0,\n",
    "            ),end=\"\")\n",
    "            d_prog, dstr_example = train_data[d_ind]\n",
    "            d_example = Example(\n",
    "                input=[\n",
    "                    p_interpreter.load_data_into_var(p)\n",
    "                    for p in dstr_example.input\n",
    "                ],\n",
    "                output=p_interpreter.load_data_into_var(\n",
    "                    dstr_example.output\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # initialize a solution\n",
    "            ps_solution = ProgramSpace(\n",
    "                p_spec, p_interpreter, d_example.input, d_example.output,\n",
    "            )\n",
    "            ps_solution.init_by_prog(d_prog) # this constructs a solution for this problem\n",
    "            \n",
    "            # initialize a new ProgramSpace\n",
    "            ps_current = ProgramSpace(\n",
    "                p_spec, p_interpreter, d_example.input, d_example.output,\n",
    "            )\n",
    "            # then initialize a shell template\n",
    "            tmp_shell_list = ps_current.get_neighboring_shells()\n",
    "            tmp_node_to_replace = ps_current.node_dict[\"ParamNode\"][0] # for chain only\n",
    "            # replace the Param Node id in shells with -1 to make them templates\n",
    "            template_list = [\n",
    "                modify_shell(tmp_shell_list[i],tmp_node_to_replace,-1)\n",
    "                for i in range(len(tmp_shell_list))\n",
    "            ]\n",
    "            \n",
    "            id_current = ps_current.get_strict_frontiers()[0]\n",
    "            var_current = ps_current.node_list[id_current].ps_data # need the real var name in r env\n",
    "            var_output = d_example.output\n",
    "            \n",
    "            map_current = p_interpreter.camb_get_shash_abs(var_current)\n",
    "            map_output = p_interpreter.camb_get_shash_abs(var_output)\n",
    "            \n",
    "            # make current shell list\n",
    "            current_shell_list = [\n",
    "                modify_shell(template_list[i],-1,id_current)\n",
    "                for i in range(len(template_list))\n",
    "            ]\n",
    "            \n",
    "            # wrap in B=1\n",
    "            if use_cuda:\n",
    "                td_current = Variable(torch.tensor([map_current],dtype=torch.float)).cuda()\n",
    "                td_output = Variable(torch.tensor([map_output],dtype=torch.float)).cuda()\n",
    "            else:\n",
    "                td_current = Variable(torch.tensor([map_current],dtype=torch.float))\n",
    "                td_output = Variable(torch.tensor([map_output],dtype=torch.float))\n",
    "\n",
    "            # (B=1, fn_vocab_size)\n",
    "            td_pred = p_model(td_current, td_output)\n",
    "            # directly give the hint / supervised, ps.solution.shell[0] works for 1\n",
    "            tmp_id = current_shell_list.index(ps_solution.shells[0])\n",
    "            d_loss = (+1)*(-td_pred[0,tmp_id])\n",
    "            batch_loss_list.append(\n",
    "                d_loss, # supervised / always correct with +1 reward\n",
    "            )\n",
    "            epoch_loss_list.append(\n",
    "                d_loss.cpu().data.numpy(),\n",
    "            )\n",
    "            \n",
    "            if len(batch_loss_list)%p_config[\"meta_train\"][\"batch_size\"]==0 or len(batch_loss_list)==len(train_data):\n",
    "                # do back-prop.\n",
    "                if len(batch_loss_list)>0:\n",
    "                    batch_loss = sum(batch_loss_list)/len(batch_loss_list)\n",
    "                    p_optim.zero_grad()\n",
    "                    batch_loss.backward()\n",
    "                    p_optim.step()\n",
    "                # after back-prop., clean up\n",
    "                batch_loss = None\n",
    "                batch_loss_list = []\n",
    "                \n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sketch(ps0,ps1):\n",
    "    if len(ps0.node_list)!=len(ps1.node_list):\n",
    "        return False\n",
    "    for i in range(len(ps0.shells)):\n",
    "        if ps0.node_list[-i-1].name != ps1.node_list[-i-1].name:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "'''\n",
    "meta-test an agent, directly run into testing / online adaptation\n",
    "'''\n",
    "def MetaTest(p_config, p_spec, p_interpreter, p_generator, p_model, p_optim, p_writer):\n",
    "    print(\"# Start Meta-Test...\")\n",
    "    \n",
    "    n_solved = 0 # track the number of solved problem\n",
    "    n_sketch_solved = 0\n",
    "    n_attempt_list = [] # track the number of attempts in every episode\n",
    "    \n",
    "    for d_episode in range(p_config[\"meta_test\"][\"n_episode\"]):\n",
    "        \n",
    "        # retrieve the given meta-trained model for testing\n",
    "        test_model = copy.deepcopy(p_model)\n",
    "        test_model.train()\n",
    "        test_optim = torch.optim.Adam(list(test_model.parameters()))\n",
    "        \n",
    "        # if doing random meta-testing\n",
    "        # then randomly generate a program for testing\n",
    "        ps_solution = p_generator.get_new_chain_program(\n",
    "            p_config[\"meta_test\"][\"fixed_depth\"],\n",
    "        )\n",
    "        solution_prod_names = [\n",
    "            ps_solution.prod_list[p[0]].name for p in ps_solution.shells\n",
    "        ]\n",
    "        # print(\"# Solution: {}\".format(str(ps_solution.node_list[-1])))\n",
    "        # print()\n",
    "        # print(\"# Solution Sketch: {}\".format(solution_prod_names))\n",
    "        solution_shells = ps_solution.shells\n",
    "        \n",
    "        \n",
    "        f = open(\"./outputs/Sophia2/Problem_{}.txt\".format(d_episode), \"w\")\n",
    "        f.write(\"# Problem: {}\\n\\n\".format(str(ps_solution.node_list[-1])))\n",
    "        f.write(\"# Input:\\n{}\\n\".format(p_interpreter.renv(ps_solution.inputs[0])))\n",
    "        f.write(\"# Output:\\n{}\\n\".format(p_interpreter.renv(ps_solution.output)))\n",
    "        f.flush()\n",
    "        \n",
    "        is_solved = False\n",
    "        is_sketch_solved = False\n",
    "        \n",
    "        for d_attempt in range(p_config[\"meta_test\"][\"maxn_attempt\"]):\n",
    "            \n",
    "            current_prod_names = []\n",
    "            current_shells = []\n",
    "            \n",
    "            selected_neurons = []\n",
    "            stored_groups = [] # with lists of neurons of the same production name\n",
    "            \n",
    "            # in every new attempt, initialize a new Program Space\n",
    "            ps_current = ProgramSpace(\n",
    "                p_spec, p_interpreter, ps_solution.inputs, ps_solution.output,\n",
    "            )\n",
    "            # then initialize a shell template\n",
    "            tmp_shell_list = ps_current.get_neighboring_shells()\n",
    "            tmp_node_to_replace = ps_current.node_dict[\"ParamNode\"][0] # for chain only\n",
    "            # replace the Param Node id in shells with -1 to make them templates\n",
    "            template_list = [\n",
    "                modify_shell(tmp_shell_list[i],tmp_node_to_replace,-1)\n",
    "                for i in range(len(tmp_shell_list))\n",
    "            ]\n",
    "                \n",
    "            d_step = 0\n",
    "            while d_step<p_config[\"meta_test\"][\"maxn_step\"]:\n",
    "                \n",
    "                # print the training progress\n",
    "                print(\"\\r# AC/SK/EP:{}/{}/{}, AT:{}, SP:{}, avg.attempt:{:.2f},\".format(\n",
    "                    n_solved, n_sketch_solved, d_episode, d_attempt, d_step,\n",
    "                    sum(n_attempt_list)/len(n_attempt_list) if len(n_attempt_list)>0 else -1,\n",
    "                ),end=\"\")\n",
    "                \n",
    "                # ### assume chain execution, so only 1 possible returns\n",
    "                # ### at d_step=0, this should be input[0]\n",
    "                id_current = ps_current.get_strict_frontiers()[0]\n",
    "                var_current = ps_current.node_list[id_current].ps_data # need the real var name in r env\n",
    "                var_output = ps_current.output\n",
    "                \n",
    "                map_current = p_interpreter.camb_get_shash_abs(var_current)\n",
    "                map_output = p_interpreter.camb_get_shash_abs(var_output)\n",
    "                \n",
    "                # make current shell list\n",
    "                current_shell_list = [\n",
    "                    modify_shell(template_list[i],-1,id_current)\n",
    "                    for i in range(len(template_list))\n",
    "                ]\n",
    "                \n",
    "                # wrap in B=1\n",
    "                if use_cuda:\n",
    "                    td_current = Variable(torch.tensor([map_current],dtype=torch.float)).cuda()\n",
    "                    td_output = Variable(torch.tensor([map_output],dtype=torch.float)).cuda()\n",
    "                else:\n",
    "                    td_current = Variable(torch.tensor([map_current],dtype=torch.float))\n",
    "                    td_output = Variable(torch.tensor([map_output],dtype=torch.float))\n",
    "                    \n",
    "                # (B=1, fn_vocab_size)\n",
    "                td_pred = test_model(td_current, td_output)\n",
    "                \n",
    "                # no hints\n",
    "                if random.random()<=p_config[\"meta_test\"][\"exploration_rate\"]:\n",
    "                    # exploration\n",
    "                    tmp_id = random.choice(range(len(current_shell_list)))\n",
    "                else:\n",
    "                    # exploitation\n",
    "                    # tmp_id = torch.multinomial(td_pred.exp().flatten(), 1).cpu().flatten().numpy()[0]\n",
    "                    tmp_id = torch.argmax(td_pred.flatten()).cpu().tolist()\n",
    "                    \n",
    "                    \n",
    "                # == Yorgia ==\n",
    "                # find out all other shells that share the same product name\n",
    "                tmp_component_name = ps_current.prod_list[current_shell_list[tmp_id][0]].name\n",
    "                tmp_group = []\n",
    "                for i in range(len(current_shell_list)):\n",
    "                    if ps_current.prod_list[current_shell_list[i][0]].name==tmp_component_name:\n",
    "                        tmp_group.append(td_pred[0,i])\n",
    "                stored_groups.append(tmp_group)\n",
    "                    \n",
    "                # == Yorgia ==\n",
    "                # add prod names first\n",
    "                current_prod_names.append(tmp_component_name)\n",
    "                # then add shells\n",
    "                current_shells.append(current_shell_list[tmp_id])\n",
    "                \n",
    "                # update ps_current\n",
    "                update_status = ps_current.add_neighboring_shell(\n",
    "                    current_shell_list[tmp_id]\n",
    "                )\n",
    "                \n",
    "                if update_status:\n",
    "                    # record selected neuron\n",
    "                    selected_neurons.append((True, td_pred[0,tmp_id]))\n",
    "                    d_step += 1\n",
    "                    \n",
    "                    # succeed\n",
    "                    if ps_current.check_eq() is not None:\n",
    "                        # and solved!\n",
    "                        is_solved = True\n",
    "                        n_solved += 1\n",
    "                        break\n",
    "                else:\n",
    "                    selected_neurons.append((False, td_pred[0,tmp_id]))\n",
    "                    break\n",
    "            \n",
    "            # <END_FOR_STEP>\n",
    "            f.write(\"# ({}) Proposed {}/{}: {}\\n\".format(\n",
    "                \"accept\" if is_solved else \"reject\",\n",
    "                d_attempt, p_config[\"meta_test\"][\"maxn_attempt\"],\n",
    "                str(ps_current.node_list[-1])\n",
    "            ))\n",
    "            f.flush()\n",
    "            \n",
    "            if not is_sketch_solved:\n",
    "                if compare_sketch(ps_current, ps_solution):\n",
    "                    is_sketch_solved = True\n",
    "                    n_sketch_solved += 1\n",
    "                    \n",
    "            if is_solved:\n",
    "                n_attempt_list.append(d_attempt)\n",
    "                break\n",
    "            \n",
    "            # print(\"# Current Sketch: {}\".format(current_prod_names))\n",
    "            # == Yorgia ==\n",
    "            # compute the loss according to the loss computation rules\n",
    "            # first component, then function call\n",
    "            batch_loss_list = []\n",
    "            for i in range(len(current_prod_names)):\n",
    "                if solution_prod_names[i]==current_prod_names[i]:\n",
    "                    # component match, promote the whole group\n",
    "                    for j in range(len(stored_groups[i])):\n",
    "                        batch_loss_list.append(\n",
    "                            (+1.0)*(-stored_groups[i][j])\n",
    "                        )\n",
    "                else:\n",
    "                    for j in range(len(stored_groups[i])):\n",
    "                        batch_loss_list.append(\n",
    "                            (-1.0)*(-stored_groups[i][j])\n",
    "                        )\n",
    "            \n",
    "            # compute delayed reward\n",
    "            # == if you reach here, it means it's not solved\n",
    "            delayed_loss = 0.\n",
    "            for i in range(len(selected_neurons)):\n",
    "                delayed_loss += (-1.0)*(-selected_neurons[i][1])\n",
    "            \n",
    "                \n",
    "            batch_loss = sum(batch_loss_list) + delayed_loss\n",
    "            test_optim.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            test_optim.step()\n",
    "            \n",
    "                \n",
    "        # <END_FOR_ATTEMPT>     \n",
    "        \n",
    "        f.close()\n",
    "            \n",
    "    # <END_FOR_EPISODE>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Total Meta-Train Data: 77038\n"
     ]
    }
   ],
   "source": [
    "m_interpreter = MorpheusInterpreter()\n",
    "m_spec = S.parse_file('./example/camb3.tyrell')\n",
    "m_generator = MorpheusGenerator(\n",
    "    spec=m_spec,\n",
    "    interpreter=m_interpreter,\n",
    ")\n",
    "\n",
    "# dumb variable to help infer the shells\n",
    "m_ps = ProgramSpace(\n",
    "    m_spec, m_interpreter, [None], None,\n",
    ")\n",
    "\n",
    "m_config = {\n",
    "    \"fn\":{\n",
    "        \"vocab_size\": len(m_ps.get_neighboring_shells())\n",
    "    },\n",
    "    \"embd_dim\": 15*5,\n",
    "    # ==== Meta-Learning Setting ==== #\n",
    "    \"meta_train\":{\n",
    "        \"n_epoch\": 10,\n",
    "        \"batch_size\": 4, # how many indices\n",
    "        \"data_path\": \"./0716MDsize1.pkl\",\n",
    "        \"n_truncated\": 1000,\n",
    "    },\n",
    "    \"meta_test\":{\n",
    "        \"n_episode\": 100000,\n",
    "        \"batch_size\": 1, # how many attempts\n",
    "        \"fixed_depth\": 3,\n",
    "        \"maxn_attempt\": 1000,\n",
    "        \"maxn_step\": 2, # program size\n",
    "        \"exploration_rate\": 0,\n",
    "        \"decay_rate\": 0.9,\n",
    "        \"dp_cap\": 50,\n",
    "    },\n",
    "}\n",
    "\n",
    "# load the size 1 supervised data\n",
    "with open(m_config[\"meta_train\"][\"data_path\"],\"rb\") as f:\n",
    "    dt_data = pickle.load(f)\n",
    "m_data = [\n",
    "    dt_data[dkey][i]\n",
    "    for dkey in dt_data.keys()\n",
    "    for i in range(len(dt_data[dkey]))\n",
    "]\n",
    "print(\"# Total Meta-Train Data: {}\".format(len(m_data)))\n",
    "\n",
    "trans_neo = TransNeo(p_config=m_config)\n",
    "if use_cuda:\n",
    "    trans_neo = trans_neo.cuda()\n",
    "optimizer = torch.optim.Adam(list(trans_neo.parameters()))\n",
    "\n",
    "# writer = SummaryWriter(\"runs/0713CAMB_RL2_camb3\")\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fn': {'vocab_size': 120},\n",
       " 'embd_dim': 75,\n",
       " 'meta_train': {'n_epoch': 10,\n",
       "  'batch_size': 4,\n",
       "  'data_path': './0716MDsize1.pkl',\n",
       "  'n_truncated': 1000},\n",
       " 'meta_test': {'n_episode': 100000,\n",
       "  'batch_size': 1,\n",
       "  'fixed_depth': 3,\n",
       "  'maxn_attempt': 1000,\n",
       "  'maxn_step': 2,\n",
       "  'exploration_rate': 0,\n",
       "  'decay_rate': 0.9,\n",
       "  'dp_cap': 50}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MetaTrain(m_config, m_spec, m_interpreter, trans_neo, m_data, optimizer, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Start Meta-Test...\n",
      "# AC/SK/EP:17/23/23, AT:22, SP:0, avg.attempt:355.76,Traceback (most recent call last):\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-cdc6b9d424ca>\", line 1, in <module>\n",
      "    MetaTest(m_config, m_spec, m_interpreter, m_generator, trans_neo, optimizer, writer)\n",
      "  File \"<ipython-input-7-b3ba6a6926f3>\", line 86, in MetaTest\n",
      "    map_output = p_interpreter.camb_get_shash_abs(var_output)\n",
      "  File \"/home/ju-ucsb/Trinity/MorpheusInterpreter.py\", line 889, in camb_get_shash_abs\n",
      "    df_hash_val = hash(self.renv(p_obj).r_repr())%15\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/rpy2/robjects/__init__.py\", line 388, in __call__\n",
      "    p = _rparse(text=StrSexpVector((string,)))\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/rpy2/rinterface_lib/conversion.py\", line 28, in _\n",
      "    cdata = function(*args, **kwargs)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/rpy2/rinterface.py\", line 771, in __call__\n",
      "    error_occured))\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2018, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/inspect.py\", line 1500, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/inspect.py\", line 1462, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 179, in findsource\n",
      "    lines = linecache.getlines(file, globals_dict)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/tokenize.py\", line 449, in open\n",
      "    encoding, lines = detect_encoding(buffer.readline)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/tokenize.py\", line 418, in detect_encoding\n",
      "    first = read_or_stop()\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/tokenize.py\", line 376, in read_or_stop\n",
      "    return readline()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "MetaTest(m_config, m_spec, m_interpreter, m_generator, trans_neo, optimizer, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed: Problem 5, 6, 30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
