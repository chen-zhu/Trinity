{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransNeo/AlphaNeo\n",
    "- AlphaNeo using pre-trained TransE embeddings (optional)\n",
    "- Stage: Cambrian\n",
    "- Version: Spriggina\n",
    "- Update Logs\n",
    "    - 0713: with DeepPath style rollback at training\n",
    "    - 0716: new learning paradigm, see memo for details\n",
    "    - **0720: replace the meta-train with MAML mechanism**\n",
    "\n",
    "#### Related Commands\n",
    "- tensorboard --logdir runs\n",
    "- nohup jupyter lab > jupyter.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tyrell.spec as S\n",
    "from tyrell.decider import Example\n",
    "\n",
    "# Morpheus Version\n",
    "from MorpheusInterpreter import *\n",
    "from ProgramSpace import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEncoder(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(ValueEncoder, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        self.vocab_size = self.config[\"val\"][\"vocab_size\"]\n",
    "        self.embd_dim = self.config[\"val\"][\"embd_dim\"]\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embd_dim,\n",
    "            self.config[\"val\"][\"IDX_PAD\"],\n",
    "        )\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels = self.config[\"val\"][\"embd_dim\"],\n",
    "            out_channels = self.config[\"val\"][\"conv_n_kernels\"],\n",
    "            kernel_size = self.config[\"val\"][\"conv_kernel_size\"],\n",
    "        )\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size = self.config[\"val\"][\"pool_kernel_size\"],\n",
    "            padding = self.config[\"val\"][\"IDX_PAD\"],\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            self.config[\"val\"][\"conv_n_kernels\"],\n",
    "            self.config[\"embd_dim\"],\n",
    "        )\n",
    "        \n",
    "    def forward(self, bp_map):\n",
    "        # batched maps, (B, map_r, map_c)\n",
    "        # in this version, every value only contains 1 map\n",
    "        B = bp_map.shape[0]\n",
    "        \n",
    "        # (B, map_r, map_c, val_embd_dim) -> (B, val_embd_dim, map_r, map_c)\n",
    "        d_embd = self.embedding(bp_map).permute(0,3,1,2)\n",
    "        \n",
    "        # (B, n_kernel, map_r, 1)\n",
    "        d_conv = F.relu(self.conv(d_embd))\n",
    "        \n",
    "        # (B, n_kernel)\n",
    "        d_pool = self.pool(d_conv).view(B,self.config[\"val\"][\"conv_n_kernels\"])\n",
    "        \n",
    "        # (B, embd_dim)\n",
    "        d_out = torch.sigmoid(\n",
    "            self.fc(d_pool)\n",
    "        )\n",
    "        \n",
    "        return d_out\n",
    "    \n",
    "    '''\n",
    "    p_weights:[\n",
    "        'mte.value_encoder.embedding.weight',\n",
    "        'mte.value_encoder.conv.weight',\n",
    "        'mte.value_encoder.conv.bias',\n",
    "        'mte.value_encoder.fc.weight',\n",
    "        'mte.value_encoder.fc.bias',\n",
    "    ]\n",
    "    '''\n",
    "    def fake_forward(self, bp_map, p_weights):\n",
    "        # batched maps, (B, map_r, map_c)\n",
    "        # in this version, every value only contains 1 map\n",
    "        B = bp_map.shape[0]\n",
    "        \n",
    "        # (B, map_r, map_c, val_embd_dim) -> (B, val_embd_dim, map_r, map_c)\n",
    "        # d_embd = self.embedding(bp_map).permute(0,3,1,2)\n",
    "        d_embd = F.embedding(\n",
    "            bp_map, p_weights[0], self.embedding.padding_idx, self.embedding.max_norm,\n",
    "            self.embedding.norm_type, self.embedding.scale_grad_by_freq, self.embedding.sparse,\n",
    "        ).permute(0,3,1,2)\n",
    "        \n",
    "        # (B, n_kernel, map_r, 1)\n",
    "        # d_conv = F.relu(self.conv(d_embd))\n",
    "        # no padding_mode=='circular', so skip the large part of conv\n",
    "        d_conv = F.relu(\n",
    "            F.conv2d(\n",
    "                d_embd, p_weights[1], p_weights[2], self.conv.stride,\n",
    "                self.conv.padding, self.conv.dilation, self.conv.groups,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # (B, n_kernel)\n",
    "        d_pool = self.pool(d_conv).view(B,self.config[\"val\"][\"conv_n_kernels\"])\n",
    "        \n",
    "        # (B, embd_dim)\n",
    "        # d_out = torch.sigmoid(\n",
    "        #     self.fc(d_pool)\n",
    "        # )\n",
    "        d_out = torch.sigmoid(\n",
    "            F.linear(d_pool, p_weights[3], p_weights[4])\n",
    "        )\n",
    "        \n",
    "        return d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphTransE(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(MorphTransE, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        self.value_encoder = ValueEncoder(p_config=p_config)\n",
    "        \n",
    "        self.fn_vocab_size = self.config[\"fn\"][\"vocab_size\"]\n",
    "        self.embd_dim = self.config[\"embd_dim\"]\n",
    "        \n",
    "        self.fn_embedding = nn.Embedding(\n",
    "            self.fn_vocab_size,\n",
    "            self.embd_dim,\n",
    "        )\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fn_embedding.weight.data)\n",
    "        self.fn_embedding.weight.data = F.normalize(\n",
    "            self.fn_embedding.weight.data, p=2, dim=1,\n",
    "        )\n",
    "# ----> skip the forward part since we don't need it <---- #\n",
    "# ----> also no need for fake_forward <---- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransNeo(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(TransNeo, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        # predict a fixed number of shells\n",
    "        # deeper\n",
    "        self.policy0 = nn.Linear(\n",
    "            self.config[\"embd_dim\"],\n",
    "            2048,\n",
    "        )\n",
    "        self.policy1 = nn.Linear(\n",
    "            2048,\n",
    "            self.config[\"fn\"][\"vocab_size\"],\n",
    "        )\n",
    "        \n",
    "        self.mte = MorphTransE(p_config=p_config)\n",
    "        # then load the parameters\n",
    "        # self.mte.load_state_dict(torch.load(self.config[\"mte_path\"]))\n",
    "        \n",
    "    def forward(self, p_mapin, p_mapout):\n",
    "        # p_mapin/p_mapout: (B, map_r, map_c)\n",
    "        # normal forward using parameters of self\n",
    "        v_in = self.mte.value_encoder(p_mapin) # (B, embd_dim)\n",
    "        v_out= self.mte.value_encoder(p_mapout) # (B, embd_dim)\n",
    "        v_delta = v_out - v_in\n",
    "        tmp_out = torch.log_softmax(\n",
    "            self.policy1(\n",
    "                F.relu(\n",
    "                    self.policy0(\n",
    "                        v_delta\n",
    "                    )\n",
    "                )\n",
    "            ),dim=1\n",
    "        )\n",
    "\n",
    "        return tmp_out\n",
    "    \n",
    "    def fake_forward(self, p_mapin, p_mapout, p_weights):\n",
    "        # p_mapin/p_mapout: (B, map_r, map_c)\n",
    "        # normal forward using parameters of self\n",
    "        v_in = self.mte.value_encoder.fake_forward(p_mapin, p_weights[4:9]) # (B, embd_dim)\n",
    "        v_out= self.mte.value_encoder.fake_forward(p_mapout, p_weights[4:9]) # (B, embd_dim)\n",
    "        v_delta = v_out - v_in\n",
    "#         tmp_out = torch.log_softmax(\n",
    "#             self.policy1(\n",
    "#                 F.relu(\n",
    "#                     self.policy0(\n",
    "#                         v_delta\n",
    "#                     )\n",
    "#                 )\n",
    "#             ),dim=1\n",
    "#         )\n",
    "        tmp_out = torch.log_softmax(\n",
    "            F.linear(\n",
    "                F.relu(\n",
    "                    F.linear(\n",
    "                        v_delta,\n",
    "                        p_weights[0],\n",
    "                        p_weights[1],\n",
    "                    )\n",
    "                ),\n",
    "                p_weights[2],\n",
    "                p_weights[3],\n",
    "            ),dim=1,\n",
    "        )\n",
    "\n",
    "        return tmp_out\n",
    "    \n",
    "    '''\n",
    "    == MAML ==\n",
    "    return a clone dict (named tuple of tensors cloned from parameters)\n",
    "    fixed sequence:[\n",
    "     'policy0.weight',\n",
    "     'policy0.bias',\n",
    "     'policy1.weight',\n",
    "     'policy1.bias',\n",
    "     'mte.value_encoder.embedding.weight',\n",
    "     'mte.value_encoder.conv.weight',\n",
    "     'mte.value_encoder.conv.bias',\n",
    "     'mte.value_encoder.fc.weight',\n",
    "     'mte.value_encoder.fc.bias',\n",
    "     'mte.fn_embedding.weight'\n",
    "    ]\n",
    "    '''\n",
    "    def clone(self):\n",
    "        ret_list = [\n",
    "            w.clone() for w in self.parameters()\n",
    "        ]\n",
    "        return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace certain node id with certain value\n",
    "def modify_shell(p_shell, p_id_from, p_id_to):\n",
    "    d_prod = p_shell[0]\n",
    "    d_rhs = p_shell[1]\n",
    "    ld_rhs = [p_id_to if d_rhs[i]==p_id_from else d_rhs[i]\n",
    "             for i in range(len(d_rhs))]\n",
    "    return (d_prod, tuple(ld_rhs))\n",
    "\n",
    "\n",
    "'''\n",
    "== MAML MetaTrain ==\n",
    "meta-train the agent in a supervised way\n",
    "epoch -> episode, one attempt with hint\n",
    "NOTICE: only valid for size 1 training\n",
    "'''\n",
    "def MetaTrain(p_config, p_spec, p_interpreter, p_model, p_data, p_optim, p_writer):\n",
    "    # p_data: dict of list of data, every key is a size 1 pattern\n",
    "    # p_data:{\n",
    "    #     key1: (\n",
    "    #         [ (prog, str_example), (prog, str_example), ... ] // train\n",
    "    #         [ (prog, str_example), (prog, str_example), ... ] // test\n",
    "    #     ),\n",
    "    #     key2: (\n",
    "    #         ...\n",
    "    #     )\n",
    "    # }\n",
    "    \n",
    "    # == MAML outer loop == #\n",
    "    print(\"# Start MAML Meta-Train...\")\n",
    "    for d_epoch in range(p_config[\"meta_train\"][\"n_epoch\"]):\n",
    "        p_model.train()\n",
    "        p_optim.zero_grad()\n",
    "        epoch_loss = 0.\n",
    "        \n",
    "        for d_task in range(p_config[\"meta_train\"][\"n_task\"]):\n",
    "            task_weights = p_model.clone()\n",
    "            sampled_task = random.choice(list(p_data.keys()))\n",
    "            \n",
    "            batch_loss_list = []\n",
    "            # == MAML inner loop == #\n",
    "            # compute inner loss on randomly sampled train data\n",
    "            for d_train in range(p_config[\"meta_train\"][\"n_train\"]):\n",
    "                \n",
    "                print(\"\\r# EP:{}, Task:{}, Train:{}, Ep.Loss:{:.2f}\".format(\n",
    "                    d_epoch, d_task, d_train, epoch_loss,\n",
    "                ),end=\"\")\n",
    "                \n",
    "                d_prog, dstr_example = random.choice(p_data[sampled_task][0]) # ind 0 is training data\n",
    "                d_example = Example(\n",
    "                    input=[\n",
    "                        p_interpreter.load_data_into_var(p)\n",
    "                        for p in dstr_example.input\n",
    "                    ],\n",
    "                    output=p_interpreter.load_data_into_var(\n",
    "                        dstr_example.output\n",
    "                    )\n",
    "                )\n",
    "                # initialize a solution\n",
    "                ps_solution = ProgramSpace(\n",
    "                    p_spec, p_interpreter, d_example.input, d_example.output,\n",
    "                )\n",
    "                # load the program into ProgramSpace\n",
    "                ps_solution.init_by_prog(d_prog)\n",
    "                # initialize a new ProgramSpace\n",
    "                ps_current = ProgramSpace(\n",
    "                    p_spec, p_interpreter, d_example.input, d_example.output,\n",
    "                )\n",
    "                # then initialize a shell template\n",
    "                tmp_shell_list = ps_current.get_neighboring_shells()\n",
    "                tmp_node_to_replace = ps_current.node_dict[\"ParamNode\"][0] # for chain only\n",
    "                # replace the Param Node id in shells with -1 to make them templates\n",
    "                template_list = [\n",
    "                    modify_shell(tmp_shell_list[i],tmp_node_to_replace,-1)\n",
    "                    for i in range(len(tmp_shell_list))\n",
    "                ]\n",
    "                \n",
    "                id_current = ps_current.get_strict_frontiers()[0]\n",
    "                var_current = ps_current.node_list[id_current].ps_data # need the real var name in r env\n",
    "                var_output = d_example.output\n",
    "            \n",
    "                map_current = p_interpreter.camb_get_abs(var_current)\n",
    "                map_output = p_interpreter.camb_get_abs(var_output)\n",
    "    \n",
    "                # make current shell list\n",
    "                current_shell_list = [\n",
    "                    modify_shell(template_list[i],-1,id_current)\n",
    "                    for i in range(len(template_list))\n",
    "                ]\n",
    "            \n",
    "                # wrap in B=1\n",
    "                if use_cuda:\n",
    "                    td_current = Variable(torch.tensor([map_current],dtype=torch.long)).cuda()\n",
    "                    td_output = Variable(torch.tensor([map_output],dtype=torch.long)).cuda()\n",
    "                else:\n",
    "                    td_current = Variable(torch.tensor([map_current],dtype=torch.long))\n",
    "                    td_output = Variable(torch.tensor([map_output],dtype=torch.long))\n",
    "            \n",
    "                # (B=1, fn_vocab_size)\n",
    "                td_pred = p_model.fake_forward(td_current, td_output, task_weights)\n",
    "                \n",
    "                # directly give the hint / supervised, ps.solution.shell[0] works for 1\n",
    "                tmp_id = current_shell_list.index(ps_solution.shells[0])\n",
    "                d_loss = (+1)*(-td_pred[0,tmp_id]) # supervised / always correct with +1 reward\n",
    "                batch_loss_list.append(d_loss)\n",
    "                \n",
    "                if (d_train+1)%p_config[\"meta_train\"][\"batch_size\"]==0 or d_train+1==p_config[\"meta_train\"][\"n_train\"]:\n",
    "                    # do back-prop.\n",
    "                    if len(batch_loss_list)>0:\n",
    "                        batch_loss = sum(batch_loss_list)/len(batch_loss_list)\n",
    "                        task_grads = torch.autograd.grad(batch_loss, task_weights, allow_unused=True)\n",
    "                        # grad update as computation graph, different from backward and step\n",
    "                        task_weights = [\n",
    "                            # fn_embedding may get None\n",
    "                            z0 - p_config[\"meta_train\"][\"inner_lr\"]*z1 if z1 is not None else z0\n",
    "                            for z0,z1 in zip(task_weights, task_grads)\n",
    "                        ]\n",
    "                    # after back-prop., clean up\n",
    "                    batch_loss = None\n",
    "                    batch_loss_list = []\n",
    "                    \n",
    "            # <ENDFOR:d_train>\n",
    "            \n",
    "            batch_loss_list = []\n",
    "            # then start compute the loss of outer loop\n",
    "            for d_test in range(p_config[\"meta_train\"][\"n_test\"]):\n",
    "                d_prog, dstr_example = random.choice(p_data[sampled_task][1]) # ind 1 is testing data\n",
    "                d_example = Example(\n",
    "                    input=[\n",
    "                        p_interpreter.load_data_into_var(p)\n",
    "                        for p in dstr_example.input\n",
    "                    ],\n",
    "                    output=p_interpreter.load_data_into_var(\n",
    "                        dstr_example.output\n",
    "                    )\n",
    "                )\n",
    "                # initialize a solution\n",
    "                ps_solution = ProgramSpace(\n",
    "                    p_spec, p_interpreter, d_example.input, d_example.output,\n",
    "                )\n",
    "                # load the program into ProgramSpace\n",
    "                ps_solution.init_by_prog(d_prog)\n",
    "                # initialize a new ProgramSpace\n",
    "                ps_current = ProgramSpace(\n",
    "                    p_spec, p_interpreter, d_example.input, d_example.output,\n",
    "                )\n",
    "                # then initialize a shell template\n",
    "                tmp_shell_list = ps_current.get_neighboring_shells()\n",
    "                tmp_node_to_replace = ps_current.node_dict[\"ParamNode\"][0] # for chain only\n",
    "                # replace the Param Node id in shells with -1 to make them templates\n",
    "                template_list = [\n",
    "                    modify_shell(tmp_shell_list[i],tmp_node_to_replace,-1)\n",
    "                    for i in range(len(tmp_shell_list))\n",
    "                ]\n",
    "                \n",
    "                id_current = ps_current.get_strict_frontiers()[0]\n",
    "                var_current = ps_current.node_list[id_current].ps_data # need the real var name in r env\n",
    "                var_output = d_example.output\n",
    "            \n",
    "                map_current = p_interpreter.camb_get_abs(var_current)\n",
    "                map_output = p_interpreter.camb_get_abs(var_output)\n",
    "    \n",
    "                # make current shell list\n",
    "                current_shell_list = [\n",
    "                    modify_shell(template_list[i],-1,id_current)\n",
    "                    for i in range(len(template_list))\n",
    "                ]\n",
    "            \n",
    "                # wrap in B=1\n",
    "                if use_cuda:\n",
    "                    td_current = Variable(torch.tensor([map_current],dtype=torch.long)).cuda()\n",
    "                    td_output = Variable(torch.tensor([map_output],dtype=torch.long)).cuda()\n",
    "                else:\n",
    "                    td_current = Variable(torch.tensor([map_current],dtype=torch.long))\n",
    "                    td_output = Variable(torch.tensor([map_output],dtype=torch.long))\n",
    "            \n",
    "                # (B=1, fn_vocab_size)\n",
    "                td_pred = p_model.fake_forward(td_current, td_output, task_weights)\n",
    "                # directly give the hint / supervised, ps.solution.shell[0] works for 1\n",
    "                tmp_id = current_shell_list.index(ps_solution.shells[0])\n",
    "                d_loss = (+1)*(-td_pred[0,tmp_id]) # supervised / always correct with +1 reward\n",
    "                batch_loss_list.append(d_loss)\n",
    "                \n",
    "            # <ENDFOR:d_train>\n",
    "            # no batching in testing, compute the loss directly\n",
    "            if len(batch_loss_list)>0:\n",
    "                batch_loss = sum(batch_loss_list)/len(batch_loss_list)\n",
    "                epoch_loss += batch_loss\n",
    "                # then clean up\n",
    "                batch_loss = None\n",
    "                batch_loss_list = []\n",
    "                \n",
    "        # <ENDFOR:d_task>\n",
    "        # then copy the grads from epoch model to p_model\n",
    "        epoch_grads = torch.autograd.grad(epoch_loss, list(p_model.parameters()), allow_unused=True)\n",
    "        for z0, z1 in zip(list(p_model.parameters()), epoch_grads):\n",
    "            z0.grad = z1\n",
    "        p_optim.step()\n",
    "        print()\n",
    "    # <ENDFOR:d_epoch>\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "meta-test an agent, directly run into testing / online adaptation\n",
    "'''\n",
    "def MetaTest(p_config, p_spec, p_interpreter, p_generator, p_model, p_optim, p_writer):\n",
    "    print(\"# Start Meta-Test...\")\n",
    "    \n",
    "    nth_attempt = 0 # tell whether to back-prop or not\n",
    "    batch_lossA_list = []\n",
    "    batch_lossD_list = []\n",
    "    \n",
    "    n_solved = 0 # track the number of solved problem\n",
    "    n_attempt_list = [] # track the number of attempts in every episode\n",
    "    \n",
    "    selected_neurons = []\n",
    "    dead_neurons = [] # DeepPath: store node with execution error\n",
    "    \n",
    "    for d_episode in range(p_config[\"meta_test\"][\"n_episode\"]):\n",
    "        \n",
    "        # retrieve the given meta-trained model for testing\n",
    "        test_model = copy.deepcopy(p_model)\n",
    "        test_model.train()\n",
    "        \n",
    "        # if doing random meta-testing\n",
    "        # then randomly generate a program for testing\n",
    "        ps_solution = p_generator.get_new_chain_program(\n",
    "            p_config[\"meta_test\"][\"fixed_depth\"],\n",
    "        )\n",
    "        \n",
    "        is_solved = False\n",
    "        \n",
    "        for d_attempt in range(p_config[\"meta_test\"][\"maxn_attempt\"]):\n",
    "            if is_solved:\n",
    "                # already solved in the last attempt, stop\n",
    "                break\n",
    "            \n",
    "            nth_attempt += 1\n",
    "            attempt_reward = None\n",
    "            \n",
    "            # in every new attempt, initialize a new Program Space\n",
    "            ps_current = ProgramSpace(\n",
    "                p_spec, p_interpreter, ps_solution.inputs, ps_solution.output,\n",
    "            )\n",
    "            # then initialize a shell template\n",
    "            tmp_shell_list = ps_current.get_neighboring_shells()\n",
    "            tmp_node_to_replace = ps_current.node_dict[\"ParamNode\"][0] # for chain only\n",
    "            # replace the Param Node id in shells with -1 to make them templates\n",
    "            template_list = [\n",
    "                modify_shell(tmp_shell_list[i],tmp_node_to_replace,-1)\n",
    "                for i in range(len(tmp_shell_list))\n",
    "            ]\n",
    "                \n",
    "            d_step = 0\n",
    "            while d_step<p_config[\"meta_test\"][\"maxn_step\"]:\n",
    "                \n",
    "                # print the training progress\n",
    "                print(\"\\r# AC/EP:{}/{}, AT:{}, SP:{}, DN:{}, avg.attempt:{:.2f}, er:{:.2f}\".format(\n",
    "                    n_solved, d_episode, d_attempt, d_step, \n",
    "                    len(dead_neurons),\n",
    "                    sum(n_attempt_list)/len(n_attempt_list) if len(n_attempt_list)>0 else -1,\n",
    "                    p_config[\"meta_test\"][\"exploration_rate\"](d_episode,d_attempt),\n",
    "                ),end=\"\")\n",
    "                \n",
    "                # ### assume chain execution, so only 1 possible returns\n",
    "                # ### at d_step=0, this should be input[0]\n",
    "                id_current = ps_current.get_strict_frontiers()[0]\n",
    "                var_current = ps_current.node_list[id_current].ps_data # need the real var name in r env\n",
    "                var_output = ps_current.output\n",
    "                \n",
    "                map_current = p_interpreter.camb_get_abs(var_current)\n",
    "                map_output = p_interpreter.camb_get_abs(var_output)\n",
    "                \n",
    "                # make current shell list\n",
    "                current_shell_list = [\n",
    "                    modify_shell(template_list[i],-1,id_current)\n",
    "                    for i in range(len(template_list))\n",
    "                ]\n",
    "                \n",
    "                # wrap in B=1\n",
    "                if use_cuda:\n",
    "                    td_current = Variable(torch.tensor([map_current],dtype=torch.long)).cuda()\n",
    "                    td_output = Variable(torch.tensor([map_output],dtype=torch.long)).cuda()\n",
    "                else:\n",
    "                    td_current = Variable(torch.tensor([map_current],dtype=torch.long))\n",
    "                    td_output = Variable(torch.tensor([map_output],dtype=torch.long))\n",
    "                    \n",
    "                # (B=1, fn_vocab_size)\n",
    "                td_pred = test_model(td_current, td_output)\n",
    "                \n",
    "                # no hints\n",
    "                if random.random()<=p_config[\"meta_test\"][\"exploration_rate\"](d_episode,d_attempt):\n",
    "                    # exploration\n",
    "                    tmp_id = random.choice(range(len(current_shell_list)))\n",
    "                else:\n",
    "                    # exploitation\n",
    "                    tmp_id = torch.multinomial(td_pred.exp().flatten(), 1).cpu().flatten().numpy()[0]\n",
    "                \n",
    "                # update ps_current\n",
    "                update_status = ps_current.add_neighboring_shell(\n",
    "                    current_shell_list[tmp_id]\n",
    "                )\n",
    "                \n",
    "                if update_status:\n",
    "                    # record selected neuron\n",
    "                    selected_neurons.append(td_pred[0,tmp_id])\n",
    "                    d_step += 1\n",
    "                    \n",
    "                    # succeed\n",
    "                    if ps_current.check_eq() is not None:\n",
    "                        # and solved!\n",
    "                        is_solved = True\n",
    "                        n_solved += 1\n",
    "                        attempt_reward = 1.0\n",
    "                        break\n",
    "                else:\n",
    "                    # DeepPath: fail, add to dead list\n",
    "                    dead_neurons.append(td_pred[0,tmp_id])\n",
    "                    if len(dead_neurons)>p_config[\"meta_test\"][\"dp_cap\"]:\n",
    "                        # exceed the max capacity of dead pool\n",
    "                        break\n",
    "            \n",
    "            # <END_FOR_STEP>\n",
    "            \n",
    "            # check the attempt_reward\n",
    "            if attempt_reward is None:\n",
    "                # means either failure in execution or exceeding max_step\n",
    "                attempt_reward = -1.\n",
    "            \n",
    "            # compute the loss (sequential selected)\n",
    "            for i in range(len(selected_neurons)):\n",
    "                d_decay = p_config[\"meta_test\"][\"decay_rate\"]**(len(selected_neurons)-1-i)\n",
    "                batch_lossA_list.append(\n",
    "                    d_decay*attempt_reward*(-selected_neurons[i]) \n",
    "                )\n",
    "            \n",
    "            # compute the loss (dead neurons)\n",
    "            for i in range(len(dead_neurons)):\n",
    "                batch_lossD_list.append(\n",
    "                    (-1.)*(-dead_neurons[i])\n",
    "                )\n",
    "            \n",
    "            if is_solved or nth_attempt>=p_config[\"meta_test\"][\"batch_size\"]:\n",
    "                # directly do the back-prop\n",
    "                if len(batch_lossD_list)>0:\n",
    "                    batch_lossD = sum(batch_lossD_list)/len(batch_lossD_list)\n",
    "                    p_optim.zero_grad()\n",
    "                    batch_lossD.backward()\n",
    "                    p_optim.step()\n",
    "                \n",
    "                if len(batch_lossA_list)>0:\n",
    "                    batch_lossA = sum(batch_lossA_list)/len(batch_lossA_list)\n",
    "                    p_optim.zero_grad()\n",
    "                    batch_lossA.backward()\n",
    "                    p_optim.step()\n",
    "            \n",
    "                nth_attempt = 0\n",
    "                batch_lossA_list = []\n",
    "                batch_lossD_list = []\n",
    "                selected_neurons = []\n",
    "                dead_neurons = []\n",
    "                \n",
    "        # <END_FOR_ATTEMPT>     \n",
    "        \n",
    "        # after all the attempts\n",
    "        n_attempt_list.append(d_attempt)\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\n",
    "                'avg.attempt',\n",
    "                sum(n_attempt_list)/len(n_attempt_list) if len(n_attempt_list)>0 else 0,\n",
    "                len(n_attempt_list),\n",
    "            )\n",
    "            \n",
    "    # <END_FOR_EPISODE>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Meta-Train: 77 Tasks, 77000 Samples\n"
     ]
    }
   ],
   "source": [
    "m_interpreter = MorpheusInterpreter()\n",
    "m_spec = S.parse_file('./example/camb3.tyrell')\n",
    "m_generator = MorpheusGenerator(\n",
    "    spec=m_spec,\n",
    "    interpreter=m_interpreter,\n",
    ")\n",
    "\n",
    "# dumb variable to help infer the shells\n",
    "m_ps = ProgramSpace(\n",
    "    m_spec, m_interpreter, [None], None,\n",
    ")\n",
    "\n",
    "m_config = {\n",
    "    # ==== TransE Setting ==== #\n",
    "    \"val\":{\n",
    "        \"vocab_size\": len(m_interpreter.CAMB_LIST),\n",
    "        \"embd_dim\": 16, # embedding dim of CAMB abstract token\n",
    "        \"conv_n_kernels\": 512,\n",
    "        \"conv_kernel_size\": (1,m_interpreter.CAMB_NCOL), \n",
    "        \"pool_kernel_size\": (m_interpreter.CAMB_NROW,1), \n",
    "        \"IDX_PAD\": 0,\n",
    "    },\n",
    "    \"fn\":{\n",
    "        \"vocab_size\": len(m_ps.get_neighboring_shells())\n",
    "    },\n",
    "    \"embd_dim\": 128,\n",
    "    \"mte_path\": \"./saved_models/0712CAMB_TransE_camb3_ep{}.pt\".format(150),\n",
    "    # ==== Meta-Learning Setting ==== #\n",
    "    \"meta_train\":{\n",
    "        \"n_epoch\": 10,\n",
    "        \"n_task\": 10,\n",
    "        \"n_train\":100, # how many training samples in every meta task\n",
    "        \"n_test\":100,\n",
    "        \"batch_size\": 4,\n",
    "        \"data_path\": \"./0716MDsize1.pkl\",\n",
    "        \"inner_lr\": 0.01,\n",
    "    },\n",
    "    \"meta_test\":{\n",
    "        \"n_episode\": 100000,\n",
    "        \"batch_size\": 1, # how many attempts\n",
    "        \"fixed_depth\": 3,\n",
    "        \"maxn_attempt\": 100,\n",
    "        \"maxn_step\": 2, # program size\n",
    "        \"exploration_rate\": lambda pep,pat:0.1,\n",
    "        \"decay_rate\": 0.9,\n",
    "        \"dp_cap\": 50,\n",
    "    },\n",
    "}\n",
    "\n",
    "# load the size 1 supervised data\n",
    "with open(m_config[\"meta_train\"][\"data_path\"],\"rb\") as f:\n",
    "    dt_data = pickle.load(f)\n",
    "# re-distribute the meta-train data\n",
    "m_data = {}\n",
    "for dkey in dt_data.keys():\n",
    "    if len(dt_data[dkey])<m_config[\"meta_train\"][\"n_train\"]+m_config[\"meta_train\"][\"n_test\"]:\n",
    "        # not enough samples, skip\n",
    "        continue\n",
    "    tmp_list = list(range(len(dt_data[dkey])))\n",
    "    tmp_half = int(len(dt_data[dkey])/2)\n",
    "    tmp_tr = [dt_data[dkey][i] for i in tmp_list[:tmp_half]]\n",
    "    tmp_te = [dt_data[dkey][i] for i in tmp_list[tmp_half:]]\n",
    "    m_data[dkey] = [tmp_tr,tmp_te]\n",
    "\n",
    "print(\"# Meta-Train: {} Tasks, {} Samples\".format(\n",
    "    len(m_data), sum([len(m_data[dkey][0])+len(m_data[dkey][1]) for dkey in m_data.keys()])\n",
    "))\n",
    "\n",
    "trans_neo = TransNeo(p_config=m_config)\n",
    "if use_cuda:\n",
    "    trans_neo = trans_neo.cuda()\n",
    "optimizer = torch.optim.Adam(list(trans_neo.parameters()))\n",
    "\n",
    "# writer = SummaryWriter(\"runs/0713CAMB_RL2_camb3\")\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': {'vocab_size': 150,\n",
       "  'embd_dim': 16,\n",
       "  'conv_n_kernels': 512,\n",
       "  'conv_kernel_size': (1, 15),\n",
       "  'pool_kernel_size': (15, 1),\n",
       "  'IDX_PAD': 0},\n",
       " 'fn': {'vocab_size': 120},\n",
       " 'embd_dim': 128,\n",
       " 'mte_path': './saved_models/0712CAMB_TransE_camb3_ep150.pt',\n",
       " 'meta_train': {'n_epoch': 10,\n",
       "  'n_task': 10,\n",
       "  'n_train': 100,\n",
       "  'n_test': 100,\n",
       "  'batch_size': 4,\n",
       "  'data_path': './0716MDsize1.pkl',\n",
       "  'inner_lr': 0.01},\n",
       " 'meta_test': {'n_episode': 100000,\n",
       "  'batch_size': 1,\n",
       "  'fixed_depth': 3,\n",
       "  'maxn_attempt': 100,\n",
       "  'maxn_step': 2,\n",
       "  'exploration_rate': <function __main__.<lambda>(pep, pat)>,\n",
       "  'decay_rate': 0.9,\n",
       "  'dp_cap': 50}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Start MAML Meta-Train...\n",
      "# EP:0, Task:9, Train:99, Ep.Loss:33.79\n",
      "# EP:1, Task:9, Train:99, Ep.Loss:33.23\n",
      "# EP:2, Task:9, Train:99, Ep.Loss:30.35\n",
      "# EP:3, Task:9, Train:99, Ep.Loss:26.67\n",
      "# EP:4, Task:9, Train:99, Ep.Loss:25.68\n",
      "# EP:5, Task:9, Train:99, Ep.Loss:29.14\n",
      "# EP:6, Task:9, Train:99, Ep.Loss:21.13\n",
      "# EP:7, Task:9, Train:99, Ep.Loss:21.54\n",
      "# EP:8, Task:9, Train:99, Ep.Loss:13.55\n",
      "# EP:9, Task:9, Train:99, Ep.Loss:23.03\n"
     ]
    }
   ],
   "source": [
    "MetaTrain(m_config, m_spec, m_interpreter, trans_neo, m_data, optimizer, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Start Meta-Test...\n",
      "# AC/EP:35/297, AT:22, SP:0, DN:0, avg.attempt:91.97, er:0.10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function SexpCapsule.__del__ at 0x7f978d835400>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/rpy2/rinterface_lib/_rinterface_capi.py\", line 100, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# AC/EP:35/297, AT:22, SP:1, DN:17, avg.attempt:91.97, er:0.10"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cdc6b9d424ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMetaTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_interpreter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_neo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-9da64eeae46a>\u001b[0m in \u001b[0;36mMetaTest\u001b[0;34m(p_config, p_spec, p_interpreter, p_generator, p_model, p_optim, p_writer)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     \u001b[0mbatch_lossD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_lossD_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_lossD_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mp_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                     \u001b[0mbatch_lossD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                     \u001b[0mp_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MetaTest(m_config, m_spec, m_interpreter, m_generator, trans_neo, optimizer, writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
