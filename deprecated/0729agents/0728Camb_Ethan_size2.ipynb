{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MetaNeo\n",
    "- a version using meta information only, while excluding value-based features\n",
    "- Stage: Cambrian\n",
    "- Version: Yorgia\n",
    "- Update Logs\n",
    "    - 0713: with DeepPath style rollback at training\n",
    "    - 0716: new learning paradigm, see memo for details\n",
    "    - 0726: this is a interactive debugging version, with dead pool deactivated\n",
    "\n",
    "#### Related Commands\n",
    "- tensorboard --logdir runs\n",
    "- nohup jupyter lab > jupyter.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tyrell.spec as S\n",
    "from tyrell.decider import Example\n",
    "\n",
    "# Morpheus Version\n",
    "from MorpheusInterpreter import *\n",
    "from ProgramSpace import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaNeo(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(MetaNeo, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        # predict a fixed number of shells\n",
    "#         self.policy = nn.Linear(\n",
    "#             self.config[\"embd_dim\"],\n",
    "#             self.config[\"fn\"][\"vocab_size\"],\n",
    "#         )\n",
    "        \n",
    "        # deeper\n",
    "        self.policy0 = nn.Linear(\n",
    "            self.config[\"embd_dim\"],\n",
    "            128,\n",
    "        )\n",
    "        self.policy1 = nn.Linear(\n",
    "            128,\n",
    "            self.config[\"fn\"][\"vocab_size\"],\n",
    "        )\n",
    "        \n",
    "    def forward(self, p_mapin, p_mapout):\n",
    "        # p_mapin/p_mapout: (B, 15*3)\n",
    "        v_delta = p_mapout-p_mapin\n",
    "#         tmp_out = torch.log_softmax(\n",
    "#             self.policy(v_delta),dim=1\n",
    "#         )\n",
    "        tmp_out = torch.log_softmax(\n",
    "            self.policy1(\n",
    "                F.relu(\n",
    "                    self.policy0(\n",
    "                        v_delta\n",
    "                    )\n",
    "                )\n",
    "            ),dim=1\n",
    "        )\n",
    "        \n",
    "        return tmp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace certain node id with certain value\n",
    "def modify_shell(p_shell, p_id_from, p_id_to):\n",
    "    d_prod = p_shell[0]\n",
    "    d_rhs = p_shell[1]\n",
    "    ld_rhs = [p_id_to if d_rhs[i]==p_id_from else d_rhs[i]\n",
    "             for i in range(len(d_rhs))]\n",
    "    return (d_prod, tuple(ld_rhs))\n",
    "\n",
    "\n",
    "# '''\n",
    "# meta-train the agent in a supervised way\n",
    "# epoch -> episode, one attempt with hint\n",
    "# NOTICE: only valid for size 1 training\n",
    "# '''\n",
    "# def MetaTrain(p_config, p_spec, p_interpreter, p_model, p_data, p_optim, p_writer):\n",
    "#     print(\"# Start Meta-Train...\")\n",
    "#     for d_epoch in range(p_config[\"meta_train\"][\"n_epoch\"]):\n",
    "#         p_model.train()\n",
    "        \n",
    "#         epoch_loss_list = []\n",
    "#         batch_loss_list = []\n",
    "#         random.shuffle(p_data)\n",
    "#         train_data = p_data[:p_config[\"meta_train\"][\"n_truncated\"]]\n",
    "        \n",
    "#         for d_ind in range(len(train_data)):\n",
    "#             print(\"\\r# epoch:{}, index:{}/{}, avg.loss:{:.2f}\".format(\n",
    "#                 d_epoch, d_ind, len(train_data),\n",
    "#                 sum(epoch_loss_list)/len(epoch_loss_list)\n",
    "#                 if len(epoch_loss_list)>0 else 0,\n",
    "#             ),end=\"\")\n",
    "#             d_prog, dstr_example = train_data[d_ind]\n",
    "#             d_example = Example(\n",
    "#                 input=[\n",
    "#                     p_interpreter.load_data_into_var(p)\n",
    "#                     for p in dstr_example.input\n",
    "#                 ],\n",
    "#                 output=p_interpreter.load_data_into_var(\n",
    "#                     dstr_example.output\n",
    "#                 )\n",
    "#             )\n",
    "            \n",
    "#             # initialize a solution\n",
    "#             ps_solution = ProgramSpace(\n",
    "#                 p_spec, p_interpreter, d_example.input, d_example.output,\n",
    "#             )\n",
    "#             ps_solution.init_by_prog(d_prog) # this constructs a solution for this problem\n",
    "            \n",
    "#             # initialize a new ProgramSpace\n",
    "#             ps_current = ProgramSpace(\n",
    "#                 p_spec, p_interpreter, d_example.input, d_example.output,\n",
    "#             )\n",
    "#             # then initialize a shell template\n",
    "#             tmp_shell_list = ps_current.get_neighboring_shells()\n",
    "#             tmp_node_to_replace = ps_current.node_dict[\"ParamNode\"][0] # for chain only\n",
    "#             # replace the Param Node id in shells with -1 to make them templates\n",
    "#             template_list = [\n",
    "#                 modify_shell(tmp_shell_list[i],tmp_node_to_replace,-1)\n",
    "#                 for i in range(len(tmp_shell_list))\n",
    "#             ]\n",
    "            \n",
    "#             id_current = ps_current.get_strict_frontiers()[0]\n",
    "#             var_current = ps_current.node_list[id_current].ps_data # need the real var name in r env\n",
    "#             var_output = d_example.output\n",
    "            \n",
    "#             map_current = p_interpreter.camb_get_simp_abs(var_current)\n",
    "#             map_output = p_interpreter.camb_get_simp_abs(var_output)\n",
    "            \n",
    "#             # make current shell list\n",
    "#             current_shell_list = [\n",
    "#                 modify_shell(template_list[i],-1,id_current)\n",
    "#                 for i in range(len(template_list))\n",
    "#             ]\n",
    "            \n",
    "#             # wrap in B=1\n",
    "#             if use_cuda:\n",
    "#                 td_current = Variable(torch.tensor([map_current],dtype=torch.float)).cuda()\n",
    "#                 td_output = Variable(torch.tensor([map_output],dtype=torch.float)).cuda()\n",
    "#             else:\n",
    "#                 td_current = Variable(torch.tensor([map_current],dtype=torch.float))\n",
    "#                 td_output = Variable(torch.tensor([map_output],dtype=torch.float))\n",
    "\n",
    "#             # (B=1, fn_vocab_size)\n",
    "#             td_pred = p_model(td_current, td_output)\n",
    "#             # directly give the hint / supervised, ps.solution.shell[0] works for 1\n",
    "#             tmp_id = current_shell_list.index(ps_solution.shells[0])\n",
    "#             d_loss = (+1)*(-td_pred[0,tmp_id])\n",
    "#             batch_loss_list.append(\n",
    "#                 d_loss, # supervised / always correct with +1 reward\n",
    "#             )\n",
    "#             epoch_loss_list.append(\n",
    "#                 d_loss.cpu().data.numpy(),\n",
    "#             )\n",
    "            \n",
    "#             if len(batch_loss_list)%p_config[\"meta_train\"][\"batch_size\"]==0 or len(batch_loss_list)==len(train_data):\n",
    "#                 # do back-prop.\n",
    "#                 if len(batch_loss_list)>0:\n",
    "#                     batch_loss = sum(batch_loss_list)/len(batch_loss_list)\n",
    "#                     p_optim.zero_grad()\n",
    "#                     batch_loss.backward()\n",
    "#                     p_optim.step()\n",
    "#                 # after back-prop., clean up\n",
    "#                 batch_loss = None\n",
    "#                 batch_loss_list = []\n",
    "                \n",
    "#         print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sketch(ps0,ps1):\n",
    "    if len(ps0.node_list)!=len(ps1.node_list):\n",
    "        return False\n",
    "    for i in range(len(ps0.shells)):\n",
    "        if ps0.node_list[-i-1].name != ps1.node_list[-i-1].name:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "'''\n",
    "meta-test an agent, directly run into testing / online adaptation\n",
    "'''\n",
    "def MetaTest(p_config, p_spec, p_interpreter, p_generator, p_model, p_optim, p_writer):\n",
    "    print(\"# Start Meta-Test...\")\n",
    "    \n",
    "    nth_attempt = 0 # tell whether to back-prop or not\n",
    "    batch_loss_list = []\n",
    "    \n",
    "    n_solved = 0 # track the number of solved problem\n",
    "    n_sketch_solved = 0\n",
    "    n_attempt_list = [] # track the number of attempts in every episode\n",
    "    \n",
    "    stored_neurons = []\n",
    "    stored_nodes = []\n",
    "    stored_groups = [] # with lists of neurons of the same production name\n",
    "    \n",
    "    for d_episode in range(p_config[\"meta_test\"][\"n_episode\"]):\n",
    "        \n",
    "        # retrieve the given meta-trained model for testing\n",
    "        test_model = copy.deepcopy(p_model)\n",
    "        test_model.train()\n",
    "        \n",
    "        # if doing random meta-testing\n",
    "        # then randomly generate a program for testing\n",
    "        ps_solution = p_generator.get_new_chain_program(\n",
    "            p_config[\"meta_test\"][\"fixed_depth\"],\n",
    "        )\n",
    "        print(\"# benchmark program: {}\".format(\n",
    "            \" -> \".join(\n",
    "                [\n",
    "                    str(ps_solution.node_list[-p_config[\"meta_test\"][\"maxn_step\"]+i]).replace(str(ps_solution.node_list[-p_config[\"meta_test\"][\"maxn_step\"]+i-1]),\"@output\") if i>=1 else str(ps_solution.node_list[-p_config[\"meta_test\"][\"maxn_step\"]+i])\n",
    "                    for i in range(p_config[\"meta_test\"][\"maxn_step\"])\n",
    "                ]\n",
    "            )\n",
    "        ))\n",
    "        print(\"# === input ===\")\n",
    "        print(p_interpreter.renv(ps_solution.inputs[0]))\n",
    "        print(\"# === output ===\")\n",
    "        print(p_interpreter.renv(ps_solution.output))\n",
    "        \n",
    "        is_solved = False\n",
    "        is_sketch_solved = False\n",
    "        \n",
    "        for d_attempt in range(p_config[\"meta_test\"][\"maxn_attempt\"]):\n",
    "            if is_solved:\n",
    "                # already solved in the last attempt, stop\n",
    "                break\n",
    "            \n",
    "            nth_attempt += 1\n",
    "            attempt_reward = None\n",
    "            \n",
    "            # in every new attempt, initialize a new Program Space\n",
    "            ps_current = ProgramSpace(\n",
    "                p_spec, p_interpreter, ps_solution.inputs, ps_solution.output,\n",
    "            )\n",
    "            # then initialize a shell template\n",
    "            tmp_shell_list = ps_current.get_neighboring_shells()\n",
    "            tmp_node_to_replace = ps_current.node_dict[\"ParamNode\"][0] # for chain only\n",
    "            # replace the Param Node id in shells with -1 to make them templates\n",
    "            template_list = [\n",
    "                modify_shell(tmp_shell_list[i],tmp_node_to_replace,-1)\n",
    "                for i in range(len(tmp_shell_list))\n",
    "            ]\n",
    "                \n",
    "            d_step = 0\n",
    "            while d_step<p_config[\"meta_test\"][\"maxn_step\"]:\n",
    "                \n",
    "#                 # print the training progress\n",
    "#                 print(\"\\r# AC/SK/EP:{}/{}/{}, AT:{}, SP:{}, DN:{}, avg.attempt:{:.2f}, er:{:.2f}\".format(\n",
    "#                     n_solved, n_sketch_solved, d_episode, d_attempt, d_step, \n",
    "#                     len(dead_neurons),\n",
    "#                     sum(n_attempt_list)/len(n_attempt_list) if len(n_attempt_list)>0 else -1,\n",
    "#                     p_config[\"meta_test\"][\"exploration_rate\"](d_episode,d_attempt),\n",
    "#                 ),end=\"\")\n",
    "                \n",
    "                # ### assume chain execution, so only 1 possible returns\n",
    "                # ### at d_step=0, this should be input[0]\n",
    "                id_current = ps_current.get_strict_frontiers()[0]\n",
    "                var_current = ps_current.node_list[id_current].ps_data # need the real var name in r env\n",
    "                var_output = ps_current.output\n",
    "                \n",
    "                map_current = p_interpreter.camb_get_simp_abs(var_current)\n",
    "                map_output = p_interpreter.camb_get_simp_abs(var_output)\n",
    "                \n",
    "                # make current shell list\n",
    "                current_shell_list = [\n",
    "                    modify_shell(template_list[i],-1,id_current)\n",
    "                    for i in range(len(template_list))\n",
    "                ]\n",
    "                \n",
    "                # wrap in B=1\n",
    "                if use_cuda:\n",
    "                    td_current = Variable(torch.tensor([map_current],dtype=torch.float)).cuda()\n",
    "                    td_output = Variable(torch.tensor([map_output],dtype=torch.float)).cuda()\n",
    "                else:\n",
    "                    td_current = Variable(torch.tensor([map_current],dtype=torch.float))\n",
    "                    td_output = Variable(torch.tensor([map_output],dtype=torch.float))\n",
    "                    \n",
    "                # (B=1, fn_vocab_size)\n",
    "                td_pred = test_model(td_current, td_output)\n",
    "                \n",
    "                # no hints\n",
    "                if random.random()<=p_config[\"meta_test\"][\"exploration_rate\"](d_episode,d_attempt):\n",
    "                    # exploration\n",
    "                    tmp_id = random.choice(range(len(current_shell_list)))\n",
    "                else:\n",
    "                    # exploitation\n",
    "                    tmp_id = torch.multinomial(td_pred.exp().flatten(), 1).cpu().flatten().numpy()[0]\n",
    "                \n",
    "                # == Yorgia ==\n",
    "                # find out all other shells that share the same product name\n",
    "                tmp_component_name = ps_current.prod_list[current_shell_list[tmp_id][0]].name\n",
    "                tmp_group = []\n",
    "                for i in range(len(current_shell_list)):\n",
    "                    if ps_current.prod_list[current_shell_list[i][0]].name==tmp_component_name:\n",
    "                        tmp_group.append(td_pred[0,i])\n",
    "                stored_groups.append(tmp_group)\n",
    "                \n",
    "                # == Yorgia ==\n",
    "                # append before adding shell to ProgramSpace\n",
    "                stored_nodes.append(ps_current.get_node_from_shell(\n",
    "                    current_shell_list[tmp_id]\n",
    "                ))\n",
    "                \n",
    "                # update ps_current\n",
    "                update_status = ps_current.add_neighboring_shell(\n",
    "                    current_shell_list[tmp_id]\n",
    "                )\n",
    "                \n",
    "                if update_status:\n",
    "                    # record selected neuron\n",
    "                    stored_neurons.append(\n",
    "                        (td_pred[0,tmp_id], True)\n",
    "                    )\n",
    "                    d_step += 1\n",
    "                    \n",
    "                    # succeed\n",
    "                    if ps_current.check_eq() is not None:\n",
    "                        # and solved!\n",
    "                        is_solved = True\n",
    "                        n_solved += 1\n",
    "                        break\n",
    "                else:\n",
    "                    stored_neurons.append(\n",
    "                        (td_pred[0,tmp_id], False)\n",
    "                    )\n",
    "                    break\n",
    "            \n",
    "            \n",
    "            if not is_sketch_solved:\n",
    "                if compare_sketch(ps_current, ps_solution):\n",
    "                    is_sketch_solved = True\n",
    "                    n_sketch_solved += 1\n",
    "                    \n",
    "            print(\"# stored groups: {}\".format(\n",
    "                \" / \".join([\n",
    "                    str(len(stored_groups[i])) for i in range(len(stored_groups))\n",
    "                ])\n",
    "            ))\n",
    "            \n",
    "            # ask for separate rewards for every step\n",
    "            ar = input(\"# attempt {}, input reward(s) for: {}\".format(\n",
    "                nth_attempt,\n",
    "                \" -> \".join([\n",
    "                    \"({}){}\".format(\n",
    "                        \"✓\" if stored_neurons[i][1] else \"x\",\n",
    "                        str(stored_nodes[i]).replace(str(stored_nodes[i-1]),\"@output\") if i>=1 else str(stored_nodes[i]),\n",
    "                    ) for i in range(len(stored_nodes))\n",
    "                ]),\n",
    "            ))\n",
    "            assigned_rewards = eval(\"[{}]\".format(ar))\n",
    "                \n",
    "            \n",
    "            # compute the loss (sequential selected)\n",
    "#             for i in range(len(stored_neurons)):\n",
    "#                 batch_loss_list.append(\n",
    "#                     assigned_rewards[i]*(-stored_neurons[i][0]) \n",
    "#                 )\n",
    "            # == Yorgia ==\n",
    "            # compute grouped loss\n",
    "            for i in range(len(stored_groups)):\n",
    "                for j in range(len(stored_groups[i])):\n",
    "                    batch_loss_list.append(\n",
    "                        assigned_rewards[i]*(-stored_groups[i][j])\n",
    "                    )\n",
    "            \n",
    "            # directly do the back-prop\n",
    "            batch_loss = sum(batch_loss_list)/len(batch_loss_list)\n",
    "            p_optim.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            p_optim.step()\n",
    "            \n",
    "            batch_loss_list = []\n",
    "            stored_neurons = []\n",
    "            stored_nodes = []\n",
    "            stored_groups = []\n",
    "            \n",
    "            if is_solved:\n",
    "                nth_attempt = 0\n",
    "                break\n",
    "                \n",
    "        # <END_FOR_ATTEMPT> \n",
    "        \n",
    "            \n",
    "    # <END_FOR_EPISODE>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_interpreter = MorpheusInterpreter()\n",
    "m_spec = S.parse_file('./example/camb3.tyrell')\n",
    "m_generator = MorpheusGenerator(\n",
    "    spec=m_spec,\n",
    "    interpreter=m_interpreter,\n",
    ")\n",
    "\n",
    "# dumb variable to help infer the shells\n",
    "m_ps = ProgramSpace(\n",
    "    m_spec, m_interpreter, [None], None,\n",
    ")\n",
    "\n",
    "m_config = {\n",
    "    \"fn\":{\n",
    "        \"vocab_size\": len(m_ps.get_neighboring_shells())\n",
    "    },\n",
    "    \"embd_dim\": 15*3,\n",
    "    # ==== Meta-Learning Setting ==== #\n",
    "#     \"meta_train\":{\n",
    "#         \"n_epoch\": 10,\n",
    "#         \"batch_size\": 4, # how many indices\n",
    "#         \"data_path\": \"./0716MDsize1.pkl\",\n",
    "#         \"n_truncated\": 1000,\n",
    "#     },\n",
    "    \"meta_test\":{\n",
    "        \"n_episode\": 100000,\n",
    "        \"batch_size\": 1, # how many attempts\n",
    "        \"fixed_depth\": 3,\n",
    "        \"maxn_attempt\": 1000000,\n",
    "        \"maxn_step\": 2, # program size\n",
    "        \"exploration_rate\": lambda pep,pat:0.1,\n",
    "        \"decay_rate\": 0.9,\n",
    "        \"dp_cap\": 50,\n",
    "    },\n",
    "}\n",
    "\n",
    "# load the size 1 supervised data\n",
    "# with open(m_config[\"meta_train\"][\"data_path\"],\"rb\") as f:\n",
    "#     dt_data = pickle.load(f)\n",
    "# m_data = [\n",
    "#     dt_data[dkey][i]\n",
    "#     for dkey in dt_data.keys()\n",
    "#     for i in range(len(dt_data[dkey]))\n",
    "# ]\n",
    "# print(\"# Total Meta-Train Data: {}\".format(len(m_data)))\n",
    "\n",
    "meta_neo = MetaNeo(p_config=m_config)\n",
    "if use_cuda:\n",
    "    meta_neo = meta_neo.cuda()\n",
    "optimizer = torch.optim.Adam(list(meta_neo.parameters()))\n",
    "\n",
    "# writer = SummaryWriter(\"runs/0713CAMB_RL2_camb3\")\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fn': {'vocab_size': 120},\n",
       " 'embd_dim': 45,\n",
       " 'meta_test': {'n_episode': 100000,\n",
       "  'batch_size': 1,\n",
       "  'fixed_depth': 3,\n",
       "  'maxn_attempt': 1000000,\n",
       "  'maxn_step': 2,\n",
       "  'exploration_rate': <function __main__.<lambda>(pep, pat)>,\n",
       "  'decay_rate': 0.9,\n",
       "  'dp_cap': 50}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MetaTrain(m_config, m_spec, m_interpreter, meta_neo, m_data, optimizer, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Start Meta-Test...\n",
      "# benchmark program: separate(@param0, 2) -> gather(@output, ['2', '4'])\n",
      "# === input ===\n",
      "                     atomisation                   dubby     capitellate\n",
      "1        stormwise_inextinguible  teenage_trionychoidean       subcostae\n",
      "2         virtualize_hematoplast         skittering_week       subcostae\n",
      "3          gonycampsis_uncoupler      undiverse_sahaptin       subcostae\n",
      "4              deliver_heliotype    coonroot_prayerfully unhideboundness\n",
      "5           opaloid_amniochorial          tetchily_anorn unhideboundness\n",
      "6    philogynous_epigrammatarian            usheen_snaky       subcostae\n",
      "7             voltairean_mobster        zootrophic_fatwa       subcostae\n",
      "8           parahopeite_banderol  defectoscope_fratority      stewarding\n",
      "9      preindicating_katabothron lutecia_overmelodiously      stewarding\n",
      "10 unspiritualizing_bacilligenic     primrosed_refilming      stewarding\n",
      "\n",
      "# === output ===\n",
      "                     atomisation        beauship          sauve       minitrack\n",
      "1        stormwise_inextinguible  trionychoidean anticapitalism         teenage\n",
      "2         virtualize_hematoplast            week anticapitalism      skittering\n",
      "3          gonycampsis_uncoupler        sahaptin anticapitalism       undiverse\n",
      "4              deliver_heliotype     prayerfully anticapitalism        coonroot\n",
      "5           opaloid_amniochorial           anorn anticapitalism        tetchily\n",
      "6    philogynous_epigrammatarian           snaky anticapitalism          usheen\n",
      "7             voltairean_mobster           fatwa anticapitalism      zootrophic\n",
      "8           parahopeite_banderol       fratority anticapitalism    defectoscope\n",
      "9      preindicating_katabothron overmelodiously anticapitalism         lutecia\n",
      "10 unspiritualizing_bacilligenic       refilming anticapitalism       primrosed\n",
      "11       stormwise_inextinguible  trionychoidean    capitellate       subcostae\n",
      "12        virtualize_hematoplast            week    capitellate       subcostae\n",
      "13         gonycampsis_uncoupler        sahaptin    capitellate       subcostae\n",
      "14             deliver_heliotype     prayerfully    capitellate unhideboundness\n",
      "15          opaloid_amniochorial           anorn    capitellate unhideboundness\n",
      "16   philogynous_epigrammatarian           snaky    capitellate       subcostae\n",
      "17            voltairean_mobster           fatwa    capitellate       subcostae\n",
      "18          parahopeite_banderol       fratority    capitellate      stewarding\n",
      "19     preindicating_katabothron overmelodiously    capitellate      stewarding\n",
      "20 unspiritualizing_bacilligenic       refilming    capitellate      stewarding\n",
      "\n",
      "# stored groups: 6\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 1, input reward(s) for: (x)separate(@param0, 6) 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 36\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 2, input reward(s) for: (x)spread(@param0, 5, 6) -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 21 / 36\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 3, input reward(s) for: (✓)gather(@param0, ['2', '3']) -> (x)spread(@output, 1, 4) -1,-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 36\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 4, input reward(s) for: (x)spread(@param0, 5, 5) -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 36\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 5, input reward(s) for: (x)spread(@param0, 5, 3) -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 36\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 6, input reward(s) for: (x)unite(@param0, 2, 6) -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 21\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 7, input reward(s) for: (x)gather(@param0, ['4', '5']) -10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 36\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 8, input reward(s) for: (x)spread(@param0, 3, 5) -10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 36\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 9, input reward(s) for: (x)spread(@param0, 4, 1) -10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 36\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 10, input reward(s) for: (x)spread(@param0, 6, 2) -10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 36 / 21\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 11, input reward(s) for: (✓)unite(@param0, 3, 1) -> (x)gather(@output, ['4', '6']) -10,1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 36\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 12, input reward(s) for: (x)spread(@param0, 4, 4) -10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 36 / 21\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 13, input reward(s) for: (✓)unite(@param0, 3, 1) -> (x)select(@output, ['5']) -10,-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 6\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 14, input reward(s) for: (x)separate(@param0, 4) -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 21\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 15, input reward(s) for: (x)gather(@param0, ['2', '4']) -10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 6\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 16, input reward(s) for: (x)separate(@param0, 5) -10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 21\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 17, input reward(s) for: (x)select(@param0, ['6']) -10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 36\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 18, input reward(s) for: (x)spread(@param0, 5, 6) -10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 36\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 19, input reward(s) for: (x)unite(@param0, 2, 6) -10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 6\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 20, input reward(s) for: (x)separate(@param0, 4) -10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 36\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "# attempt 21, input reward(s) for: (x)spread(@param0, 5, 1) -10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stored groups: 36\n"
     ]
    }
   ],
   "source": [
    "MetaTest(m_config, m_spec, m_interpreter, m_generator, meta_neo, optimizer, writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
