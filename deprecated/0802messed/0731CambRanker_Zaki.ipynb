{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MetaNeo Ranker\n",
    "- tell which pair of tables are functionally/logically closer on execution chain\n",
    "- Stage: Cambrian\n",
    "- Version: Yorgia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tyrell.spec as S\n",
    "from tyrell.decider import Example\n",
    "\n",
    "# Morpheus Version\n",
    "from MorpheusInterpreter import *\n",
    "from ProgramSpace import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dataset for training: load random positive/negative examples every time\n",
    "'''\n",
    "class RankerDataset(Dataset):\n",
    "    def __init__(self, p_config=None, p_dataset=None, p_interpreter=None, p_spec=None):\n",
    "        self.n_sample = None # should manually assign\n",
    "        self.interpreter = p_interpreter\n",
    "        self.spec = p_spec\n",
    "        self.dataset = p_dataset\n",
    "        self.config = p_config\n",
    "        \n",
    "        # record all possible programs\n",
    "        self.progs = [\n",
    "            self.dataset[dkey][0][0]\n",
    "            for dkey in self.dataset.keys()\n",
    "        ]\n",
    "        \n",
    "        self.n_exp = len(self.progs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_sample\n",
    "    \n",
    "    '''\n",
    "    return a single triangle sample\n",
    "    (input, output1, output2, random)\n",
    "    A->B, A->C, D\n",
    "    pos: (A,B), (A,C)\n",
    "    neg: (B,A), (C,A)\n",
    "    neg: (B,C), (C,B)\n",
    "    neg: (A,D), (B,D), (C,D)\n",
    "    neg: (D,A), (D,B), (D,C)\n",
    "    '''\n",
    "    def get_triangle(self):\n",
    "        # sample (A,B)\n",
    "        while True:\n",
    "            d_progAB = random.choice(self.progs)\n",
    "            d_inputA = self.interpreter.random_table()\n",
    "            try:\n",
    "                d_evalB = self.interpreter.eval(\n",
    "                    d_progAB,\n",
    "                    [d_inputA],\n",
    "                )\n",
    "            except Exception:\n",
    "                continue\n",
    "            d_exampleAB = Example(\n",
    "                input=[d_inputA],\n",
    "                output=d_evalB,\n",
    "            )\n",
    "            d_psAB = ProgramSpace(\n",
    "                self.spec, self.interpreter,\n",
    "                d_exampleAB.input, d_exampleAB.output,\n",
    "            )\n",
    "            d_psAB.init_by_prog(d_progAB)\n",
    "            d_checkAB = self.interpreter.sanity_check(d_psAB)\n",
    "            if d_checkAB[0]:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # sample (A,C)\n",
    "        while True:\n",
    "            d_progAC = random.choice(self.progs)\n",
    "            if d_progAC==d_progAB:\n",
    "                continue\n",
    "            try:\n",
    "                d_evalC = self.interpreter.eval(\n",
    "                    d_progAC,\n",
    "                    [d_inputA],\n",
    "                )\n",
    "            except Exception:\n",
    "                continue\n",
    "            d_exampleAC = Example(\n",
    "                input=[d_inputA],\n",
    "                output=d_evalC,\n",
    "            )\n",
    "            d_psAC = ProgramSpace(\n",
    "                self.spec, self.interpreter,\n",
    "                d_exampleAC.input, d_exampleAC.output,\n",
    "            )\n",
    "            d_psAC.init_by_prog(d_progAC)\n",
    "            d_checkAC = self.interpreter.sanity_check(d_psAC)\n",
    "            if d_checkAC[0]:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # then randomly generate a table D\n",
    "        d_randD = self.interpreter.random_table()\n",
    "        \n",
    "        # all (map_r, map_c)\n",
    "        map_inputA = numpy.array([self.interpreter.camb_get_yorgia(d_inputA,modn=self.config[\"vocab_size\"])])\n",
    "        map_evalB = numpy.array([self.interpreter.camb_get_yorgia(d_evalB,modn=self.config[\"vocab_size\"])])\n",
    "        map_evalC = numpy.array([self.interpreter.camb_get_yorgia(d_evalC,modn=self.config[\"vocab_size\"])])\n",
    "        map_randD = numpy.array([self.interpreter.camb_get_yorgia(d_randD,modn=self.config[\"vocab_size\"])])\n",
    "        \n",
    "        return (map_inputA, map_evalB, map_evalC, map_randD)\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    should always use batch_size=1 so as to ensure the ratio of negative examples\n",
    "    '''\n",
    "    def __getitem__(self, p_ind):\n",
    "        return self.get_triangle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ranker(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(Ranker, self).__init__()\n",
    "        self.config = p_config\n",
    "        self.value_embedding = nn.Embedding(\n",
    "            self.config[\"vocab_size\"],\n",
    "            self.config[\"embd_dim\"],\n",
    "        )\n",
    "        self.fc0 = nn.Linear(\n",
    "            self.config[\"embd_dim\"] * self.config[\"max_length\"] * 2,\n",
    "            4096,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(\n",
    "            4096,\n",
    "            2048,\n",
    "        )\n",
    "        self.fc2 = nn.Linear(\n",
    "            2048,\n",
    "            128\n",
    "        )\n",
    "        self.fc3 = nn.Linear(\n",
    "            128,\n",
    "            2\n",
    "        )\n",
    "        \n",
    "    def forward(self, pA, pB, pC, pD):\n",
    "        # pA/pB/pC/pD: (B=1, seq)\n",
    "        B = pA.shape[0]\n",
    "        \n",
    "        # (B, seq, embd_dim)\n",
    "        vA = self.value_embedding(pA).view(B, self.config[\"max_length\"]*self.config[\"embd_dim\"])\n",
    "        vB = self.value_embedding(pB).view(B, self.config[\"max_length\"]*self.config[\"embd_dim\"])\n",
    "        vC = self.value_embedding(pC).view(B, self.config[\"max_length\"]*self.config[\"embd_dim\"])\n",
    "        vD = self.value_embedding(pD).view(B, self.config[\"max_length\"]*self.config[\"embd_dim\"])\n",
    "        \n",
    "        # (B=2, embd_dim * 2)\n",
    "        vPOS = torch.cat(\n",
    "            [\n",
    "                torch.cat([vA,vB],dim=1),\n",
    "                torch.cat([vA,vC],dim=1),\n",
    "            ],dim=0\n",
    "        )\n",
    "        \n",
    "        # (B=10, embd_dim * 2)\n",
    "        vNEG = torch.cat(\n",
    "            [\n",
    "                torch.cat([vB,vA],dim=1),\n",
    "                torch.cat([vB,vC],dim=1),\n",
    "                torch.cat([vC,vA],dim=1),\n",
    "                torch.cat([vC,vB],dim=1),\n",
    "            ],dim=0\n",
    "        )\n",
    "        \n",
    "        # (B=4*3=12, embd_dim * 2)\n",
    "        vII = torch.cat([vPOS,vNEG],dim=0)\n",
    "        \n",
    "        v00 = F.relu(self.fc0(vII))\n",
    "        v01 = F.relu(self.fc1(v00))\n",
    "        v02 = F.relu(self.fc2(v01))\n",
    "        v03 = self.fc3(v02)\n",
    "        # == Notice: don't do any activation at the last layer ==\n",
    "        # (B=4*3=12, 2)\n",
    "        \n",
    "        return v03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RankerTester(p_config, p_model, pld_test, p_lossfn):\n",
    "    test_loss_list = []\n",
    "    test_pPOS_list = []\n",
    "    test_pNEG_list = []\n",
    "    test_aPOS_list = []\n",
    "    test_aNEG_list = []\n",
    "    test_sPOS_list = []\n",
    "    test_sNEG_list = []\n",
    "    for batch_idx, (dA, dB, dC, dD) in enumerate(pld_test):\n",
    "        p_model.eval()\n",
    "        if use_cuda:\n",
    "            tdA = Variable(dA).cuda() # (B=1, map_r, map_c)\n",
    "            tdB = Variable(dB).cuda() # (B=1, map_r, map_c)\n",
    "            tdC = Variable(dC).cuda() # (B=1, map_r, map_c)\n",
    "            tdD = Variable(dD).cuda() # (B=1, map_r, map_c)\n",
    "            td_label = Variable(torch.tensor(\n",
    "                [1 for _ in range(2)]+\\\n",
    "                [0 for _ in range(4)]\n",
    "            )).cuda()\n",
    "        else:\n",
    "            tdA = Variable(dA) # (B=1, map_r, map_c)\n",
    "            tdB = Variable(dB) # (B=1, map_r, map_c)\n",
    "            tdC = Variable(dC) # (B=1, map_r, map_c)\n",
    "            tdD = Variable(dD) # (B=1, map_r, map_c)\n",
    "            td_label = Variable(torch.tensor(\n",
    "                [1 for _ in range(2)]+\\\n",
    "                [0 for _ in range(4)]\n",
    "            ))\n",
    "            \n",
    "        d_output = p_model(tdA, tdB, tdC, tdD) # (B, 2)\n",
    "        d_loss = p_lossfn(\n",
    "            F.log_softmax(d_output, dim=1),\n",
    "            td_label,\n",
    "        )\n",
    "        \n",
    "        test_loss_list.append(d_loss.cpu().data.numpy())\n",
    "        test_pPOS_list += F.softmax(d_output,dim=1)[:2,1].cpu().data.tolist()\n",
    "        test_pNEG_list += F.softmax(d_output,dim=1)[2:,0].cpu().data.tolist()\n",
    "        test_aPOS_list += (torch.argmax(d_output,dim=1)[:2]==td_label[:2]).cpu().data.tolist()\n",
    "        test_aNEG_list += (torch.argmax(d_output,dim=1)[2:]==td_label[2:]).cpu().data.tolist()\n",
    "        test_sPOS_list += F.softmax(d_output,dim=1)[:2,1].cpu().data.tolist()\n",
    "        test_sNEG_list += F.softmax(d_output,dim=1)[2:,1].cpu().data.tolist()\n",
    "        \n",
    "    print(\"# Test avg.loss:{:.2f}, avg.prob.:{:.2f}/{:.2f}, avg.acc.:{:.2f}/{:.2f}, avg.score:{:.2f}/{:.2f}\".format(\n",
    "        sum(test_loss_list)/len(test_loss_list),\n",
    "        sum(test_pNEG_list)/len(test_pNEG_list),\n",
    "        sum(test_pPOS_list)/len(test_pPOS_list),\n",
    "        sum(test_aNEG_list)/len(test_aNEG_list),\n",
    "        sum(test_aPOS_list)/len(test_aPOS_list),\n",
    "        sum(test_sNEG_list)/len(test_sNEG_list), # score is the similarity score\n",
    "        sum(test_sPOS_list)/len(test_sPOS_list),\n",
    "    ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RankerTrainer(p_config, p_model, pld_train, pld_test, p_optim, p_lossfn):\n",
    "    RankerTester(p_config, p_model, pld_test, p_lossfn)\n",
    "    for d_ep in range(p_config[\"ranker\"][\"n_ep\"]):\n",
    "        epoch_loss_list = []\n",
    "        for batch_idx, (dA, dB, dC, dD) in enumerate(pld_train):\n",
    "            p_model.train()\n",
    "            \n",
    "            if use_cuda:\n",
    "                tdA = Variable(dA).cuda() # (B=1, map_r, map_c)\n",
    "                tdB = Variable(dB).cuda() # (B=1, map_r, map_c)\n",
    "                tdC = Variable(dC).cuda() # (B=1, map_r, map_c)\n",
    "                tdD = Variable(dD).cuda() # (B=1, map_r, map_c)\n",
    "                td_label = Variable(torch.tensor(\n",
    "                    [1 for _ in range(2)]+\\\n",
    "                    [0 for _ in range(4)]\n",
    "                )).cuda()\n",
    "            else:\n",
    "                tdA = Variable(dA) # (B=1, map_r, map_c)\n",
    "                tdB = Variable(dB) # (B=1, map_r, map_c)\n",
    "                tdC = Variable(dC) # (B=1, map_r, map_c)\n",
    "                tdD = Variable(dD) # (B=1, map_r, map_c)\n",
    "                td_label = Variable(torch.tensor(\n",
    "                    [1 for _ in range(2)]+\\\n",
    "                    [0 for _ in range(4)]\n",
    "                ))\n",
    "                \n",
    "            # (B=12, 2)\n",
    "            d_output = p_model(tdA, tdB, tdC, tdD)\n",
    "            p_optim.zero_grad()\n",
    "            d_loss = p_lossfn(\n",
    "                F.log_softmax(d_output, dim=1),\n",
    "                td_label,\n",
    "            )\n",
    "            epoch_loss_list.append(d_loss.cpu().data.numpy())\n",
    "            d_loss.backward()\n",
    "            p_optim.step()\n",
    "            \n",
    "            print(\"\\r# Training EP:{}, B:{}, ep.loss:{:.2f}\".format(\n",
    "                d_ep, batch_idx, sum(epoch_loss_list),\n",
    "            ),end=\"\")\n",
    "        \n",
    "        # end of epoch print a new line\n",
    "        print()\n",
    "        RankerTester(p_config, p_model, pld_test, p_lossfn)\n",
    "        \n",
    "        # save the model\n",
    "        if d_ep%10==0:\n",
    "            torch.save(p_model.state_dict(), \"./saved_models/0731Ranker_Zaki_ep{}.pt\".format(d_ep))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_interpreter = MorpheusInterpreter()\n",
    "m_spec = S.parse_file('./example/camb3.tyrell')\n",
    "\n",
    "m_config = {\n",
    "    \"vocab_size\": 2048,\n",
    "    \"embd_dim\": 16,\n",
    "    \"max_length\": m_interpreter.CAMB_NROW + m_interpreter.CAMB_NCOL,\n",
    "    \"ranker\":{\n",
    "        \"data_path\": \"./0716MDsize1.pkl\",\n",
    "        \"train_size\": 1000, # how many samples in every epoch\n",
    "        \"test_size\": 100,\n",
    "        \"n_ep\": 1000000,\n",
    "    },\n",
    "}\n",
    "\n",
    "# ########### BIG NOTICE ########### #\n",
    "# The dataset 0716MDsize1.pkl is problematic\n",
    "# and please only use the program not the table variables\n",
    "# due to hash inconsistency\n",
    "# ########### BIG NOTICE ########### #\n",
    "\n",
    "# load the data and dataset\n",
    "with open(m_config[\"ranker\"][\"data_path\"],\"rb\") as f:\n",
    "    m_data = pickle.load(f)\n",
    "    \n",
    "dt_train = RankerDataset(\n",
    "    p_config=m_config, \n",
    "    p_dataset=m_data, \n",
    "    p_interpreter=m_interpreter,\n",
    "    p_spec=m_spec,\n",
    ")\n",
    "dt_train.n_sample = m_config[\"ranker\"][\"train_size\"]\n",
    "ld_train = DataLoader(dataset=dt_train, batch_size=1, shuffle=True)\n",
    "\n",
    "dt_test = RankerDataset(\n",
    "    p_config=m_config, \n",
    "    p_dataset=m_data, \n",
    "    p_interpreter=m_interpreter,\n",
    "    p_spec=m_spec,\n",
    ")\n",
    "dt_test.n_sample = m_config[\"ranker\"][\"test_size\"]\n",
    "ld_test = DataLoader(dataset=dt_test, batch_size=1, shuffle=True)\n",
    "\n",
    "m_ranker = Ranker(p_config=m_config)\n",
    "if use_cuda:\n",
    "    m_ranker = m_ranker.cuda()\n",
    "optimizer = torch.optim.Adam(list(m_ranker.parameters()))\n",
    "lossfn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Test avg.loss:0.69, avg.prob.:0.50/0.50, avg.acc.:0.53/0.42, avg.score:0.50/0.50\n",
      "# Training EP:0, B:999, ep.loss:508.96\n",
      "# Test avg.loss:0.48, avg.prob.:0.76/0.58, avg.acc.:0.74/0.78, avg.score:0.24/0.58\n",
      "# Training EP:1, B:999, ep.loss:442.74\n",
      "# Test avg.loss:0.42, avg.prob.:0.82/0.51, avg.acc.:0.85/0.66, avg.score:0.18/0.51\n",
      "# Training EP:2, B:999, ep.loss:400.82\n",
      "# Test avg.loss:0.39, avg.prob.:0.86/0.62, avg.acc.:0.85/0.75, avg.score:0.14/0.62\n",
      "# Training EP:3, B:999, ep.loss:384.29\n",
      "# Test avg.loss:0.30, avg.prob.:0.86/0.70, avg.acc.:0.86/0.84, avg.score:0.14/0.70\n",
      "# Training EP:4, B:999, ep.loss:367.63\n",
      "# Test avg.loss:0.37, avg.prob.:0.80/0.66, avg.acc.:0.80/0.90, avg.score:0.20/0.66\n",
      "# Training EP:5, B:999, ep.loss:365.32\n",
      "# Test avg.loss:0.38, avg.prob.:0.82/0.63, avg.acc.:0.80/0.85, avg.score:0.18/0.63\n",
      "# Training EP:6, B:999, ep.loss:351.84\n",
      "# Test avg.loss:0.33, avg.prob.:0.85/0.68, avg.acc.:0.85/0.86, avg.score:0.15/0.68\n",
      "# Training EP:7, B:999, ep.loss:344.41\n",
      "# Test avg.loss:0.36, avg.prob.:0.82/0.62, avg.acc.:0.82/0.88, avg.score:0.18/0.62\n",
      "# Training EP:8, B:999, ep.loss:346.49\n",
      "# Test avg.loss:0.38, avg.prob.:0.80/0.69, avg.acc.:0.78/0.89, avg.score:0.20/0.69\n",
      "# Training EP:9, B:999, ep.loss:344.25\n",
      "# Test avg.loss:0.32, avg.prob.:0.86/0.60, avg.acc.:0.87/0.84, avg.score:0.14/0.60\n",
      "# Training EP:10, B:999, ep.loss:329.34\n",
      "# Test avg.loss:0.31, avg.prob.:0.83/0.81, avg.acc.:0.86/0.92, avg.score:0.17/0.81\n",
      "# Training EP:11, B:999, ep.loss:341.94\n",
      "# Test avg.loss:0.37, avg.prob.:0.85/0.69, avg.acc.:0.83/0.86, avg.score:0.15/0.69\n",
      "# Training EP:12, B:999, ep.loss:327.91\n",
      "# Test avg.loss:0.27, avg.prob.:0.88/0.68, avg.acc.:0.92/0.80, avg.score:0.12/0.68\n",
      "# Training EP:13, B:999, ep.loss:320.53\n",
      "# Test avg.loss:0.27, avg.prob.:0.89/0.71, avg.acc.:0.88/0.84, avg.score:0.11/0.71\n",
      "# Training EP:14, B:999, ep.loss:300.54\n",
      "# Test avg.loss:0.25, avg.prob.:0.87/0.71, avg.acc.:0.91/0.94, avg.score:0.13/0.71\n",
      "# Training EP:15, B:999, ep.loss:282.56\n",
      "# Test avg.loss:0.29, avg.prob.:0.87/0.72, avg.acc.:0.84/0.91, avg.score:0.13/0.72\n",
      "# Training EP:16, B:999, ep.loss:305.05\n",
      "# Test avg.loss:0.31, avg.prob.:0.81/0.67, avg.acc.:0.85/0.92, avg.score:0.19/0.67\n",
      "# Training EP:17, B:999, ep.loss:283.58\n",
      "# Test avg.loss:0.24, avg.prob.:0.90/0.74, avg.acc.:0.89/0.92, avg.score:0.10/0.74\n",
      "# Training EP:18, B:999, ep.loss:326.96\n",
      "# Test avg.loss:0.27, avg.prob.:0.86/0.73, avg.acc.:0.85/0.92, avg.score:0.14/0.73\n",
      "# Training EP:19, B:999, ep.loss:264.52\n",
      "# Test avg.loss:0.38, avg.prob.:0.84/0.77, avg.acc.:0.79/0.92, avg.score:0.16/0.77\n",
      "# Training EP:20, B:999, ep.loss:300.29\n",
      "# Test avg.loss:0.29, avg.prob.:0.85/0.66, avg.acc.:0.94/0.73, avg.score:0.15/0.66\n",
      "# Training EP:21, B:999, ep.loss:318.62\n",
      "# Test avg.loss:0.30, avg.prob.:0.85/0.74, avg.acc.:0.90/0.79, avg.score:0.15/0.74\n",
      "# Training EP:22, B:999, ep.loss:285.08\n",
      "# Test avg.loss:0.25, avg.prob.:0.86/0.71, avg.acc.:0.96/0.81, avg.score:0.14/0.71\n",
      "# Training EP:23, B:999, ep.loss:292.83\n",
      "# Test avg.loss:0.28, avg.prob.:0.84/0.72, avg.acc.:0.92/0.80, avg.score:0.16/0.72\n",
      "# Training EP:24, B:999, ep.loss:271.20\n",
      "# Test avg.loss:0.32, avg.prob.:0.87/0.72, avg.acc.:0.93/0.70, avg.score:0.13/0.72\n",
      "# Training EP:25, B:999, ep.loss:268.74\n",
      "# Test avg.loss:0.31, avg.prob.:0.85/0.73, avg.acc.:0.93/0.73, avg.score:0.15/0.73\n",
      "# Training EP:26, B:999, ep.loss:287.82\n",
      "# Test avg.loss:0.30, avg.prob.:0.87/0.83, avg.acc.:0.86/0.93, avg.score:0.13/0.83\n",
      "# Training EP:27, B:999, ep.loss:268.92\n",
      "# Test avg.loss:0.24, avg.prob.:0.88/0.72, avg.acc.:0.96/0.78, avg.score:0.12/0.72\n",
      "# Training EP:28, B:999, ep.loss:284.14\n",
      "# Test avg.loss:0.29, avg.prob.:0.85/0.73, avg.acc.:0.89/0.86, avg.score:0.15/0.73\n",
      "# Training EP:29, B:999, ep.loss:279.67\n",
      "# Test avg.loss:0.31, avg.prob.:0.88/0.81, avg.acc.:0.86/0.92, avg.score:0.12/0.81\n",
      "# Training EP:30, B:999, ep.loss:289.26\n",
      "# Test avg.loss:0.29, avg.prob.:0.88/0.74, avg.acc.:0.90/0.89, avg.score:0.12/0.74\n",
      "# Training EP:31, B:999, ep.loss:278.61\n",
      "# Test avg.loss:0.25, avg.prob.:0.87/0.75, avg.acc.:0.91/0.88, avg.score:0.13/0.75\n",
      "# Training EP:32, B:999, ep.loss:284.82\n",
      "# Test avg.loss:0.24, avg.prob.:0.89/0.74, avg.acc.:0.92/0.78, avg.score:0.11/0.74\n",
      "# Training EP:33, B:999, ep.loss:293.44\n",
      "# Test avg.loss:0.31, avg.prob.:0.84/0.74, avg.acc.:0.88/0.80, avg.score:0.16/0.74\n",
      "# Training EP:34, B:999, ep.loss:270.61\n",
      "# Test avg.loss:0.28, avg.prob.:0.89/0.73, avg.acc.:0.85/0.91, avg.score:0.11/0.73\n",
      "# Training EP:35, B:999, ep.loss:287.83\n",
      "# Test avg.loss:0.27, avg.prob.:0.86/0.67, avg.acc.:0.90/0.91, avg.score:0.14/0.67\n",
      "# Training EP:36, B:999, ep.loss:271.40\n",
      "# Test avg.loss:0.26, avg.prob.:0.87/0.73, avg.acc.:0.94/0.79, avg.score:0.13/0.73\n",
      "# Training EP:37, B:999, ep.loss:277.72\n",
      "# Test avg.loss:0.31, avg.prob.:0.89/0.63, avg.acc.:0.94/0.56, avg.score:0.11/0.63\n",
      "# Training EP:38, B:999, ep.loss:313.21\n",
      "# Test avg.loss:0.29, avg.prob.:0.87/0.77, avg.acc.:0.94/0.76, avg.score:0.13/0.77\n",
      "# Training EP:39, B:999, ep.loss:279.96\n",
      "# Test avg.loss:0.30, avg.prob.:0.85/0.73, avg.acc.:0.89/0.80, avg.score:0.15/0.73\n",
      "# Training EP:40, B:999, ep.loss:266.77\n",
      "# Test avg.loss:0.32, avg.prob.:0.86/0.73, avg.acc.:0.91/0.74, avg.score:0.14/0.73\n",
      "# Training EP:41, B:999, ep.loss:282.58\n",
      "# Test avg.loss:0.39, avg.prob.:0.86/0.71, avg.acc.:0.90/0.69, avg.score:0.14/0.71\n",
      "# Training EP:42, B:999, ep.loss:278.36\n",
      "# Test avg.loss:0.28, avg.prob.:0.85/0.72, avg.acc.:0.94/0.73, avg.score:0.15/0.72\n",
      "# Training EP:43, B:999, ep.loss:269.94\n",
      "# Test avg.loss:0.29, avg.prob.:0.88/0.74, avg.acc.:0.94/0.72, avg.score:0.12/0.74\n",
      "# Training EP:44, B:999, ep.loss:273.49\n",
      "# Test avg.loss:0.25, avg.prob.:0.88/0.77, avg.acc.:0.95/0.78, avg.score:0.12/0.77\n",
      "# Training EP:45, B:999, ep.loss:250.21\n",
      "# Test avg.loss:0.27, avg.prob.:0.87/0.71, avg.acc.:0.93/0.71, avg.score:0.13/0.71\n",
      "# Training EP:46, B:999, ep.loss:274.05\n",
      "# Test avg.loss:0.27, avg.prob.:0.87/0.74, avg.acc.:0.93/0.73, avg.score:0.13/0.74\n",
      "# Training EP:47, B:999, ep.loss:291.93\n",
      "# Test avg.loss:0.22, avg.prob.:0.89/0.73, avg.acc.:0.95/0.76, avg.score:0.11/0.73\n",
      "# Training EP:48, B:999, ep.loss:269.71\n",
      "# Test avg.loss:0.24, avg.prob.:0.89/0.70, avg.acc.:0.94/0.76, avg.score:0.11/0.70\n",
      "# Training EP:49, B:999, ep.loss:264.83\n",
      "# Test avg.loss:0.29, avg.prob.:0.87/0.79, avg.acc.:0.93/0.78, avg.score:0.13/0.79\n",
      "# Training EP:50, B:999, ep.loss:253.62\n",
      "# Test avg.loss:0.21, avg.prob.:0.88/0.79, avg.acc.:0.95/0.81, avg.score:0.12/0.79\n",
      "# Training EP:51, B:999, ep.loss:285.63\n",
      "# Test avg.loss:0.22, avg.prob.:0.88/0.79, avg.acc.:0.88/0.94, avg.score:0.12/0.79\n",
      "# Training EP:52, B:999, ep.loss:265.44\n",
      "# Test avg.loss:0.20, avg.prob.:0.91/0.79, avg.acc.:0.90/0.93, avg.score:0.09/0.79\n",
      "# Training EP:53, B:999, ep.loss:305.76\n",
      "# Test avg.loss:0.32, avg.prob.:0.87/0.70, avg.acc.:0.83/0.89, avg.score:0.13/0.70\n",
      "# Training EP:54, B:999, ep.loss:256.16\n",
      "# Test avg.loss:0.25, avg.prob.:0.87/0.75, avg.acc.:0.87/0.94, avg.score:0.13/0.75\n",
      "# Training EP:55, B:999, ep.loss:296.54\n",
      "# Test avg.loss:0.26, avg.prob.:0.87/0.75, avg.acc.:0.94/0.78, avg.score:0.13/0.75\n",
      "# Training EP:56, B:999, ep.loss:293.52\n",
      "# Test avg.loss:0.29, avg.prob.:0.87/0.72, avg.acc.:0.87/0.90, avg.score:0.13/0.72\n",
      "# Training EP:57, B:999, ep.loss:269.88\n",
      "# Test avg.loss:0.31, avg.prob.:0.85/0.70, avg.acc.:0.87/0.89, avg.score:0.15/0.70\n",
      "# Training EP:58, B:999, ep.loss:269.65\n",
      "# Test avg.loss:0.29, avg.prob.:0.84/0.72, avg.acc.:0.85/0.89, avg.score:0.16/0.72\n",
      "# Training EP:59, B:999, ep.loss:263.35\n",
      "# Test avg.loss:0.31, avg.prob.:0.85/0.75, avg.acc.:0.83/0.88, avg.score:0.15/0.75\n",
      "# Training EP:60, B:999, ep.loss:269.81\n",
      "# Test avg.loss:0.24, avg.prob.:0.87/0.74, avg.acc.:0.90/0.93, avg.score:0.13/0.74\n",
      "# Training EP:61, B:999, ep.loss:276.23\n",
      "# Test avg.loss:0.25, avg.prob.:0.88/0.74, avg.acc.:0.89/0.92, avg.score:0.12/0.74\n",
      "# Training EP:62, B:999, ep.loss:277.93\n",
      "# Test avg.loss:0.35, avg.prob.:0.87/0.78, avg.acc.:0.86/0.89, avg.score:0.13/0.78\n",
      "# Training EP:63, B:999, ep.loss:265.17\n",
      "# Test avg.loss:0.22, avg.prob.:0.89/0.77, avg.acc.:0.92/0.85, avg.score:0.11/0.77\n",
      "# Training EP:64, B:999, ep.loss:253.54\n",
      "# Test avg.loss:0.28, avg.prob.:0.90/0.74, avg.acc.:0.86/0.92, avg.score:0.10/0.74\n",
      "# Training EP:65, B:999, ep.loss:256.62\n",
      "# Test avg.loss:0.27, avg.prob.:0.89/0.78, avg.acc.:0.87/0.91, avg.score:0.11/0.78\n",
      "# Training EP:66, B:999, ep.loss:257.89\n",
      "# Test avg.loss:0.25, avg.prob.:0.89/0.76, avg.acc.:0.87/0.92, avg.score:0.11/0.76\n",
      "# Training EP:67, B:999, ep.loss:280.22\n",
      "# Test avg.loss:0.33, avg.prob.:0.86/0.77, avg.acc.:0.84/0.88, avg.score:0.14/0.77\n",
      "# Training EP:68, B:999, ep.loss:250.28\n",
      "# Test avg.loss:0.26, avg.prob.:0.86/0.77, avg.acc.:0.85/0.92, avg.score:0.14/0.77\n",
      "# Training EP:69, B:999, ep.loss:266.65\n",
      "# Test avg.loss:0.28, avg.prob.:0.85/0.70, avg.acc.:0.87/0.92, avg.score:0.15/0.70\n",
      "# Training EP:70, B:999, ep.loss:273.18\n",
      "# Test avg.loss:0.20, avg.prob.:0.88/0.83, avg.acc.:0.88/0.95, avg.score:0.12/0.83\n",
      "# Training EP:71, B:999, ep.loss:263.38\n",
      "# Test avg.loss:0.24, avg.prob.:0.89/0.76, avg.acc.:0.88/0.92, avg.score:0.11/0.76\n",
      "# Training EP:72, B:30, ep.loss:9.73"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From cffi callback <function _consolewrite_ex at 0x7fc3c0cb2158>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/rpy2/rinterface_lib/callbacks.py\", line 122, in _consolewrite_ex\n",
      "    @ffi.callback(WRITECONSOLE_EX_SIGNATURE)\n",
      "KeyboardInterrupt\n",
      "From cffi callback <function _consolewrite_ex at 0x7fc3c0cb2158>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/rpy2/rinterface_lib/callbacks.py\", line 122, in _consolewrite_ex\n",
      "    @ffi.callback(WRITECONSOLE_EX_SIGNATURE)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training EP:72, B:32, ep.loss:10.72"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d2e73a8cc945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRankerTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_ranker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mld_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mld_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-e1252bb70941>\u001b[0m in \u001b[0;36mRankerTrainer\u001b[0;34m(p_config, p_model, pld_train, pld_test, p_optim, p_lossfn)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md_ep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ranker\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_ep\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mepoch_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdD\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpld_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7c9067da4230>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, p_ind)\u001b[0m\n\u001b[1;32m    101\u001b[0m     '''\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_triangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-7c9067da4230>\u001b[0m in \u001b[0;36mget_triangle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             )\n\u001b[1;32m     80\u001b[0m             \u001b[0md_psAC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_by_prog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_progAC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0md_checkAC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_psAC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0md_checkAC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Trinity/MorpheusInterpreter.py\u001b[0m in \u001b[0;36msanity_check\u001b[0;34m(self, p_ps)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;31m# inputs should not be equal to output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_ps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_ps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp_ps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Rule/IONEQ\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Trinity/MorpheusInterpreter.py\u001b[0m in \u001b[0;36mequal\u001b[0;34m(self, actual, expect)\u001b[0m\n\u001b[1;32m     89\u001b[0m         '''.format(lhs=actual, rhs=expect)\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# ignoreNames:TRUE, work for benchmark 23\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mret_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rpy2/robjects/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStrSexpVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rpy2/robjects/functions.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr_k\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         return (super(SignatureTranslatedFunction, self)\n\u001b[0;32m--> 192\u001b[0;31m                 .__call__(*args, **kwargs))\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rpy2/robjects/functions.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mnew_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy2rpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rpy2/rinterface_lib/conversion.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cdata_res_to_rinterface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mcdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m# TODO: test cdata is of the expected CType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cdata_to_rinterface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rpy2/rinterface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m                     \u001b[0mcall_r\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m                     \u001b[0membedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobalenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sexp__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m                     error_occured))\n\u001b[0m\u001b[1;32m    772\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merror_occured\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rinterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_geterrmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "RankerTrainer(m_config, m_ranker, ld_train, ld_test, optimizer, lossfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
