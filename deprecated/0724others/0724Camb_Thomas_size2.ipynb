{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PolyNeo\n",
    "- Introducing a poly policy for fast adaptation\n",
    "- Stage: Cambrian\n",
    "- Version: Inaria\n",
    "- Update Logs\n",
    "    - 0713: with DeepPath style rollback at training\n",
    "    - 0716: new learning paradigm, see memo for details\n",
    "    - 0724: poly structure to fit fast online adaptation\n",
    "\n",
    "#### Related Commands\n",
    "- tensorboard --logdir runs\n",
    "- nohup jupyter lab > jupyter.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tyrell.spec as S\n",
    "from tyrell.decider import Example\n",
    "\n",
    "# Morpheus Version\n",
    "from MorpheusInterpreter import *\n",
    "from ProgramSpace import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListModule(object):\n",
    "    def __init__(self, module, prefix, *args):\n",
    "        self.module = module\n",
    "        self.prefix = prefix\n",
    "        self.num_module = 0\n",
    "        for new_module in args:\n",
    "            self.append(new_module)\n",
    "    \n",
    "    def append(self, new_module):\n",
    "        if not isinstance(new_module, nn.Module):\n",
    "            raise ValueError('Not a Module')\n",
    "        else:\n",
    "            self.module.add_module(self.prefix + str(self.num_module), new_module)\n",
    "            self.num_module += 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.num_module\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if i<0 or i>=self.num_module:\n",
    "            raise IndexError('Out of bound')\n",
    "        return getattr(self.module, self.prefix+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEncoder(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(ValueEncoder, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        self.vocab_size = self.config[\"val\"][\"vocab_size\"]\n",
    "        self.embd_dim = self.config[\"val\"][\"embd_dim\"]\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embd_dim,\n",
    "            self.config[\"val\"][\"IDX_PAD\"],\n",
    "        )\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels = self.config[\"val\"][\"embd_dim\"],\n",
    "            out_channels = self.config[\"val\"][\"conv_n_kernels\"],\n",
    "            kernel_size = self.config[\"val\"][\"conv_kernel_size\"],\n",
    "        )\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size = self.config[\"val\"][\"pool_kernel_size\"],\n",
    "            padding = self.config[\"val\"][\"IDX_PAD\"],\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            self.config[\"val\"][\"conv_n_kernels\"],\n",
    "            self.config[\"embd_dim\"],\n",
    "        )\n",
    "        \n",
    "    def forward(self, bp_map):\n",
    "        # batched maps, (B, map_r, map_c)\n",
    "        # in this version, every value only contains 1 map\n",
    "        B = bp_map.shape[0]\n",
    "        \n",
    "        # (B, map_r, map_c, val_embd_dim) -> (B, val_embd_dim, map_r, map_c)\n",
    "        d_embd = self.embedding(bp_map).permute(0,3,1,2)\n",
    "        \n",
    "        # (B, n_kernel, map_r, 1)\n",
    "        d_conv = F.relu(self.conv(d_embd))\n",
    "        \n",
    "        # (B, n_kernel)\n",
    "        d_pool = self.pool(d_conv).view(B,self.config[\"val\"][\"conv_n_kernels\"])\n",
    "        \n",
    "        # (B, embd_dim)\n",
    "        d_out = F.relu(\n",
    "            self.fc(d_pool)\n",
    "        )\n",
    "        \n",
    "        return d_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyNeo(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(PolyNeo, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        self.value_encoder = ValueEncoder(self.config)\n",
    "        self.fn_embedding = nn.Embedding(\n",
    "            self.config[\"fn\"][\"vocab_size\"]+1, # +1 for pad\n",
    "            self.config[\"embd_dim\"],\n",
    "            padding_idx=self.config[\"fn\"][\"vocab_size\"], # NOTICE: use +1 as padding\n",
    "        )\n",
    "        self.core_policy = nn.Linear(\n",
    "            self.config[\"embd_dim\"],\n",
    "            self.config[\"fn\"][\"vocab_size\"],\n",
    "        )\n",
    "        self.poly_policy = nn.Linear(\n",
    "            self.config[\"fn\"][\"vocab_size\"] + \\\n",
    "            (self.config[\"adaptation\"][\"maxn_step\"]-1) * self.config[\"embd_dim\"],\n",
    "            self.config[\"fn\"][\"vocab_size\"],\n",
    "        )\n",
    "        \n",
    "    def forward(self, p_mapin, p_mapout, p_fns):\n",
    "        # p_mapin/p_mapout: (B=1, map_r, map_c)\n",
    "        # p_fns: (B=1, #f_called)\n",
    "        # p_poly: int, step/#f_called\n",
    "        B = p_mapin.shape[0]\n",
    "        \n",
    "        v_in = self.value_encoder(p_mapin)\n",
    "        v_out= self.value_encoder(p_mapout)\n",
    "        v_delta = v_out-v_in # (B=1, embd_dim)\n",
    "        v_core = F.relu(\n",
    "            self.core_policy(\n",
    "                v_delta,\n",
    "            )\n",
    "        ) # (B=1, fn_vocab_size)\n",
    "        \n",
    "        v_fns = self.fn_embedding(p_fns) # (B=1, maxn_step, embd_dim)\n",
    "        vi_fns = v_fns.view(B, -1) # (B=1, maxn_step*embd_dim)\n",
    "        \n",
    "        v_po = torch.cat([v_core,vi_fns],dim=1)\n",
    "        # (B=1, fn_vocab_size+maxn_step*embd_dim)\n",
    "        \n",
    "        v_pred = self.poly_policy(\n",
    "            v_po,\n",
    "        )\n",
    "        # (B=1, fn_vocab_size)\n",
    "        \n",
    "        # didn't apply any activation\n",
    "        return F.log_softmax(\n",
    "            v_pred, dim=1\n",
    "        )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace certain node id with certain value\n",
    "def modify_shell(p_shell, p_id_from, p_id_to):\n",
    "    d_prod = p_shell[0]\n",
    "    d_rhs = p_shell[1]\n",
    "    ld_rhs = [p_id_to if d_rhs[i]==p_id_from else d_rhs[i]\n",
    "             for i in range(len(d_rhs))]\n",
    "    return (d_prod, tuple(ld_rhs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "meta-test an agent, directly run into testing / online adaptation\n",
    "'''\n",
    "def Adaptation(p_config, p_spec, p_interpreter, p_generator, p_model, p_optim, p_writer):\n",
    "    print(\"# Start Adaptation...\")\n",
    "    \n",
    "    n_solved = 0 # track the number of solved problem\n",
    "    n_attempt_list = [] # track the number of attempts in every episode\n",
    "    \n",
    "    for d_episode in range(p_config[\"adaptation\"][\"n_episode\"]):\n",
    "        \n",
    "        # retrieve the given meta-trained model for testing\n",
    "        test_model = copy.deepcopy(p_model)\n",
    "        test_model.train()\n",
    "        \n",
    "        # if doing random meta-testing\n",
    "        # then randomly generate a program for testing\n",
    "        ps_solution = p_generator.get_new_chain_program(\n",
    "            p_config[\"adaptation\"][\"fixed_depth\"],\n",
    "        )\n",
    "        # print(\"# Problem: {}\".format(str(ps_solution.node_list[-1])))\n",
    "        \n",
    "        is_solved = False\n",
    "        for d_attempt in range(p_config[\"adaptation\"][\"maxn_attempt\"]):\n",
    "            \n",
    "            attempt_reward = None\n",
    "            \n",
    "            # in every new attempt, initialize a new Program Space\n",
    "            ps_current = ProgramSpace(\n",
    "                p_spec, p_interpreter, ps_solution.inputs, ps_solution.output,\n",
    "            )\n",
    "            # then initialize a shell template\n",
    "            tmp_shell_list = ps_current.get_neighboring_shells()\n",
    "            tmp_node_to_replace = ps_current.node_dict[\"ParamNode\"][0] # for chain only\n",
    "            # replace the Param Node id in shells with -1 to make them templates\n",
    "            template_list = [\n",
    "                modify_shell(tmp_shell_list[i],tmp_node_to_replace,-1)\n",
    "                for i in range(len(tmp_shell_list))\n",
    "            ]\n",
    "            \n",
    "            n_dead = 0\n",
    "            n_sanity = 0\n",
    "            d_step = 0\n",
    "            selected_neurons = []\n",
    "            selected_functions = [\n",
    "                p_config[\"fn\"][\"vocab_size\"]\n",
    "                for _ in range(p_config[\"adaptation\"][\"maxn_step\"]-1)\n",
    "            ]\n",
    "            \n",
    "            var_input = ps_current.inputs[0]\n",
    "            var_output = ps_current.output\n",
    "            map_input = p_interpreter.camb_get_abs(var_input)\n",
    "            map_output = p_interpreter.camb_get_abs(var_output)\n",
    "            # wrap in B=1\n",
    "            if use_cuda:\n",
    "                td_input = Variable(torch.tensor([map_input],dtype=torch.long)).cuda()\n",
    "                td_output = Variable(torch.tensor([map_output],dtype=torch.long)).cuda()\n",
    "            else:\n",
    "                td_input = Variable(torch.tensor([map_input],dtype=torch.long))\n",
    "                td_output = Variable(torch.tensor([map_output],dtype=torch.long))\n",
    "            \n",
    "            while d_step<p_config[\"adaptation\"][\"maxn_step\"]:\n",
    "                \n",
    "                # print the training progress\n",
    "                print(\"\\r# AC/EP:{}/{}, AT:{}, SP:{}, ND:{}, NS:{}, avg.attempt:{:.2f}\".format(\n",
    "                    n_solved, d_episode, d_attempt, d_step,\n",
    "                    n_dead, n_sanity,\n",
    "                    sum(n_attempt_list)/len(n_attempt_list) if len(n_attempt_list)>0 else -1,\n",
    "                ),end=\"\")\n",
    "                \n",
    "                # ### assume chain execution, so only 1 possible returns\n",
    "                # ### at d_step=0, this should be input[0]\n",
    "                id_current = ps_current.get_strict_frontiers()[0]\n",
    "                # make current shell list\n",
    "                current_shell_list = [\n",
    "                    modify_shell(template_list[i],-1,id_current)\n",
    "                    for i in range(len(template_list))\n",
    "                ]\n",
    "                \n",
    "                # wrap in B=1\n",
    "                if use_cuda:\n",
    "                    td_fns = Variable(torch.tensor([selected_functions],dtype=torch.long)).cuda()\n",
    "                else:\n",
    "                    td_fns = Variable(torch.tensor([selected_functions],dtype=torch.long))\n",
    "                    \n",
    "                # (B=1, fn_vocab_size)\n",
    "                td_pred = test_model(td_input, td_output, td_fns)\n",
    "                \n",
    "                # no hints\n",
    "                if random.random()<=p_config[\"adaptation\"][\"exploration_rate\"]:\n",
    "                    # exploration\n",
    "                    tmp_id = random.choice(range(len(current_shell_list)))\n",
    "                else:\n",
    "                    # exploitation\n",
    "                    tmp_id = torch.multinomial(td_pred.exp().flatten(), 1).cpu().flatten().numpy()[0]\n",
    "                \n",
    "                # update ps_current\n",
    "                ps_backup = ps_current.make_copy() # supports undo for failed sanity_check\n",
    "                update_status = ps_current.add_neighboring_shell(\n",
    "                    current_shell_list[tmp_id]\n",
    "                )\n",
    "                \n",
    "                if update_status:\n",
    "                    \n",
    "                    # succeed\n",
    "                    if ps_current.check_eq() is not None:\n",
    "                        # and solved!\n",
    "                        d_step += 1\n",
    "                        is_solved = True\n",
    "                        n_solved += 1\n",
    "                        attempt_reward = 1. # useless, but still attach it\n",
    "                        break\n",
    "                    else:\n",
    "                        # do sanity check\n",
    "                        check_current = p_interpreter.sanity_check(ps_current)\n",
    "                        if check_current[0]:\n",
    "                            d_step += 1\n",
    "                            selected_neurons.append(td_pred[0,tmp_id])\n",
    "                            selected_functions.append(tmp_id)\n",
    "                            selected_functions = selected_functions[1:]\n",
    "                            attempt_reward = -1. # this is temporal, may change later\n",
    "                            continue\n",
    "                        else:\n",
    "                            # Inaria: fail the sanity check, bp immediately and restart the same step\n",
    "                            dead_loss = (-1.)*td_pred[0,tmp_id]\n",
    "                            p_optim.zero_grad()\n",
    "                            dead_loss.backward()\n",
    "                            p_optim.step()\n",
    "                            dead_loss = None\n",
    "                            \n",
    "                            # between this, undo the ProgramSpace\n",
    "                            ps_current = ps_backup\n",
    "                            \n",
    "                            n_sanity += 1\n",
    "                            if n_sanity>=p_config[\"adaptation\"][\"maxn_sanity\"]:\n",
    "                                # reach the limit, perhaps all are dead, restart the attempt\n",
    "                                attempt_reward = -0.5\n",
    "                                break\n",
    "                            else:\n",
    "                                continue\n",
    "                            \n",
    "                else:\n",
    "                    # Inaria: fail, back prop **immediately** and restart the **same step**\n",
    "                    dead_loss = (-1.)*td_pred[0,tmp_id]\n",
    "                    p_optim.zero_grad()\n",
    "                    dead_loss.backward()\n",
    "                    p_optim.step()\n",
    "                    dead_loss = None\n",
    "                    \n",
    "                    n_dead += 1\n",
    "                    if n_dead>=p_config[\"adaptation\"][\"maxn_dead\"]:\n",
    "                        # reach the limit, perhaps all are dead, restart the attempt\n",
    "                        attempt_reward = -0.5\n",
    "                        break\n",
    "                        # and then still need to deal with bp of previous choices\n",
    "                    else:\n",
    "                        continue\n",
    "            \n",
    "            # <END_FOR_STEP>\n",
    "            \n",
    "            \n",
    "            if is_solved:\n",
    "                # already solved in the last attempt, stop\n",
    "                n_attempt_list.append(d_attempt)\n",
    "                # print(\"Solution: {}\".format(ps_current.node_list[-1]))\n",
    "                break\n",
    "            \n",
    "            if len(selected_neurons)>0:\n",
    "                attempt_loss = 0.\n",
    "                # compute the loss (sequential selected)\n",
    "                for i in range(len(selected_neurons)):\n",
    "                    d_decay = p_config[\"adaptation\"][\"decay_rate\"]**(len(selected_neurons)-1-i)\n",
    "                    attempt_loss += d_decay*attempt_reward*(-selected_neurons[i]) \n",
    "                p_optim.zero_grad()\n",
    "                attempt_loss.backward()\n",
    "                p_optim.step()\n",
    "                attepmt_loss = None\n",
    "            # else: do nothing\n",
    "                \n",
    "        # <END_FOR_ATTEMPT>     \n",
    "            \n",
    "    # <END_FOR_EPISODE>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_interpreter = MorpheusInterpreter()\n",
    "m_spec = S.parse_file('./example/camb3.tyrell')\n",
    "m_generator = MorpheusGenerator(\n",
    "    spec=m_spec,\n",
    "    interpreter=m_interpreter,\n",
    ")\n",
    "\n",
    "# dumb variable to help infer the shells\n",
    "m_ps = ProgramSpace(\n",
    "    m_spec, m_interpreter, [None], None,\n",
    ")\n",
    "\n",
    "m_config = {\n",
    "    # ==== TransE Setting ==== #\n",
    "    \"val\":{\n",
    "        \"vocab_size\": len(m_interpreter.CAMB_LIST),\n",
    "        \"embd_dim\": 16, # embedding dim of CAMB abstract token\n",
    "        \"conv_n_kernels\": 512,\n",
    "        \"conv_kernel_size\": (1,m_interpreter.CAMB_NCOL), \n",
    "        \"pool_kernel_size\": (m_interpreter.CAMB_NROW,1), \n",
    "        \"IDX_PAD\": 0,\n",
    "    },\n",
    "    \"fn\":{\n",
    "        \"vocab_size\": len(m_ps.get_neighboring_shells())\n",
    "    },\n",
    "    \"embd_dim\": 128,\n",
    "    \"adaptation\":{\n",
    "        \"n_episode\": 100000,\n",
    "        \"fixed_depth\": 3,\n",
    "        \"maxn_attempt\": 100,\n",
    "        \"maxn_step\": 2, # program size\n",
    "        \"maxn_dead\": 50,\n",
    "        \"maxn_sanity\": 50,\n",
    "        \"exploration_rate\": 0,\n",
    "        \"decay_rate\": 0.9,\n",
    "    },\n",
    "}\n",
    "\n",
    "poly_neo = PolyNeo(p_config=m_config)\n",
    "if use_cuda:\n",
    "    poly_neo = poly_neo.cuda()\n",
    "optimizer = torch.optim.Adam(list(poly_neo.parameters()))\n",
    "\n",
    "# writer = SummaryWriter(\"runs/0713CAMB_RL2_camb3\")\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': {'vocab_size': 150,\n",
       "  'embd_dim': 16,\n",
       "  'conv_n_kernels': 512,\n",
       "  'conv_kernel_size': (1, 15),\n",
       "  'pool_kernel_size': (15, 1),\n",
       "  'IDX_PAD': 0},\n",
       " 'fn': {'vocab_size': 120},\n",
       " 'embd_dim': 128,\n",
       " 'adaptation': {'n_episode': 100000,\n",
       "  'fixed_depth': 3,\n",
       "  'maxn_attempt': 100,\n",
       "  'maxn_step': 2,\n",
       "  'maxn_dead': 50,\n",
       "  'maxn_sanity': 50,\n",
       "  'exploration_rate': 0,\n",
       "  'decay_rate': 0.9}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Start Adaptation...\n",
      "# AC/EP:39/166, AT:68, SP:1, ND:21, NS:1, avg.attempt:44.56"
     ]
    }
   ],
   "source": [
    "Adaptation(m_config, m_spec, m_interpreter, m_generator, poly_neo, optimizer, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
