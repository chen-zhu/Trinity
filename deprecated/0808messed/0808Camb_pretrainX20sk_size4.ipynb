{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransNeo/AlphaNeo\n",
    "- AlphaNeo using pre-trained TransE embeddings (optional)\n",
    "- Stage: Cambrian\n",
    "- Version: Spriggina\n",
    "- Update Logs\n",
    "    - 0713: with DeepPath style rollback at training\n",
    "    - **0716: new learning paradigm, see memo for details**\n",
    "\n",
    "#### Related Commands\n",
    "- tensorboard --logdir runs\n",
    "- nohup jupyter lab > jupyter.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tyrell.spec as S\n",
    "from tyrell.decider import Example\n",
    "\n",
    "# Morpheus Version\n",
    "from MorpheusInterpreter import *\n",
    "from ProgramSpace import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransNeo(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(TransNeo, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        # deeper\n",
    "        self.policy0 = nn.Linear(\n",
    "            self.config[\"embd_dim\"]*2,\n",
    "#             self.config[\"embd_dim\"],\n",
    "            128,\n",
    "        )\n",
    "        self.policy1 = nn.Linear(\n",
    "            128,\n",
    "            self.config[\"fn\"][\"vocab_size\"],\n",
    "        )\n",
    "        \n",
    "    def forward(self, p_mapin, p_mapout):\n",
    "        # p_mapin/p_mapout: (B, 15*7)\n",
    "#         v_delta = p_mapout-p_mapin\n",
    "        v_con = torch.cat([p_mapin,p_mapout],dim=1)\n",
    "        tmp_out = torch.log_softmax(\n",
    "            self.policy1(\n",
    "                F.relu(\n",
    "                    self.policy0(\n",
    "                        v_con\n",
    "#                         v_delta\n",
    "                    )\n",
    "                )\n",
    "            ),dim=1\n",
    "        )\n",
    "        \n",
    "        return tmp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace certain node id with certain value\n",
    "def modify_shell(p_shell, p_id_from, p_id_to):\n",
    "    d_prod = p_shell[0]\n",
    "    d_rhs = p_shell[1]\n",
    "    ld_rhs = [p_id_to if d_rhs[i]==p_id_from else d_rhs[i]\n",
    "             for i in range(len(d_rhs))]\n",
    "    return (d_prod, tuple(ld_rhs))\n",
    "\n",
    "\n",
    "# '''\n",
    "# meta-train the agent in a supervised way\n",
    "# epoch -> episode, one attempt with hint\n",
    "# NOTICE: only valid for size 1 training\n",
    "# '''\n",
    "# def MetaTrain(p_config, p_spec, p_interpreter, p_generator, p_model, p_optim, p_writer):\n",
    "#     print(\"# Start Meta-Train...\")\n",
    "#     for d_epoch in range(p_config[\"meta_train\"][\"n_epoch\"]):\n",
    "#         p_model.train()\n",
    "        \n",
    "#         epoch_loss_list = []\n",
    "#         batch_loss_list = []\n",
    "        \n",
    "#         for d_ind in range(p_config[\"meta_train\"][\"n_total\"]):\n",
    "#             print(\"\\r# epoch:{}, index:{}/{}, avg.loss:{:.2f}\".format(\n",
    "#                 d_epoch, d_ind, p_config[\"meta_train\"][\"n_total\"],\n",
    "#                 sum(epoch_loss_list)/len(epoch_loss_list)\n",
    "#                 if len(epoch_loss_list)>0 else 0,\n",
    "#             ),end=\"\")\n",
    "            \n",
    "#             # initialize a solution\n",
    "#             ps_solution = p_generator.get_new_chain_program(\n",
    "#                 2,\n",
    "#             )\n",
    "            \n",
    "#             # initialize a new ProgramSpace\n",
    "#             ps_current = ProgramSpace(\n",
    "#                 p_spec, p_interpreter, ps_solution.inputs, ps_solution.output,\n",
    "#             )\n",
    "#             # then initialize a shell template\n",
    "#             tmp_shell_list = ps_current.get_neighboring_shells()\n",
    "#             tmp_node_to_replace = ps_current.node_dict[\"ParamNode\"][0] # for chain only\n",
    "#             # replace the Param Node id in shells with -1 to make them templates\n",
    "#             template_list = [\n",
    "#                 modify_shell(tmp_shell_list[i],tmp_node_to_replace,-1)\n",
    "#                 for i in range(len(tmp_shell_list))\n",
    "#             ]\n",
    "            \n",
    "#             id_current = ps_current.get_strict_frontiers()[0]\n",
    "#             var_current = ps_current.node_list[id_current].ps_data # need the real var name in r env\n",
    "#             var_output = ps_current.output\n",
    "            \n",
    "#             map_current = p_interpreter.camb_get_ventogyrus(var_current)\n",
    "#             map_output = p_interpreter.camb_get_ventogyrus(var_output)\n",
    "            \n",
    "#             # make current shell list\n",
    "#             current_shell_list = [\n",
    "#                 modify_shell(template_list[i],-1,id_current)\n",
    "#                 for i in range(len(template_list))\n",
    "#             ]\n",
    "            \n",
    "#             # wrap in B=1\n",
    "#             if use_cuda:\n",
    "#                 td_current = Variable(torch.tensor([map_current],dtype=torch.float)).cuda()\n",
    "#                 td_output = Variable(torch.tensor([map_output],dtype=torch.float)).cuda()\n",
    "#             else:\n",
    "#                 td_current = Variable(torch.tensor([map_current],dtype=torch.float))\n",
    "#                 td_output = Variable(torch.tensor([map_output],dtype=torch.float))\n",
    "\n",
    "#             # (B=1, fn_vocab_size)\n",
    "#             td_pred = p_model(td_current, td_output)\n",
    "#             # directly give the hint / supervised, ps.solution.shell[0] works for 1\n",
    "#             tmp_id = current_shell_list.index(ps_solution.shells[0])\n",
    "#             d_loss = (+1)*(-td_pred[0,tmp_id])\n",
    "#             batch_loss_list.append(\n",
    "#                 d_loss, # supervised / always correct with +1 reward\n",
    "#             )\n",
    "#             epoch_loss_list.append(\n",
    "#                 d_loss.cpu().data.numpy(),\n",
    "#             )\n",
    "            \n",
    "#             if len(batch_loss_list)%p_config[\"meta_train\"][\"batch_size\"]==0:\n",
    "#                 # do back-prop.\n",
    "#                 if len(batch_loss_list)>0:\n",
    "#                     batch_loss = sum(batch_loss_list)/len(batch_loss_list)\n",
    "#                     p_optim.zero_grad()\n",
    "#                     batch_loss.backward()\n",
    "#                     p_optim.step()\n",
    "#                 # after back-prop., clean up\n",
    "#                 batch_loss = None\n",
    "#                 batch_loss_list = []\n",
    "                \n",
    "#         print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MetaTrain(p_config, p_spec, p_interpreter, p_generator, p_model, p_bmrks, p_optim, p_writer):\n",
    "    print(\"# Start Meta-Train...\")\n",
    "    \n",
    "    for d_epoch in range(p_config[\"meta_train\"][\"n_epoch\"]):\n",
    "        epoch_loss_list = []\n",
    "        for d_episode in range(p_config[\"meta_train\"][\"n_episode\"]):\n",
    "            print(\"\\r# EP:{}/{}, loss:{:.2f}\".format(\n",
    "                d_epoch, d_episode,\n",
    "                sum(epoch_loss_list)/len(epoch_loss_list) if len(epoch_loss_list)>0 else -1,\n",
    "            ),end=\"\")\n",
    "            p_model.train()\n",
    "            \n",
    "            eid = random.choice(range(p_config[\"meta_train\"][\"n_episode\"]))\n",
    "\n",
    "            # ==== prepare the benchmark ====\n",
    "            bmrk_prog, bmrk_str_example = p_bmrks[eid]\n",
    "            bmrk_example = Example(\n",
    "                input=[p_interpreter.load_data_into_var(p) for p in bmrk_str_example.input],\n",
    "                output=p_interpreter.load_data_into_var(bmrk_str_example.output),\n",
    "            )\n",
    "            ps_solution = ProgramSpace(\n",
    "                p_spec, p_interpreter, bmrk_example.input, bmrk_example.output,\n",
    "            )\n",
    "            ps_solution.init_by_prog(bmrk_prog)\n",
    "            solution_prod_names = [\n",
    "                ps_solution.prod_list[p[0]].name for p in ps_solution.shells\n",
    "            ]\n",
    "            solution_shells = ps_solution.shells\n",
    "\n",
    "            # solution self-check\n",
    "            if ps_solution.check_eq() is None:\n",
    "                continue\n",
    "                # print(\"ERROR, SOLUTION NOT CONSISTENT!\")\n",
    "\n",
    "            is_solved = False\n",
    "\n",
    "            current_prod_names = []\n",
    "\n",
    "            selected_neurons = []\n",
    "            stored_groups = [] # with lists of neurons of the same production name\n",
    "\n",
    "            # in every new attempt, initialize a new Program Space\n",
    "            ps_current = ProgramSpace(\n",
    "                p_spec, p_interpreter, ps_solution.inputs, ps_solution.output,\n",
    "            )\n",
    "            # then initialize a shell template\n",
    "            tmp_shell_list = ps_current.get_neighboring_shells()\n",
    "            tmp_node_to_replace = ps_current.node_dict[\"ParamNode\"][0] # for chain only\n",
    "            # replace the Param Node id in shells with -1 to make them templates\n",
    "            template_list = [\n",
    "                modify_shell(tmp_shell_list[i],tmp_node_to_replace,-1)\n",
    "                for i in range(len(tmp_shell_list))\n",
    "            ]\n",
    "\n",
    "            # for d_step in range(p_config[\"meta_test\"][\"maxn_step\"]):\n",
    "            for d_step in range(len(ps_solution.shells)):\n",
    "\n",
    "                # ### assume chain execution, so only 1 possible returns\n",
    "                # ### at d_step=0, this should be input[0]\n",
    "                id_current = ps_current.get_strict_frontiers()[0]\n",
    "                var_current = ps_current.node_list[id_current].ps_data # need the real var name in r env\n",
    "                var_output = ps_current.output\n",
    "\n",
    "                map_current = p_interpreter.camb_get_ventogyrus(var_current)\n",
    "                map_output = p_interpreter.camb_get_ventogyrus(var_output)\n",
    "\n",
    "                # make current shell list\n",
    "                current_shell_list = [\n",
    "                    modify_shell(template_list[i],-1,id_current)\n",
    "                    for i in range(len(template_list))\n",
    "                ]\n",
    "\n",
    "                # wrap in B=1\n",
    "                if use_cuda:\n",
    "                    td_current = Variable(torch.tensor([map_current],dtype=torch.float)).cuda()\n",
    "                    td_output = Variable(torch.tensor([map_output],dtype=torch.float)).cuda()\n",
    "                else:\n",
    "                    td_current = Variable(torch.tensor([map_current],dtype=torch.float))\n",
    "                    td_output = Variable(torch.tensor([map_output],dtype=torch.float))\n",
    "\n",
    "                # (B=1, fn_vocab_size)\n",
    "                td_pred = p_model(td_current, td_output)\n",
    "\n",
    "                # no hints\n",
    "                # exploitation\n",
    "                # tmp_id = torch.argmax(td_pred.flatten()).cpu().tolist()\n",
    "                tmp_id = current_shell_list.index(ps_solution.shells[d_step])\n",
    "\n",
    "                # update ps_current\n",
    "                update_status = ps_current.add_neighboring_shell(\n",
    "                    current_shell_list[tmp_id]\n",
    "                )\n",
    "\n",
    "                if update_status:\n",
    "                    # record selected neuron\n",
    "                    selected_neurons.append((True, td_pred[0,tmp_id]))\n",
    "\n",
    "                    # succeed\n",
    "                    if ps_current.check_eq() is not None:\n",
    "                        # and solved!\n",
    "                        is_solved = True\n",
    "                        break\n",
    "                else:\n",
    "                    selected_neurons.append((False, td_pred[0,tmp_id]))\n",
    "                    break\n",
    "\n",
    "            # <END_FOR_STEP>\n",
    "            if not is_solved:\n",
    "                raise Exception()\n",
    "\n",
    "            # == Yorgia ==\n",
    "            # compute the loss according to the loss computation rules\n",
    "            # first component, then function call\n",
    "            batch_loss_list = []\n",
    "            for i in range(len(ps_current.shells)):\n",
    "                if not selected_neurons[i][0]:\n",
    "                    raise Exception()\n",
    "                batch_loss_list.append(\n",
    "                    (+1.0)*(-selected_neurons[i][1])\n",
    "                )\n",
    "\n",
    "            batch_loss = sum(batch_loss_list)\n",
    "            epoch_loss_list.append(batch_loss)\n",
    "            p_optim.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            p_optim.step()\n",
    "\n",
    "                \n",
    "        # <END_FOR_EPISODE>  \n",
    "        print()\n",
    "            \n",
    "    # <END_FOR_EPOCH>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_sketch(ps0,ps1):\n",
    "#     if len(ps0.node_list)!=len(ps1.node_list):\n",
    "#         return False\n",
    "#     for i in range(len(ps0.shells)):\n",
    "#         if ps0.node_list[-i-1].name != ps1.node_list[-i-1].name:\n",
    "#             return False\n",
    "#     return True\n",
    "\n",
    "'''\n",
    "meta-test an agent, directly run into testing / online adaptation\n",
    "'''\n",
    "def MetaTest(p_config, p_spec, p_interpreter, p_generator, p_model, p_ngram, p_bmrks, p_optim, p_writer):\n",
    "    print(\"# Start Meta-Test...\")\n",
    "    \n",
    "    n_solved = 0 # track the number of solved problem\n",
    "    n_sketch_solved = 0\n",
    "    n_attempt_list = [] # track the number of attempts in every episode\n",
    "    n_sketch_atlist = [] # track the number of attempts when hitting the sketch\n",
    "    n_sat_list = [] # track the number of sketches when hitting the sketch\n",
    "    \n",
    "    n_ngram_sketch = [] # sketch ranking from ngram (falling back)\n",
    "    \n",
    "    for d_episode in range(p_config[\"meta_test\"][\"n_episode\"]):\n",
    "        proposal_tracker = set()\n",
    "        sketch_tracker = set()\n",
    "        sketch_tracker_sizeN = set()\n",
    "        \n",
    "        # retrieve the given meta-trained model for testing\n",
    "        test_model = copy.deepcopy(p_model)\n",
    "        test_model.train()\n",
    "        test_optim = torch.optim.Adam(list(test_model.parameters()))\n",
    "        \n",
    "        # ==== prepare the benchmark ====\n",
    "        bmrk_prog, bmrk_str_example = p_bmrks[d_episode]\n",
    "        bmrk_example = Example(\n",
    "            input=[p_interpreter.load_data_into_var(p) for p in bmrk_str_example.input],\n",
    "            output=p_interpreter.load_data_into_var(bmrk_str_example.output),\n",
    "        )\n",
    "        ps_solution = ProgramSpace(\n",
    "            p_spec, p_interpreter, bmrk_example.input, bmrk_example.output,\n",
    "        )\n",
    "        ps_solution.init_by_prog(bmrk_prog)\n",
    "        solution_prod_names = [\n",
    "            ps_solution.prod_list[p[0]].name for p in ps_solution.shells\n",
    "        ]\n",
    "        solution_shells = ps_solution.shells\n",
    "        \n",
    "        # solution self-check\n",
    "        if ps_solution.check_eq() is None:\n",
    "            print(\"ERROR, SOLUTION NOT CONSISTENT!\")\n",
    "        \n",
    "        \n",
    "#         f = open(\"./outputs/Sarah3/Problem_{}.txt\".format(d_episode), \"w\")\n",
    "#         f.write(\"# Problem: {}\\n\\n\".format(str(ps_solution.node_list[-1])))\n",
    "#         f.write(\"# Input:\\n{}\\n\".format(p_interpreter.renv(ps_solution.inputs[0])))\n",
    "#         f.write(\"# Output:\\n{}\\n\".format(p_interpreter.renv(ps_solution.output)))\n",
    "#         f.flush()\n",
    "        \n",
    "        is_solved = False\n",
    "        is_sketch_solved = False\n",
    "        \n",
    "        d_attempt = -1\n",
    "        d_update = 0\n",
    "        while d_attempt<p_config[\"meta_test\"][\"maxn_attempt\"]:\n",
    "            if d_update>=p_config[\"meta_test\"][\"maxn_update\"]:\n",
    "                # try too long, stop\n",
    "                break\n",
    "            d_attempt += 1\n",
    "            d_update += 1\n",
    "        # for d_attempt in range(p_config[\"meta_test\"][\"maxn_attempt\"]):\n",
    "            \n",
    "            current_prod_names = []\n",
    "            current_shells = []\n",
    "            current_outputs = []\n",
    "            \n",
    "            selected_neurons = []\n",
    "            stored_groups = [] # with lists of neurons of the same production name\n",
    "            \n",
    "            # in every new attempt, initialize a new Program Space\n",
    "            ps_current = ProgramSpace(\n",
    "                p_spec, p_interpreter, ps_solution.inputs, ps_solution.output,\n",
    "            )\n",
    "            # then initialize a shell template\n",
    "            tmp_shell_list = ps_current.get_neighboring_shells()\n",
    "            tmp_node_to_replace = ps_current.node_dict[\"ParamNode\"][0] # for chain only\n",
    "            # replace the Param Node id in shells with -1 to make them templates\n",
    "            template_list = [\n",
    "                modify_shell(tmp_shell_list[i],tmp_node_to_replace,-1)\n",
    "                for i in range(len(tmp_shell_list))\n",
    "            ]\n",
    "                \n",
    "            d_step = 0\n",
    "            while d_step<p_config[\"meta_test\"][\"maxn_step\"]:\n",
    "                \n",
    "                # print the training progress\n",
    "                print(\"\\r# SK/EP:{}/{}, AT:{}, SP:{}, att.ske.:{:.2f}, ske.:{:.2f}, ngram:{:.2f}\".format(\n",
    "                    n_sketch_solved, d_episode, d_attempt, d_step,\n",
    "                    sum(n_sketch_atlist)/len(n_sketch_atlist) if len(n_sketch_atlist)>0 else -1,\n",
    "                    sum(n_sat_list)/len(n_sat_list) if len(n_sat_list)>0 else -1,\n",
    "                    sum(n_ngram_sketch)/len(n_ngram_sketch) if len(n_ngram_sketch)>0 else -1,\n",
    "                ),end=\"\")\n",
    "                \n",
    "                # ### assume chain execution, so only 1 possible returns\n",
    "                # ### at d_step=0, this should be input[0]\n",
    "                id_current = ps_current.get_strict_frontiers()[0]\n",
    "                var_current = ps_current.node_list[id_current].ps_data # need the real var name in r env\n",
    "                var_output = ps_current.output\n",
    "                \n",
    "                map_current = p_interpreter.camb_get_ventogyrus(var_current)\n",
    "                map_output = p_interpreter.camb_get_ventogyrus(var_output)\n",
    "                \n",
    "                # make current shell list\n",
    "                current_shell_list = [\n",
    "                    modify_shell(template_list[i],-1,id_current)\n",
    "                    for i in range(len(template_list))\n",
    "                ]\n",
    "                \n",
    "                # wrap in B=1\n",
    "                if use_cuda:\n",
    "                    td_current = Variable(torch.tensor([map_current],dtype=torch.float)).cuda()\n",
    "                    td_output = Variable(torch.tensor([map_output],dtype=torch.float)).cuda()\n",
    "                else:\n",
    "                    td_current = Variable(torch.tensor([map_current],dtype=torch.float))\n",
    "                    td_output = Variable(torch.tensor([map_output],dtype=torch.float))\n",
    "                    \n",
    "                # (B=1, fn_vocab_size)\n",
    "                td_pred = test_model(td_current, td_output)\n",
    "                \n",
    "                # no hints\n",
    "                # exploitation\n",
    "                tmp_id = torch.argmax(td_pred.flatten()).cpu().tolist()\n",
    "                # tmp_id = current_shell_list.index(ps_solution.shells[d_step])\n",
    "                    \n",
    "                    \n",
    "                # == Yorgia ==\n",
    "                # find out all other shells that share the same product name\n",
    "                tmp_component_name = ps_current.prod_list[current_shell_list[tmp_id][0]].name\n",
    "                tmp_group = []\n",
    "                for i in range(len(current_shell_list)):\n",
    "                    if ps_current.prod_list[current_shell_list[i][0]].name==tmp_component_name:\n",
    "                        tmp_group.append(td_pred[0,i])\n",
    "                stored_groups.append(tmp_group)\n",
    "                    \n",
    "                # == Yorgia ==\n",
    "                # add prod names first\n",
    "                current_prod_names.append(tmp_component_name)\n",
    "                # then add shells\n",
    "                current_shells.append(current_shell_list[tmp_id])\n",
    "                \n",
    "                # update ps_current\n",
    "                update_status = ps_current.add_neighboring_shell(\n",
    "                    current_shell_list[tmp_id]\n",
    "                )\n",
    "                \n",
    "                if update_status:\n",
    "                    # record selected neuron\n",
    "                    selected_neurons.append((True, td_pred[0,tmp_id]))\n",
    "                    current_outputs.append(ps_current.node_list[-1].ps_data)\n",
    "                    d_step += 1\n",
    "                    \n",
    "                    # succeed\n",
    "                    if ps_current.check_eq() is not None:\n",
    "                        # and solved!\n",
    "                        is_solved = True\n",
    "                        n_solved += 1\n",
    "                        break\n",
    "                else:\n",
    "                    selected_neurons.append((False, td_pred[0,tmp_id]))\n",
    "                    break\n",
    "            \n",
    "            # <END_FOR_STEP>\n",
    "#             f.write(\"# ({}) Proposed {}/{}: {}\\n\".format(\n",
    "#                 \"accept\" if is_solved else \"reject\",\n",
    "#                 d_attempt, p_config[\"meta_test\"][\"maxn_attempt\"],\n",
    "#                 str(ps_current.node_list[-1])\n",
    "#             ))\n",
    "#             f.flush()\n",
    "            sketch_tracker.add(str(current_prod_names))\n",
    "            if len(current_prod_names)==p_config[\"meta_test\"][\"maxn_step\"]:\n",
    "                sketch_tracker_sizeN.add(str(current_prod_names))\n",
    "            if not is_sketch_solved:\n",
    "                if current_prod_names==solution_prod_names:\n",
    "                    n_sketch_atlist.append(d_attempt)\n",
    "                    n_sat_list.append(len(sketch_tracker_sizeN))\n",
    "                    is_sketch_solved = True\n",
    "                    n_sketch_solved += 1\n",
    "                    break\n",
    "            # early force to ngram\n",
    "            if len(sketch_tracker_sizeN)>25:\n",
    "                break\n",
    "                    \n",
    "            if is_solved:\n",
    "                n_attempt_list.append(d_attempt)\n",
    "                break\n",
    "                \n",
    "            d_proposal = str(ps_current.node_list[-1])\n",
    "            if d_proposal in proposal_tracker:\n",
    "                d_attempt -= 1\n",
    "            else:\n",
    "                proposal_tracker.add(d_proposal)\n",
    "            \n",
    "            # print(\"# Current Sketch: {}\".format(current_prod_names))\n",
    "            # == Yorgia ==\n",
    "            # compute the loss according to the loss computation rules\n",
    "            # first component, then function call\n",
    "            batch_loss_list = []\n",
    "            for i in range(len(current_prod_names)):\n",
    "                batch_loss_list.append(\n",
    "                    (-1.0)*(-selected_neurons[i][1])\n",
    "                )\n",
    "#                 if solution_prod_names[i]==current_prod_names[i]:\n",
    "#                     # component match, promote the whole group\n",
    "#                     for j in range(len(stored_groups[i])):\n",
    "#                         batch_loss_list.append(\n",
    "#                             (+1.0)*(-stored_groups[i][j])\n",
    "#                         )\n",
    "#                 else:\n",
    "#                     for j in range(len(stored_groups[i])):\n",
    "#                         batch_loss_list.append(\n",
    "#                             (-1.0)*(-stored_groups[i][j])\n",
    "#                         )\n",
    "#                 # then compare function call\n",
    "#                 if solution_shells[i]==current_shells[i]:\n",
    "#                     batch_loss_list.append(\n",
    "#                         (+10.0)*(-selected_neurons[i][1])\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     batch_loss_list.append(\n",
    "#                         (-10.0)*(-selected_neurons[i][1])\n",
    "#                     )\n",
    "            \n",
    "            batch_loss = sum(batch_loss_list)\n",
    "            test_optim.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            test_optim.step()\n",
    "            \n",
    "                \n",
    "        # <END_FOR_ATTEMPT>    \n",
    "        \n",
    "        # end of trial, if not solved, fall back to ngram version\n",
    "        if not is_sketch_solved:\n",
    "            target_ngram = str(tuple(solution_prod_names))\n",
    "            ngram_ranking = len(sketch_tracker_sizeN)\n",
    "            for p in p_ngram:\n",
    "                if p in sketch_tracker_sizeN:\n",
    "                    continue\n",
    "                if p==target_ngram:\n",
    "                    break\n",
    "                else:\n",
    "                    ngram_ranking += 1\n",
    "            n_ngram_sketch.append(ngram_ranking)\n",
    "#         print(\"ngram rank:{}\".format(ngram_ranking))\n",
    "        \n",
    "#         f.close()\n",
    "            \n",
    "    # <END_FOR_EPISODE>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_interpreter = MorpheusInterpreter()\n",
    "m_spec = S.parse_file('./example/camb5.tyrell')\n",
    "m_generator = MorpheusGenerator(\n",
    "    spec=m_spec,\n",
    "    interpreter=m_interpreter,\n",
    ")\n",
    "\n",
    "# dumb variable to help infer the shells\n",
    "m_ps = ProgramSpace(\n",
    "    m_spec, m_interpreter, [None], None,\n",
    ")\n",
    "\n",
    "m_config = {\n",
    "    \"fn\":{\n",
    "        \"vocab_size\": len(m_ps.get_neighboring_shells())\n",
    "    },\n",
    "    \"embd_dim\": 15*7+1,\n",
    "    # ==== Meta-Learning Setting ==== #\n",
    "    \"meta_train\":{\n",
    "        \"n_epoch\": 5,\n",
    "        \"n_episode\": 100,\n",
    "    },\n",
    "    \"meta_test\":{\n",
    "        \"n_episode\": 100, # only pick the first 250\n",
    "        \"batch_size\": 1, # how many attempts\n",
    "        # \"fixed_depth\": 4,\n",
    "        \"maxn_attempt\": 1000,\n",
    "        \"maxn_update\": 2000,\n",
    "        \"maxn_step\": 4, # program size\n",
    "        \"exploration_rate\": 0,\n",
    "        \"benchmarks\": \"./0807SOSize4.pkl\",\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(m_config[\"meta_test\"][\"benchmarks\"],\"rb\") as f:\n",
    "    bmrks = pickle.load(f)\n",
    "\n",
    "\n",
    "trans_neo = TransNeo(p_config=m_config)\n",
    "if use_cuda:\n",
    "    trans_neo = trans_neo.cuda()\n",
    "optimizer = torch.optim.Adam(list(trans_neo.parameters()))\n",
    "\n",
    "# writer = SummaryWriter(\"runs/0713CAMB_RL2_camb3\")\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the ngram info\n",
    "with open(\"./size4-ngram-camb5.txt\",\"r\") as f:\n",
    "    ngram4_raw = f.readlines()\n",
    "ngram4_parsed = [str(tuple(p.split())) for p in ngram4_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fn': {'vocab_size': 201},\n",
       " 'embd_dim': 106,\n",
       " 'meta_train': {'n_epoch': 5, 'n_episode': 100},\n",
       " 'meta_test': {'n_episode': 100,\n",
       "  'batch_size': 1,\n",
       "  'maxn_attempt': 1000,\n",
       "  'maxn_update': 2000,\n",
       "  'maxn_step': 4,\n",
       "  'exploration_rate': 0,\n",
       "  'benchmarks': './0807SOSize4.pkl'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Start Meta-Train...\n",
      "# EP:0/99, loss:19.18\n",
      "# EP:1/99, loss:16.21\n",
      "# EP:2/99, loss:15.09\n",
      "# EP:3/99, loss:13.40\n",
      "# EP:4/99, loss:12.03\n"
     ]
    }
   ],
   "source": [
    "MetaTrain(m_config, m_spec, m_interpreter, m_generator, trans_neo, bmrks[100:200], optimizer, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Start Meta-Test...\n",
      "# SK/EP:8/10, AT:3, SP:3, att.ske.:33.62, ske.:13.25, ngram:70.00ERROR, SOLUTION NOT CONSISTENT!\n",
      "# SK/EP:11/19, AT:2, SP:3, att.ske.:29.45, ske.:12.18, ngram:300.75ERROR, SOLUTION NOT CONSISTENT!\n",
      "# SK/EP:17/27, AT:48, SP:3, att.ske.:27.65, ske.:11.76, ngram:318.40ERROR, SOLUTION NOT CONSISTENT!\n",
      "# SK/EP:31/60, AT:96, SP:3, att.ske.:26.84, ske.:11.61, ngram:189.31ERROR, SOLUTION NOT CONSISTENT!\n",
      "# SK/EP:33/69, AT:76, SP:3, att.ske.:26.52, ske.:11.64, ngram:200.78ERROR, SOLUTION NOT CONSISTENT!\n",
      "# SK/EP:33/73, AT:137, SP:3, att.ske.:26.52, ske.:11.64, ngram:185.10ERROR, SOLUTION NOT CONSISTENT!\n",
      "# SK/EP:42/99, AT:98, SP:3, att.ske.:23.10, ske.:10.36, ngram:161.88"
     ]
    }
   ],
   "source": [
    "MetaTest(m_config, m_spec, m_interpreter, m_generator, trans_neo, ngram4_parsed, bmrks[:100], optimizer, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12/75/246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 99/151, 128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
