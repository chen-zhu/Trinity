{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component/Abstraction Representation Learning using TransE\n",
    "- Stage: Cambrian\n",
    "- Version: Charniodiscus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "print(\"use_cuda: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morpheus Version\n",
    "from utils_morpheus import *\n",
    "from ProgramSpace import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphGenDataset(Dataset):\n",
    "    def __init__(self, p_spec=None, p_generator=None, p_interpreter=None, p_dps=None, p_len=None):\n",
    "        self.spec = p_spec\n",
    "        self.generator = p_generator\n",
    "        self.interpreter = p_interpreter\n",
    "        self.dps = p_dps\n",
    "        self.len = p_len\n",
    "        \n",
    "        # construct a shell list/dict so that every function call (shell) has id\n",
    "        self.shell_list = self.dps.get_neighboring_shells()\n",
    "        self.shell_dict = {\n",
    "            self.shell_list[i]:i for i in range(len(self.shell_list))\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, p_ind):\n",
    "        # basically ignore the p_ind parameter and generate randomly\n",
    "        # returns a pair of abstraction maps\n",
    "        # (tmp_in, tmp_out)\n",
    "        # which is (n_maps, CAMB_NROW, CAMB_NCOL)\n",
    "        # currently assuming single input single output\n",
    "        \n",
    "        while True:\n",
    "            tmp_input = self.interpreter.random_table()\n",
    "            tmp_prog, tmp_example = self.generator.generate(\n",
    "                fixed_depth=2, # this should be fixed to 2 since we learn size 1\n",
    "                example=Example(input=[tmp_input], output=None),\n",
    "            )\n",
    "            if tmp_prog is not None and tmp_prog.is_apply():\n",
    "                break\n",
    "\n",
    "        tmp_in = camb_get_abs(tmp_example.input[0])\n",
    "        tmp_out= camb_get_abs(tmp_example.output)\n",
    "\n",
    "        tmp_func = self.shell_dict[\n",
    "            (\n",
    "                self.dps.prod_list.index(tmp_prog.production),\n",
    "                tuple(\n",
    "                    [self.dps.node_list.index(tmp_prog.args[i])\n",
    "                    for i in range(len(tmp_prog.args))]\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # =========================================================\n",
    "        # you should also produce a negative sample here and return\n",
    "\n",
    "        while True:\n",
    "            rep_in = tmp_in\n",
    "            rep_out = tmp_out\n",
    "            rep_func = random.choice(range(len(self.shell_list)))\n",
    "            if rep_func==tmp_func:\n",
    "                continue\n",
    "            else:\n",
    "                # in fact, not safe\n",
    "                # should verify that rep_func(rep_in)!=rep_out\n",
    "                # but never mind here\n",
    "                break\n",
    "        \n",
    "        return (tmp_in, tmp_func, tmp_out,\n",
    "                rep_in, rep_func, rep_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarginLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MarginLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pos, neg, margin):\n",
    "        if use_cuda:\n",
    "            zero_tensor = torch.tensor(pos.size(),dtype=torch.float).cuda()\n",
    "        else:\n",
    "            zero_tensor = torch.tensor(pos.size(),dtype=torch.float)\n",
    "        zero_tensor.zero_()\n",
    "        zero_tensor = Variable(zero_tensor)\n",
    "        return torch.sum(torch.max(pos - neg + margin, zero_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_loss(embeddings, dim=1):\n",
    "    norm = torch.sum(embeddings ** 2, dim=dim, keepdim=True)\n",
    "    if use_cuda:\n",
    "        return torch.sum(\n",
    "            torch.max(\n",
    "                norm - Variable(torch.tensor([1.0],dtype=torch.float)).cuda(), \n",
    "                Variable(torch.tensor([0.0],dtype=torch.float)).cuda(),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        return torch.sum(\n",
    "            torch.max(\n",
    "                norm - Variable(torch.tensor([1.0],dtype=torch.float)), \n",
    "                Variable(torch.tensor([0.0],dtype=torch.float)),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEncoder(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(ValueEncoder, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        self.vocab_size = self.config[\"val\"][\"vocab_size\"]\n",
    "        self.embd_dim = self.config[\"val\"][\"embd_dim\"]\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embd_dim,\n",
    "            self.config[\"val\"][\"IDX_PAD\"],\n",
    "        )\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels = self.config[\"val\"][\"embd_dim\"],\n",
    "            out_channels = self.config[\"val\"][\"conv_n_kernels\"],\n",
    "            kernel_size = self.config[\"val\"][\"conv_kernel_size\"],\n",
    "        )\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size = self.config[\"val\"][\"pool_kernel_size\"],\n",
    "            padding = self.config[\"val\"][\"IDX_PAD\"],\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            self.config[\"val\"][\"conv_n_kernels\"],\n",
    "            self.config[\"embd_dim\"],\n",
    "        )\n",
    "        \n",
    "    def forward(self, bp_map):\n",
    "        # batched maps, (B, map_r, map_c)\n",
    "        # in this version, every value only contains 1 map\n",
    "        B = bp_map.shape[0]\n",
    "        \n",
    "        # (B, map_r, map_c, val_embd_dim) -> (B, val_embd_dim, map_r, map_c)\n",
    "        d_embd = self.embedding(bp_map).permute(0,3,1,2)\n",
    "        \n",
    "        # (B, n_kernel, map_r, 1)\n",
    "        d_conv = F.relu(self.conv(d_embd))\n",
    "        \n",
    "        # (B, n_kernel)\n",
    "        d_pool = self.pool(d_conv).view(B,self.config[\"val\"][\"conv_n_kernels\"])\n",
    "        \n",
    "        # (B, embd_dim)\n",
    "        d_out = torch.sigmoid(\n",
    "            self.fc(d_pool)\n",
    "        )\n",
    "        \n",
    "        return d_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphTransE(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(MorphTransE, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        self.value_encoder = ValueEncoder(p_config=p_config)\n",
    "        \n",
    "        self.fn_vocab_size = self.config[\"fn\"][\"vocab_size\"]\n",
    "        self.embd_dim = self.config[\"embd_dim\"]\n",
    "        \n",
    "        self.fn_embedding = nn.Embedding(\n",
    "            self.fn_vocab_size,\n",
    "            self.embd_dim,\n",
    "        )\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fn_embedding.weight.data)\n",
    "        self.fn_embedding.weight.data = F.normalize(\n",
    "            self.fn_embedding.weight.data, p=2, dim=1,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, batch_triplets):\n",
    "        # print(\"batch_triplets:{}\".format(batch_triplets))\n",
    "        \n",
    "        # v_in = self.val_embedding(batch_triplets[0])\n",
    "        v_in = self.value_encoder(batch_triplets[0])\n",
    "        v_fn = self.fn_embedding(batch_triplets[1])\n",
    "        # v_out = self.val_embedding(batch_triplets[2])\n",
    "        v_out = self.value_encoder(batch_triplets[2])\n",
    "        \n",
    "        # r_in = self.val_embedding(batch_triplets[3])\n",
    "        r_in = self.value_encoder(batch_triplets[3])\n",
    "        r_fn = self.fn_embedding(batch_triplets[4])\n",
    "        # r_out = self.val_embedding(batch_triplets[5])\n",
    "        r_out = self.value_encoder(batch_triplets[5])\n",
    "        \n",
    "        pos_score = torch.sum((v_in + v_fn - v_out) ** 2, 1)\n",
    "        neg_score = torch.sum((r_in + r_fn - r_out) ** 2, 1)\n",
    "        \n",
    "        # (B, 1), ..\n",
    "        return (pos_score, neg_score)\n",
    "    \n",
    "    def infer_fn(self, ios):\n",
    "        # ios: (\n",
    "        #   (B, n_maps, map_r, map_c), --> input\n",
    "        #   (B, n_maps, map_r, map_c), --> output\n",
    "        # )\n",
    "        B = ios[0].shape[0]\n",
    "        \n",
    "        # v_in = self.val_embedding(ios[0]) # (B, embd_dim)\n",
    "        # v_out = self.val_embedding(ios[1]) # (B, embd_dim)\n",
    "        v_in = self.value_encoder(ios[0])\n",
    "        v_out = self.value_encoder(ios[1])\n",
    "        \n",
    "        est_fn = v_out-v_in # (B, embd_dim)\n",
    "        # print(est_fn.shape)\n",
    "        # print(self.fn_embedding.weight.data.shape)\n",
    "        # input(\"PAUSE\")\n",
    "        \n",
    "        dlist = []\n",
    "        for i in range(B):\n",
    "            sublist = []\n",
    "            for j in range(self.fn_vocab_size):\n",
    "                sublist.append(\n",
    "                    torch.dist(est_fn[i,:], self.fn_embedding.weight.data[j,:]) # (1,)\n",
    "                )\n",
    "            dlist.append(\n",
    "                torch.tensor([sublist])\n",
    "            )\n",
    "        \n",
    "        ret_dlist = torch.cat(dlist,dim=0)\n",
    "        \n",
    "        return ret_dlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "use the same ld in trainer if sampling randomly every time\n",
    "'''\n",
    "def MTETester(p_model, p_ld_data):\n",
    "    \n",
    "    # ### FUNCTION prediction ### #\n",
    "    rank_list = []\n",
    "    for batch_idx, bts in enumerate(p_ld_data):\n",
    "        p_model.eval()\n",
    "        B = bts[0].shape[0]\n",
    "        \n",
    "        if use_cuda:\n",
    "            td_bts = [Variable(bts[i]).cuda() for i in range(len(bts))]\n",
    "        else:\n",
    "            td_bts = [Variable(bts[i]) for i in range(len(bts))]\n",
    "        \n",
    "        # feed the true input and output, and infer the function\n",
    "        # return the function scores\n",
    "        # (B, fn_vocab_size)\n",
    "        \n",
    "        d_scores = p_model.infer_fn(\n",
    "            (td_bts[0],td_bts[2])\n",
    "        )\n",
    "        \n",
    "        sorted_scores = torch.argsort(d_scores,dim=1).cpu().numpy()\n",
    "        for i in range(B):\n",
    "            rank_list.append(\n",
    "                sorted_scores[i,:].tolist().index(bts[1][i])\n",
    "            )\n",
    "            \n",
    "        print(\"\\r# TEST/FUNCTION B:{}\".format(batch_idx),end=\"\")\n",
    "        \n",
    "    print()\n",
    "    print(\"# TEST/FUNCTION avg.rank:{:.2f}, b:{:.2f}, w:{:.2f}, 50p:{:.2f}, 75p:{:.2f}, 90p:{:.2f}\".format(\n",
    "        sum(rank_list)/len(rank_list), \n",
    "        min(rank_list), \n",
    "        max(rank_list), \n",
    "        np.percentile(rank_list,50),\n",
    "        np.percentile(rank_list,75),\n",
    "        np.percentile(rank_list,90)\n",
    "    ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MTETrainer(p_nep, p_model, p_ld_train, p_ld_test, p_optim, p_lossfn):\n",
    "    MTETester(p_model, p_ld_test)\n",
    "    for d_ep in range(p_nep):\n",
    "        train_loss_list = []\n",
    "        for batch_idx, bts in enumerate(p_ld_train):\n",
    "            p_model.train()\n",
    "            if use_cuda:\n",
    "                td_bts = [Variable(bts[i]).cuda() for i in range(len(bts))]\n",
    "            else:\n",
    "                td_bts = [Variable(bts[i]) for i in range(len(bts))]\n",
    "            \n",
    "            d_scores = p_model(td_bts) # (pos,neg)\n",
    "            p_optim.zero_grad()\n",
    "            if use_cuda:\n",
    "                margin = Variable(torch.tensor([1],dtype=torch.float)).cuda()\n",
    "            else:\n",
    "                margin = Variable(torch.tensor([1],dtype=torch.float))\n",
    "            d_loss = p_lossfn(d_scores[0],d_scores[1],margin)\n",
    "            d_loss += norm_loss(p_model.value_encoder(td_bts[0]))\n",
    "            d_loss += norm_loss(p_model.fn_embedding(td_bts[1]))\n",
    "            d_loss += norm_loss(p_model.value_encoder(td_bts[2]))\n",
    "            d_loss += norm_loss(p_model.value_encoder(td_bts[3]))\n",
    "            d_loss += norm_loss(p_model.fn_embedding(td_bts[4]))\n",
    "            d_loss += norm_loss(p_model.value_encoder(td_bts[5]))\n",
    "            train_loss_list.append(d_loss)\n",
    "            d_loss.backward()\n",
    "            p_optim.step()\n",
    "            \n",
    "            print(\"\\r# TRAIN EP{}, B:{}, L:{:.4f}, AvgL:{:.4f}\".format(\n",
    "                d_ep, batch_idx, d_loss,\n",
    "                float(sum(train_loss_list))/float(len(train_loss_list))\n",
    "            ),end=\"\")\n",
    "        print()\n",
    "        if d_ep%10==0:\n",
    "            MTETester(p_model, p_ld_test)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_interpreter = MorpheusInterpreter()\n",
    "# m_spec = S.parse_file('./example/set_select.tyrell')\n",
    "# m_spec = S.parse_file('./example/set_unite.tyrell')\n",
    "# m_spec = S.parse_file('./example/camb2.tyrell')\n",
    "# m_spec = S.parse_file('./example/camb3.tyrell')\n",
    "m_spec = S.parse_file('./example/camb4.tyrell')\n",
    "m_generator = MorpheusGenerator(\n",
    "    spec=m_spec,\n",
    "    interpreter=m_interpreter,\n",
    "    sfn=m_interpreter.sanity_check,\n",
    ")\n",
    "# dumb Program Space\n",
    "m_dps = ProgramSpace(\n",
    "    m_spec, m_interpreter, None, None\n",
    ")\n",
    "\n",
    "\n",
    "dt_mg_train = MorphGenDataset(p_spec=m_spec, p_generator=m_generator, p_interpreter=m_interpreter, p_dps=m_dps, p_len=1024)\n",
    "ld_mg_train = DataLoader(dataset=dt_mg_train, batch_size=8, shuffle=False)\n",
    "\n",
    "dt_mg_test = MorphGenDataset(p_spec=m_spec, p_generator=m_generator, p_interpreter=m_interpreter, p_dps=m_dps, p_len=512)\n",
    "ld_mg_test = DataLoader(dataset=dt_mg_test, batch_size=8, shuffle=False)\n",
    "\n",
    "m_config = {\n",
    "    \"val\":{\n",
    "        \"vocab_size\": len(CAMB_LIST),\n",
    "        \"embd_dim\": 16, # embedding dim of CAMB abstract token\n",
    "        \"conv_n_kernels\": 512,\n",
    "        \"conv_kernel_size\": (1,CAMB_NCOL), \n",
    "        \"pool_kernel_size\": (CAMB_NROW,1), \n",
    "        \"IDX_PAD\": 0,\n",
    "    },\n",
    "    \"fn\":{\n",
    "        \"vocab_size\": len(dt_mg_train.shell_list)\n",
    "    },\n",
    "    \"embd_dim\":128,\n",
    "}\n",
    "\n",
    "mte = MorphTransE(p_config=m_config)\n",
    "m_loss = MarginLoss()\n",
    "if use_cuda:\n",
    "    mte = mte.cuda()\n",
    "    m_loss = m_loss.cuda()\n",
    "optimizer = torch.optim.Adam(mte.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': {'vocab_size': 150,\n",
       "  'embd_dim': 16,\n",
       "  'conv_n_kernels': 512,\n",
       "  'conv_kernel_size': (1, 15),\n",
       "  'pool_kernel_size': (15, 1),\n",
       "  'IDX_PAD': 0},\n",
       " 'fn': {'vocab_size': 156},\n",
       " 'embd_dim': 128}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:74.78, b:0.00, w:155.00, 50p:73.00, 75p:112.25, 90p:136.00\n",
      "# TRAIN EP0, B:127, L:7.6243, AvgL:41.9604\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:32.78, b:0.00, w:144.00, 50p:20.00, 75p:49.25, 90p:90.00\n",
      "# TRAIN EP1, B:127, L:7.7420, AvgL:6.4322\n",
      "# TRAIN EP2, B:127, L:6.7090, AvgL:5.5321\n",
      "# TRAIN EP3, B:127, L:3.6829, AvgL:5.1132\n",
      "# TRAIN EP4, B:127, L:1.8619, AvgL:4.9098\n",
      "# TRAIN EP5, B:127, L:4.6367, AvgL:4.8223\n",
      "# TRAIN EP6, B:127, L:3.6599, AvgL:4.8730\n",
      "# TRAIN EP7, B:127, L:4.2586, AvgL:4.7530\n",
      "# TRAIN EP8, B:127, L:3.4518, AvgL:4.5684\n",
      "# TRAIN EP9, B:127, L:3.4735, AvgL:4.3515\n",
      "# TRAIN EP10, B:127, L:4.0161, AvgL:4.6847\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:29.92, b:0.00, w:155.00, 50p:14.00, 75p:47.00, 90p:79.80\n",
      "# TRAIN EP11, B:127, L:7.7637, AvgL:4.7202\n",
      "# TRAIN EP12, B:127, L:5.5639, AvgL:4.1479\n",
      "# TRAIN EP13, B:127, L:4.3381, AvgL:4.0546\n",
      "# TRAIN EP14, B:127, L:5.4213, AvgL:4.1779\n",
      "# TRAIN EP15, B:127, L:4.3365, AvgL:3.8477\n",
      "# TRAIN EP16, B:127, L:3.1399, AvgL:3.6550\n",
      "# TRAIN EP17, B:127, L:2.2804, AvgL:3.6163\n",
      "# TRAIN EP18, B:127, L:3.9054, AvgL:3.7687\n",
      "# TRAIN EP19, B:127, L:5.0815, AvgL:3.4355\n",
      "# TRAIN EP20, B:127, L:3.4431, AvgL:3.3049\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:19.81, b:0.00, w:155.00, 50p:7.00, 75p:27.00, 90p:62.90\n",
      "# TRAIN EP21, B:127, L:4.1231, AvgL:3.4545\n",
      "# TRAIN EP22, B:127, L:4.5116, AvgL:3.3103\n",
      "# TRAIN EP23, B:127, L:2.2200, AvgL:3.3707\n",
      "# TRAIN EP24, B:127, L:5.0561, AvgL:3.0060\n",
      "# TRAIN EP25, B:127, L:3.9183, AvgL:3.1894\n",
      "# TRAIN EP26, B:127, L:3.0204, AvgL:2.7996\n",
      "# TRAIN EP27, B:127, L:4.4911, AvgL:3.0924\n",
      "# TRAIN EP28, B:127, L:4.0004, AvgL:2.9092\n",
      "# TRAIN EP29, B:127, L:3.3548, AvgL:2.5918\n",
      "# TRAIN EP30, B:127, L:2.3513, AvgL:2.6393\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:15.41, b:0.00, w:152.00, 50p:6.00, 75p:15.00, 90p:46.90\n",
      "# TRAIN EP31, B:127, L:3.4344, AvgL:2.7279\n",
      "# TRAIN EP32, B:127, L:3.4012, AvgL:2.4678\n",
      "# TRAIN EP33, B:127, L:4.8317, AvgL:2.4965\n",
      "# TRAIN EP34, B:127, L:3.7065, AvgL:2.7226\n",
      "# TRAIN EP35, B:127, L:4.8067, AvgL:2.6201\n",
      "# TRAIN EP36, B:127, L:4.0850, AvgL:2.3685\n",
      "# TRAIN EP37, B:127, L:2.6714, AvgL:2.6246\n",
      "# TRAIN EP38, B:127, L:2.1227, AvgL:2.2002\n",
      "# TRAIN EP39, B:127, L:1.4307, AvgL:2.2260\n",
      "# TRAIN EP40, B:127, L:2.6056, AvgL:2.3044\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:11.52, b:0.00, w:112.00, 50p:5.00, 75p:12.00, 90p:25.00\n",
      "# TRAIN EP41, B:127, L:1.4169, AvgL:2.1457\n",
      "# TRAIN EP42, B:127, L:0.6224, AvgL:2.0349\n",
      "# TRAIN EP43, B:127, L:1.5449, AvgL:2.1192\n",
      "# TRAIN EP44, B:127, L:1.2368, AvgL:2.2288\n",
      "# TRAIN EP45, B:127, L:1.4836, AvgL:1.8754\n",
      "# TRAIN EP46, B:127, L:0.9974, AvgL:1.8329\n",
      "# TRAIN EP47, B:127, L:2.6789, AvgL:1.9982\n",
      "# TRAIN EP48, B:127, L:3.8739, AvgL:2.0649\n",
      "# TRAIN EP49, B:127, L:1.8234, AvgL:2.1123\n",
      "# TRAIN EP50, B:127, L:0.7936, AvgL:1.8821\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:11.19, b:0.00, w:135.00, 50p:5.00, 75p:13.00, 90p:25.00\n",
      "# TRAIN EP51, B:127, L:1.8446, AvgL:2.1026\n",
      "# TRAIN EP52, B:127, L:2.3320, AvgL:1.7901\n",
      "# TRAIN EP53, B:127, L:2.1713, AvgL:1.9313\n",
      "# TRAIN EP54, B:127, L:2.4208, AvgL:1.8476\n",
      "# TRAIN EP55, B:127, L:1.0617, AvgL:1.7493\n",
      "# TRAIN EP56, B:127, L:1.2563, AvgL:1.9820\n",
      "# TRAIN EP57, B:127, L:3.3975, AvgL:1.7633\n",
      "# TRAIN EP58, B:127, L:2.5668, AvgL:1.7488\n",
      "# TRAIN EP59, B:127, L:0.7381, AvgL:1.7057\n",
      "# TRAIN EP60, B:127, L:3.2591, AvgL:1.8206\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:9.68, b:0.00, w:153.00, 50p:5.00, 75p:12.00, 90p:22.00\n",
      "# TRAIN EP61, B:127, L:1.6752, AvgL:1.7129\n",
      "# TRAIN EP62, B:127, L:3.8493, AvgL:1.8446\n",
      "# TRAIN EP63, B:127, L:1.3794, AvgL:1.7157\n",
      "# TRAIN EP64, B:127, L:2.1522, AvgL:1.6822\n",
      "# TRAIN EP65, B:127, L:1.9763, AvgL:1.6453\n",
      "# TRAIN EP66, B:127, L:3.7245, AvgL:1.8409\n",
      "# TRAIN EP67, B:127, L:2.9487, AvgL:1.6783\n",
      "# TRAIN EP68, B:127, L:1.3730, AvgL:1.6328\n",
      "# TRAIN EP69, B:127, L:1.9528, AvgL:1.5607\n",
      "# TRAIN EP70, B:127, L:2.9722, AvgL:1.7665\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:8.92, b:0.00, w:83.00, 50p:5.00, 75p:12.00, 90p:21.90\n",
      "# TRAIN EP71, B:127, L:3.0871, AvgL:1.8003\n",
      "# TRAIN EP72, B:127, L:0.6879, AvgL:1.7915\n",
      "# TRAIN EP73, B:127, L:1.4003, AvgL:1.9555\n",
      "# TRAIN EP74, B:127, L:0.5450, AvgL:1.7511\n",
      "# TRAIN EP75, B:127, L:3.4097, AvgL:1.7198\n",
      "# TRAIN EP76, B:127, L:4.6794, AvgL:1.6383\n",
      "# TRAIN EP77, B:127, L:0.1657, AvgL:1.6149\n",
      "# TRAIN EP78, B:127, L:2.3115, AvgL:1.7223\n",
      "# TRAIN EP79, B:127, L:0.5584, AvgL:1.6608\n",
      "# TRAIN EP80, B:127, L:4.1983, AvgL:1.7835\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:8.54, b:0.00, w:92.00, 50p:4.00, 75p:12.00, 90p:20.00\n",
      "# TRAIN EP81, B:127, L:2.3394, AvgL:1.8247\n",
      "# TRAIN EP82, B:127, L:2.1736, AvgL:1.7106\n",
      "# TRAIN EP83, B:127, L:0.2970, AvgL:1.6160\n",
      "# TRAIN EP84, B:127, L:2.3680, AvgL:1.8305\n",
      "# TRAIN EP85, B:127, L:1.5325, AvgL:1.6311\n",
      "# TRAIN EP86, B:127, L:2.0429, AvgL:1.7680\n",
      "# TRAIN EP87, B:127, L:1.5810, AvgL:1.6195\n",
      "# TRAIN EP88, B:127, L:2.6576, AvgL:1.8756\n",
      "# TRAIN EP89, B:127, L:3.0368, AvgL:1.6854\n",
      "# TRAIN EP90, B:127, L:2.3958, AvgL:1.6729\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:8.41, b:0.00, w:116.00, 50p:4.00, 75p:10.00, 90p:18.90\n",
      "# TRAIN EP91, B:127, L:0.3118, AvgL:1.4962\n",
      "# TRAIN EP92, B:127, L:1.1938, AvgL:1.5542\n",
      "# TRAIN EP93, B:127, L:2.1720, AvgL:1.6937\n",
      "# TRAIN EP94, B:127, L:2.2836, AvgL:1.5622\n",
      "# TRAIN EP95, B:127, L:2.8581, AvgL:1.6606\n",
      "# TRAIN EP96, B:127, L:2.1541, AvgL:1.5872\n",
      "# TRAIN EP97, B:127, L:1.2739, AvgL:1.4849\n",
      "# TRAIN EP98, B:127, L:1.6771, AvgL:1.6562\n",
      "# TRAIN EP99, B:127, L:1.7107, AvgL:1.6024\n",
      "# TRAIN EP100, B:127, L:1.9046, AvgL:1.7959\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:9.61, b:0.00, w:107.00, 50p:5.00, 75p:13.00, 90p:22.00\n",
      "# TRAIN EP101, B:127, L:3.2964, AvgL:1.6964\n",
      "# TRAIN EP102, B:127, L:0.7357, AvgL:1.6162\n",
      "# TRAIN EP103, B:127, L:0.9850, AvgL:1.7693\n",
      "# TRAIN EP104, B:127, L:0.2195, AvgL:1.6529\n",
      "# TRAIN EP105, B:127, L:0.6767, AvgL:1.7386\n",
      "# TRAIN EP106, B:127, L:3.8507, AvgL:1.8290\n",
      "# TRAIN EP107, B:127, L:0.0021, AvgL:1.6450\n",
      "# TRAIN EP108, B:127, L:2.7017, AvgL:1.4345\n",
      "# TRAIN EP109, B:127, L:1.2285, AvgL:1.6595\n",
      "# TRAIN EP110, B:127, L:0.1728, AvgL:1.6025\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:9.03, b:0.00, w:152.00, 50p:4.00, 75p:11.00, 90p:20.00\n",
      "# TRAIN EP111, B:127, L:3.0181, AvgL:1.5018\n",
      "# TRAIN EP112, B:127, L:2.5492, AvgL:1.6518\n",
      "# TRAIN EP113, B:127, L:5.2427, AvgL:1.7222\n",
      "# TRAIN EP114, B:127, L:1.5479, AvgL:1.7066\n",
      "# TRAIN EP115, B:127, L:1.8718, AvgL:1.5456\n",
      "# TRAIN EP116, B:127, L:1.0784, AvgL:1.5878\n",
      "# TRAIN EP117, B:127, L:1.9183, AvgL:1.7371\n",
      "# TRAIN EP118, B:127, L:1.8330, AvgL:1.4869\n",
      "# TRAIN EP119, B:127, L:1.5047, AvgL:1.6283\n",
      "# TRAIN EP120, B:127, L:3.6335, AvgL:1.5830\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:8.75, b:0.00, w:147.00, 50p:4.00, 75p:11.00, 90p:19.00\n",
      "# TRAIN EP121, B:127, L:0.3506, AvgL:1.4893\n",
      "# TRAIN EP122, B:127, L:1.1409, AvgL:1.4972\n",
      "# TRAIN EP123, B:127, L:2.0654, AvgL:1.4553\n",
      "# TRAIN EP124, B:127, L:1.5750, AvgL:1.4792\n",
      "# TRAIN EP125, B:127, L:1.3961, AvgL:1.4533\n",
      "# TRAIN EP126, B:127, L:1.4180, AvgL:1.6473\n",
      "# TRAIN EP127, B:127, L:6.1061, AvgL:1.6169\n",
      "# TRAIN EP128, B:127, L:3.0900, AvgL:1.5642\n",
      "# TRAIN EP129, B:127, L:1.4195, AvgL:1.4902\n",
      "# TRAIN EP130, B:127, L:3.1176, AvgL:1.5354\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:7.79, b:0.00, w:98.00, 50p:4.00, 75p:10.00, 90p:18.00\n",
      "# TRAIN EP131, B:127, L:2.4448, AvgL:1.5637\n",
      "# TRAIN EP132, B:127, L:2.1881, AvgL:1.4394\n",
      "# TRAIN EP133, B:127, L:0.4288, AvgL:1.6391\n",
      "# TRAIN EP134, B:127, L:0.9280, AvgL:1.5727\n",
      "# TRAIN EP135, B:127, L:1.2511, AvgL:1.5050\n",
      "# TRAIN EP136, B:127, L:1.6699, AvgL:1.4190\n",
      "# TRAIN EP137, B:127, L:0.6998, AvgL:1.3274\n",
      "# TRAIN EP138, B:127, L:1.0357, AvgL:1.6100\n",
      "# TRAIN EP139, B:127, L:1.0808, AvgL:1.6782\n",
      "# TRAIN EP140, B:127, L:2.4921, AvgL:1.6427\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:9.05, b:0.00, w:117.00, 50p:4.00, 75p:10.25, 90p:20.90\n",
      "# TRAIN EP141, B:127, L:2.4845, AvgL:1.4275\n",
      "# TRAIN EP142, B:127, L:0.7123, AvgL:1.3526\n",
      "# TRAIN EP143, B:127, L:0.6177, AvgL:1.6791\n",
      "# TRAIN EP144, B:127, L:3.4533, AvgL:1.4067\n",
      "# TRAIN EP145, B:127, L:1.0326, AvgL:1.3705\n",
      "# TRAIN EP146, B:127, L:1.7528, AvgL:1.2969\n",
      "# TRAIN EP147, B:127, L:0.4544, AvgL:1.5151\n",
      "# TRAIN EP148, B:127, L:1.0294, AvgL:1.4047\n",
      "# TRAIN EP149, B:127, L:2.9174, AvgL:1.5302\n",
      "# TRAIN EP150, B:127, L:0.1041, AvgL:1.5094\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:8.94, b:0.00, w:133.00, 50p:4.00, 75p:11.00, 90p:23.00\n",
      "# TRAIN EP151, B:127, L:0.2381, AvgL:1.5904\n",
      "# TRAIN EP152, B:127, L:2.0737, AvgL:1.4995\n",
      "# TRAIN EP153, B:127, L:2.2700, AvgL:1.4415\n",
      "# TRAIN EP154, B:127, L:1.3616, AvgL:1.4723\n",
      "# TRAIN EP155, B:127, L:1.0148, AvgL:1.4998\n",
      "# TRAIN EP156, B:127, L:0.3776, AvgL:1.4739\n",
      "# TRAIN EP157, B:127, L:1.5980, AvgL:1.3646\n",
      "# TRAIN EP158, B:127, L:1.9548, AvgL:1.3658\n",
      "# TRAIN EP159, B:127, L:1.2163, AvgL:1.4153\n",
      "# TRAIN EP160, B:127, L:2.0654, AvgL:1.5251\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:8.78, b:0.00, w:123.00, 50p:5.00, 75p:11.00, 90p:19.90\n",
      "# TRAIN EP161, B:127, L:1.2073, AvgL:1.3998\n",
      "# TRAIN EP162, B:127, L:0.1405, AvgL:1.2762\n",
      "# TRAIN EP163, B:127, L:3.5988, AvgL:1.4104\n",
      "# TRAIN EP164, B:127, L:0.8289, AvgL:1.3723\n",
      "# TRAIN EP165, B:127, L:1.8099, AvgL:1.4638\n",
      "# TRAIN EP166, B:127, L:3.4464, AvgL:1.3038\n",
      "# TRAIN EP167, B:127, L:1.5192, AvgL:1.4821\n",
      "# TRAIN EP168, B:127, L:1.5210, AvgL:1.2413\n",
      "# TRAIN EP169, B:127, L:0.0258, AvgL:1.2926\n",
      "# TRAIN EP170, B:127, L:1.1695, AvgL:1.4761\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:8.12, b:0.00, w:141.00, 50p:4.00, 75p:10.00, 90p:18.00\n",
      "# TRAIN EP171, B:127, L:2.6925, AvgL:1.3292\n",
      "# TRAIN EP172, B:127, L:1.2700, AvgL:1.4033\n",
      "# TRAIN EP173, B:127, L:0.2035, AvgL:1.4444\n",
      "# TRAIN EP174, B:127, L:0.1982, AvgL:1.3787\n",
      "# TRAIN EP175, B:127, L:1.4957, AvgL:1.4373\n",
      "# TRAIN EP176, B:127, L:1.0471, AvgL:1.3150\n",
      "# TRAIN EP177, B:127, L:0.8717, AvgL:1.2516\n",
      "# TRAIN EP178, B:127, L:0.3800, AvgL:1.4210\n",
      "# TRAIN EP179, B:127, L:2.9873, AvgL:1.3157\n",
      "# TRAIN EP180, B:127, L:1.1123, AvgL:1.3536\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:8.26, b:0.00, w:130.00, 50p:5.00, 75p:12.00, 90p:19.90\n",
      "# TRAIN EP181, B:127, L:1.0432, AvgL:1.3744\n",
      "# TRAIN EP182, B:127, L:0.9574, AvgL:1.2537\n",
      "# TRAIN EP183, B:127, L:1.0197, AvgL:1.3382\n",
      "# TRAIN EP184, B:127, L:3.1646, AvgL:1.3227\n",
      "# TRAIN EP185, B:127, L:0.5114, AvgL:1.2157\n",
      "# TRAIN EP186, B:127, L:0.5607, AvgL:1.3123\n",
      "# TRAIN EP187, B:59, L:0.2749, AvgL:1.3484Traceback (most recent call last):\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-15-b312d53f6add>\", line 1, in <module>\n",
      "    MTETrainer(1000000, mte, ld_mg_train, ld_mg_test, optimizer, m_loss)\n",
      "  File \"<ipython-input-12-6826c09534c1>\", line 26, in MTETrainer\n",
      "    d_loss.backward()\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/torch/tensor.py\", line 102, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 90, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2018, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/inspect.py\", line 1500, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/inspect.py\", line 1458, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/posixpath.py\", line 388, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/posixpath.py\", line 422, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/ju-ucsb/anaconda3/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "MTETrainer(1000000, mte, ld_mg_train, ld_mg_test, optimizer, m_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
