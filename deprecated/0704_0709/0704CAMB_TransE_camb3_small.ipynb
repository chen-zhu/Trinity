{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component/Abstraction Representation Learning using TransE\n",
    "- Stage: Cambrian\n",
    "- Version: Charniodiscus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "print(\"use_cuda: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morpheus Version\n",
    "from utils_morpheus import *\n",
    "from ProgramSpace import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphGenDataset(Dataset):\n",
    "    def __init__(self, p_spec=None, p_generator=None, p_interpreter=None, p_dps=None, p_len=None):\n",
    "        self.spec = p_spec\n",
    "        self.generator = p_generator\n",
    "        self.interpreter = p_interpreter\n",
    "        self.dps = p_dps\n",
    "        self.len = p_len\n",
    "        \n",
    "        # construct a shell list/dict so that every function call (shell) has id\n",
    "        self.shell_list = self.dps.get_neighboring_shells()\n",
    "        self.shell_dict = {\n",
    "            self.shell_list[i]:i for i in range(len(self.shell_list))\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, p_ind):\n",
    "        # basically ignore the p_ind parameter and generate randomly\n",
    "        # returns a pair of abstraction maps\n",
    "        # (tmp_in, tmp_out)\n",
    "        # which is (n_maps, CAMB_NROW, CAMB_NCOL)\n",
    "        # currently assuming single input single output\n",
    "        \n",
    "        while True:\n",
    "            tmp_input = self.interpreter.random_table()\n",
    "            tmp_prog, tmp_example = self.generator.generate(\n",
    "                fixed_depth=2, # this should be fixed to 2 since we learn size 1\n",
    "                example=Example(input=[tmp_input], output=None),\n",
    "            )\n",
    "            if tmp_prog is not None and tmp_prog.is_apply():\n",
    "                break\n",
    "\n",
    "        tmp_in = camb_get_abs(tmp_example.input[0])\n",
    "        tmp_out= camb_get_abs(tmp_example.output)\n",
    "\n",
    "        tmp_func = self.shell_dict[\n",
    "            (\n",
    "                self.dps.prod_list.index(tmp_prog.production),\n",
    "                tuple(\n",
    "                    [self.dps.node_list.index(tmp_prog.args[i])\n",
    "                    for i in range(len(tmp_prog.args))]\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # =========================================================\n",
    "        # you should also produce a negative sample here and return\n",
    "\n",
    "        while True:\n",
    "            rep_in = tmp_in\n",
    "            rep_out = tmp_out\n",
    "            rep_func = random.choice(range(len(self.shell_list)))\n",
    "            if rep_func==tmp_func:\n",
    "                continue\n",
    "            else:\n",
    "                # in fact, not safe\n",
    "                # should verify that rep_func(rep_in)!=rep_out\n",
    "                # but never mind here\n",
    "                break\n",
    "        \n",
    "        return (tmp_in, tmp_func, tmp_out,\n",
    "                rep_in, rep_func, rep_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarginLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MarginLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pos, neg, margin):\n",
    "        if use_cuda:\n",
    "            zero_tensor = torch.tensor(pos.size(),dtype=torch.float).cuda()\n",
    "        else:\n",
    "            zero_tensor = torch.tensor(pos.size(),dtype=torch.float)\n",
    "        zero_tensor.zero_()\n",
    "        zero_tensor = Variable(zero_tensor)\n",
    "        return torch.sum(torch.max(pos - neg + margin, zero_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_loss(embeddings, dim=1):\n",
    "    norm = torch.sum(embeddings ** 2, dim=dim, keepdim=True)\n",
    "    if use_cuda:\n",
    "        return torch.sum(\n",
    "            torch.max(\n",
    "                norm - Variable(torch.tensor([1.0],dtype=torch.float)).cuda(), \n",
    "                Variable(torch.tensor([0.0],dtype=torch.float)).cuda(),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        return torch.sum(\n",
    "            torch.max(\n",
    "                norm - Variable(torch.tensor([1.0],dtype=torch.float)), \n",
    "                Variable(torch.tensor([0.0],dtype=torch.float)),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEncoder(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(ValueEncoder, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        self.vocab_size = self.config[\"val\"][\"vocab_size\"]\n",
    "        self.embd_dim = self.config[\"val\"][\"embd_dim\"]\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embd_dim,\n",
    "            self.config[\"val\"][\"IDX_PAD\"],\n",
    "        )\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels = self.config[\"val\"][\"embd_dim\"],\n",
    "            out_channels = self.config[\"val\"][\"conv_n_kernels\"],\n",
    "            kernel_size = self.config[\"val\"][\"conv_kernel_size\"],\n",
    "        )\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size = self.config[\"val\"][\"pool_kernel_size\"],\n",
    "            padding = self.config[\"val\"][\"IDX_PAD\"],\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            self.config[\"val\"][\"conv_n_kernels\"],\n",
    "            self.config[\"embd_dim\"],\n",
    "        )\n",
    "        \n",
    "    def forward(self, bp_map):\n",
    "        # batched maps, (B, map_r, map_c)\n",
    "        # in this version, every value only contains 1 map\n",
    "        B = bp_map.shape[0]\n",
    "        \n",
    "        # (B, map_r, map_c, val_embd_dim) -> (B, val_embd_dim, map_r, map_c)\n",
    "        d_embd = self.embedding(bp_map).permute(0,3,1,2)\n",
    "        \n",
    "        # (B, n_kernel, map_r, 1)\n",
    "        d_conv = F.relu(self.conv(d_embd))\n",
    "        \n",
    "        # (B, n_kernel)\n",
    "        d_pool = self.pool(d_conv).view(B,self.config[\"val\"][\"conv_n_kernels\"])\n",
    "        \n",
    "        # (B, embd_dim)\n",
    "        d_out = torch.sigmoid(\n",
    "            self.fc(d_pool)\n",
    "        )\n",
    "        \n",
    "        return d_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphTransE(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(MorphTransE, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        self.value_encoder = ValueEncoder(p_config=p_config)\n",
    "        \n",
    "        self.fn_vocab_size = self.config[\"fn\"][\"vocab_size\"]\n",
    "        self.embd_dim = self.config[\"embd_dim\"]\n",
    "        \n",
    "        self.fn_embedding = nn.Embedding(\n",
    "            self.fn_vocab_size,\n",
    "            self.embd_dim,\n",
    "        )\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fn_embedding.weight.data)\n",
    "        self.fn_embedding.weight.data = F.normalize(\n",
    "            self.fn_embedding.weight.data, p=2, dim=1,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, batch_triplets):\n",
    "        # print(\"batch_triplets:{}\".format(batch_triplets))\n",
    "        \n",
    "        # v_in = self.val_embedding(batch_triplets[0])\n",
    "        v_in = self.value_encoder(batch_triplets[0])\n",
    "        v_fn = self.fn_embedding(batch_triplets[1])\n",
    "        # v_out = self.val_embedding(batch_triplets[2])\n",
    "        v_out = self.value_encoder(batch_triplets[2])\n",
    "        \n",
    "        # r_in = self.val_embedding(batch_triplets[3])\n",
    "        r_in = self.value_encoder(batch_triplets[3])\n",
    "        r_fn = self.fn_embedding(batch_triplets[4])\n",
    "        # r_out = self.val_embedding(batch_triplets[5])\n",
    "        r_out = self.value_encoder(batch_triplets[5])\n",
    "        \n",
    "        pos_score = torch.sum((v_in + v_fn - v_out) ** 2, 1)\n",
    "        neg_score = torch.sum((r_in + r_fn - r_out) ** 2, 1)\n",
    "        \n",
    "        # (B, 1), ..\n",
    "        return (pos_score, neg_score)\n",
    "    \n",
    "    def infer_fn(self, ios):\n",
    "        # ios: (\n",
    "        #   (B, n_maps, map_r, map_c), --> input\n",
    "        #   (B, n_maps, map_r, map_c), --> output\n",
    "        # )\n",
    "        B = ios[0].shape[0]\n",
    "        \n",
    "        # v_in = self.val_embedding(ios[0]) # (B, embd_dim)\n",
    "        # v_out = self.val_embedding(ios[1]) # (B, embd_dim)\n",
    "        v_in = self.value_encoder(ios[0])\n",
    "        v_out = self.value_encoder(ios[1])\n",
    "        \n",
    "        est_fn = v_out-v_in # (B, embd_dim)\n",
    "        # print(est_fn.shape)\n",
    "        # print(self.fn_embedding.weight.data.shape)\n",
    "        # input(\"PAUSE\")\n",
    "        \n",
    "        dlist = []\n",
    "        for i in range(B):\n",
    "            sublist = []\n",
    "            for j in range(self.fn_vocab_size):\n",
    "                sublist.append(\n",
    "                    torch.dist(est_fn[i,:], self.fn_embedding.weight.data[j,:]) # (1,)\n",
    "                )\n",
    "            dlist.append(\n",
    "                torch.tensor([sublist])\n",
    "            )\n",
    "        \n",
    "        ret_dlist = torch.cat(dlist,dim=0)\n",
    "        \n",
    "        return ret_dlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "use the same ld in trainer if sampling randomly every time\n",
    "'''\n",
    "def MTETester(p_model, p_ld_data):\n",
    "    \n",
    "    # ### FUNCTION prediction ### #\n",
    "    rank_list = []\n",
    "    for batch_idx, bts in enumerate(p_ld_data):\n",
    "        p_model.eval()\n",
    "        B = bts[0].shape[0]\n",
    "        \n",
    "        if use_cuda:\n",
    "            td_bts = [Variable(bts[i]).cuda() for i in range(len(bts))]\n",
    "        else:\n",
    "            td_bts = [Variable(bts[i]) for i in range(len(bts))]\n",
    "        \n",
    "        # feed the true input and output, and infer the function\n",
    "        # return the function scores\n",
    "        # (B, fn_vocab_size)\n",
    "        \n",
    "        d_scores = p_model.infer_fn(\n",
    "            (td_bts[0],td_bts[2])\n",
    "        )\n",
    "        \n",
    "        sorted_scores = torch.argsort(d_scores,dim=1).cpu().numpy()\n",
    "        for i in range(B):\n",
    "            rank_list.append(\n",
    "                sorted_scores[i,:].tolist().index(bts[1][i])\n",
    "            )\n",
    "            \n",
    "        print(\"\\r# TEST/FUNCTION B:{}\".format(batch_idx),end=\"\")\n",
    "        \n",
    "    print()\n",
    "    print(\"# TEST/FUNCTION avg.rank:{:.2f}, b:{:.2f}, w:{:.2f}, 50p:{:.2f}, 75p:{:.2f}, 90p:{:.2f}\".format(\n",
    "        sum(rank_list)/len(rank_list), \n",
    "        min(rank_list), \n",
    "        max(rank_list), \n",
    "        np.percentile(rank_list,50),\n",
    "        np.percentile(rank_list,75),\n",
    "        np.percentile(rank_list,90)\n",
    "    ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MTETrainer(p_nep, p_model, p_ld_train, p_ld_test, p_optim, p_lossfn):\n",
    "    MTETester(p_model, p_ld_test)\n",
    "    for d_ep in range(p_nep):\n",
    "        train_loss_list = []\n",
    "        for batch_idx, bts in enumerate(p_ld_train):\n",
    "            p_model.train()\n",
    "            if use_cuda:\n",
    "                td_bts = [Variable(bts[i]).cuda() for i in range(len(bts))]\n",
    "            else:\n",
    "                td_bts = [Variable(bts[i]) for i in range(len(bts))]\n",
    "            \n",
    "            d_scores = p_model(td_bts) # (pos,neg)\n",
    "            p_optim.zero_grad()\n",
    "            if use_cuda:\n",
    "                margin = Variable(torch.tensor([1],dtype=torch.float)).cuda()\n",
    "            else:\n",
    "                margin = Variable(torch.tensor([1],dtype=torch.float))\n",
    "            d_loss = p_lossfn(d_scores[0],d_scores[1],margin)\n",
    "            d_loss += norm_loss(p_model.value_encoder(td_bts[0]))\n",
    "            d_loss += norm_loss(p_model.fn_embedding(td_bts[1]))\n",
    "            d_loss += norm_loss(p_model.value_encoder(td_bts[2]))\n",
    "            d_loss += norm_loss(p_model.value_encoder(td_bts[3]))\n",
    "            d_loss += norm_loss(p_model.fn_embedding(td_bts[4]))\n",
    "            d_loss += norm_loss(p_model.value_encoder(td_bts[5]))\n",
    "            train_loss_list.append(d_loss)\n",
    "            d_loss.backward()\n",
    "            p_optim.step()\n",
    "            \n",
    "            print(\"\\r# TRAIN EP{}, B:{}, L:{:.4f}, AvgL:{:.4f}\".format(\n",
    "                d_ep, batch_idx, d_loss,\n",
    "                float(sum(train_loss_list))/float(len(train_loss_list))\n",
    "            ),end=\"\")\n",
    "        print()\n",
    "        if d_ep%10==0:\n",
    "            MTETester(p_model, p_ld_test)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_interpreter = MorpheusInterpreter()\n",
    "# m_spec = S.parse_file('./example/set_select.tyrell')\n",
    "# m_spec = S.parse_file('./example/set_unite.tyrell')\n",
    "# m_spec = S.parse_file('./example/camb2.tyrell')\n",
    "m_spec = S.parse_file('./example/camb3.tyrell')\n",
    "m_generator = MorpheusGenerator(\n",
    "    spec=m_spec,\n",
    "    interpreter=m_interpreter,\n",
    "    sfn=m_interpreter.sanity_check,\n",
    ")\n",
    "# dumb Program Space\n",
    "m_dps = ProgramSpace(\n",
    "    m_spec, m_interpreter, None, None\n",
    ")\n",
    "\n",
    "\n",
    "dt_mg_train = MorphGenDataset(p_spec=m_spec, p_generator=m_generator, p_interpreter=m_interpreter, p_dps=m_dps, p_len=1024)\n",
    "ld_mg_train = DataLoader(dataset=dt_mg_train, batch_size=8, shuffle=False)\n",
    "\n",
    "dt_mg_test = MorphGenDataset(p_spec=m_spec, p_generator=m_generator, p_interpreter=m_interpreter, p_dps=m_dps, p_len=512)\n",
    "ld_mg_test = DataLoader(dataset=dt_mg_test, batch_size=8, shuffle=False)\n",
    "\n",
    "m_config = {\n",
    "    \"val\":{\n",
    "        \"vocab_size\": len(CAMB_LIST),\n",
    "        \"embd_dim\": 8, # embedding dim of CAMB abstract token\n",
    "        \"conv_n_kernels\": 256,\n",
    "        \"conv_kernel_size\": (1,CAMB_NCOL), \n",
    "        \"pool_kernel_size\": (CAMB_NROW,1), \n",
    "        \"IDX_PAD\": 0,\n",
    "    },\n",
    "    \"fn\":{\n",
    "        \"vocab_size\": len(dt_mg_train.shell_list)\n",
    "    },\n",
    "    \"embd_dim\":64,\n",
    "}\n",
    "\n",
    "mte = MorphTransE(p_config=m_config)\n",
    "m_loss = MarginLoss()\n",
    "if use_cuda:\n",
    "    mte = mte.cuda()\n",
    "    m_loss = m_loss.cuda()\n",
    "optimizer = torch.optim.Adam(mte.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': {'vocab_size': 150,\n",
       "  'embd_dim': 8,\n",
       "  'conv_n_kernels': 256,\n",
       "  'conv_kernel_size': (1, 15),\n",
       "  'pool_kernel_size': (15, 1),\n",
       "  'IDX_PAD': 0},\n",
       " 'fn': {'vocab_size': 120},\n",
       " 'embd_dim': 64}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:64.48, b:0.00, w:119.00, 50p:68.00, 75p:94.00, 90p:107.00\n",
      "# TRAIN EP0, B:127, L:9.3577, AvgL:36.3649\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:25.42, b:0.00, w:119.00, 50p:17.00, 75p:39.00, 90p:71.00\n",
      "# TRAIN EP1, B:127, L:4.5395, AvgL:6.7211\n",
      "# TRAIN EP2, B:127, L:6.0406, AvgL:5.5295\n",
      "# TRAIN EP3, B:127, L:4.6084, AvgL:5.1398\n",
      "# TRAIN EP4, B:127, L:5.3504, AvgL:4.9087\n",
      "# TRAIN EP5, B:127, L:1.4634, AvgL:4.5830\n",
      "# TRAIN EP6, B:127, L:3.2872, AvgL:4.6523\n",
      "# TRAIN EP7, B:127, L:3.0792, AvgL:4.6194\n",
      "# TRAIN EP8, B:127, L:4.5013, AvgL:4.4774\n",
      "# TRAIN EP9, B:127, L:5.3736, AvgL:4.3461\n",
      "# TRAIN EP10, B:127, L:4.5953, AvgL:4.2217\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:21.93, b:0.00, w:119.00, 50p:11.00, 75p:34.00, 90p:55.00\n",
      "# TRAIN EP11, B:127, L:4.0133, AvgL:4.1079\n",
      "# TRAIN EP12, B:127, L:3.9041, AvgL:4.1475\n",
      "# TRAIN EP13, B:127, L:3.3477, AvgL:4.1857\n",
      "# TRAIN EP14, B:127, L:5.1747, AvgL:4.0459\n",
      "# TRAIN EP15, B:127, L:3.9677, AvgL:3.8783\n",
      "# TRAIN EP16, B:127, L:3.8669, AvgL:4.0697\n",
      "# TRAIN EP17, B:127, L:3.3119, AvgL:3.8409\n",
      "# TRAIN EP18, B:127, L:3.8584, AvgL:3.8992\n",
      "# TRAIN EP19, B:127, L:4.0276, AvgL:3.5606\n",
      "# TRAIN EP20, B:127, L:1.1121, AvgL:3.6599\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:15.85, b:0.00, w:102.00, 50p:7.00, 75p:22.00, 90p:42.00\n",
      "# TRAIN EP21, B:127, L:2.9228, AvgL:3.4032\n",
      "# TRAIN EP22, B:127, L:2.7470, AvgL:3.3705\n",
      "# TRAIN EP23, B:127, L:4.1509, AvgL:2.9195\n",
      "# TRAIN EP24, B:127, L:4.0621, AvgL:3.0124\n",
      "# TRAIN EP25, B:127, L:4.0134, AvgL:2.7699\n",
      "# TRAIN EP26, B:127, L:4.3798, AvgL:2.7158\n",
      "# TRAIN EP27, B:127, L:2.7364, AvgL:2.6884\n",
      "# TRAIN EP28, B:127, L:2.6422, AvgL:2.5614\n",
      "# TRAIN EP29, B:127, L:0.9398, AvgL:2.6828\n",
      "# TRAIN EP30, B:127, L:2.0970, AvgL:2.5903\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:12.12, b:0.00, w:114.00, 50p:5.00, 75p:14.00, 90p:33.00\n",
      "# TRAIN EP31, B:127, L:0.6618, AvgL:2.7089\n",
      "# TRAIN EP32, B:127, L:2.6328, AvgL:2.5103\n",
      "# TRAIN EP33, B:127, L:3.8741, AvgL:2.3217\n",
      "# TRAIN EP34, B:127, L:3.2495, AvgL:2.4228\n",
      "# TRAIN EP35, B:127, L:1.1593, AvgL:2.4248\n",
      "# TRAIN EP36, B:127, L:1.7527, AvgL:2.3669\n",
      "# TRAIN EP37, B:127, L:1.9348, AvgL:2.0282\n",
      "# TRAIN EP38, B:127, L:2.1824, AvgL:2.2312\n",
      "# TRAIN EP39, B:127, L:2.9347, AvgL:2.3482\n",
      "# TRAIN EP40, B:127, L:2.9283, AvgL:2.0881\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:9.10, b:0.00, w:96.00, 50p:4.00, 75p:12.00, 90p:25.00\n",
      "# TRAIN EP41, B:127, L:1.0317, AvgL:2.1221\n",
      "# TRAIN EP42, B:127, L:0.5369, AvgL:1.9071\n",
      "# TRAIN EP43, B:127, L:0.4533, AvgL:1.7763\n",
      "# TRAIN EP44, B:127, L:1.0342, AvgL:1.9983\n",
      "# TRAIN EP45, B:127, L:2.7549, AvgL:1.9953\n",
      "# TRAIN EP46, B:127, L:1.7464, AvgL:1.8670\n",
      "# TRAIN EP47, B:127, L:1.3196, AvgL:1.8529\n",
      "# TRAIN EP48, B:127, L:1.8339, AvgL:1.9226\n",
      "# TRAIN EP49, B:127, L:1.2858, AvgL:1.9072\n",
      "# TRAIN EP50, B:127, L:1.7880, AvgL:1.7917\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:6.88, b:0.00, w:93.00, 50p:3.00, 75p:8.00, 90p:19.00\n",
      "# TRAIN EP51, B:127, L:0.4786, AvgL:1.7745\n",
      "# TRAIN EP52, B:127, L:3.9799, AvgL:1.6953\n",
      "# TRAIN EP53, B:127, L:2.7308, AvgL:1.6713\n",
      "# TRAIN EP54, B:127, L:1.0354, AvgL:1.4922\n",
      "# TRAIN EP55, B:127, L:1.1014, AvgL:1.5082\n",
      "# TRAIN EP56, B:127, L:1.2731, AvgL:1.5559\n",
      "# TRAIN EP57, B:127, L:0.3594, AvgL:1.5291\n",
      "# TRAIN EP58, B:127, L:2.1609, AvgL:1.5131\n",
      "# TRAIN EP59, B:127, L:1.9036, AvgL:1.6903\n",
      "# TRAIN EP60, B:127, L:2.9157, AvgL:1.4151\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:6.20, b:0.00, w:65.00, 50p:3.00, 75p:8.00, 90p:18.00\n",
      "# TRAIN EP61, B:127, L:1.4563, AvgL:1.5536\n",
      "# TRAIN EP62, B:127, L:1.6772, AvgL:1.4916\n",
      "# TRAIN EP63, B:127, L:2.2324, AvgL:1.4919\n",
      "# TRAIN EP64, B:127, L:2.2448, AvgL:1.6497\n",
      "# TRAIN EP65, B:127, L:1.3797, AvgL:1.4728\n",
      "# TRAIN EP66, B:127, L:1.6356, AvgL:1.2888\n",
      "# TRAIN EP67, B:127, L:0.6030, AvgL:1.4934\n",
      "# TRAIN EP68, B:127, L:3.2835, AvgL:1.3677\n",
      "# TRAIN EP69, B:127, L:0.8608, AvgL:1.3938\n",
      "# TRAIN EP70, B:127, L:0.1648, AvgL:1.3577\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:5.57, b:0.00, w:75.00, 50p:3.00, 75p:7.00, 90p:14.00\n",
      "# TRAIN EP71, B:127, L:2.2683, AvgL:1.3309\n",
      "# TRAIN EP72, B:127, L:2.6182, AvgL:1.2939\n",
      "# TRAIN EP73, B:127, L:1.0035, AvgL:1.2974\n",
      "# TRAIN EP74, B:127, L:1.8723, AvgL:1.3425\n",
      "# TRAIN EP75, B:127, L:0.2475, AvgL:1.3149\n",
      "# TRAIN EP76, B:127, L:1.8159, AvgL:1.2811\n",
      "# TRAIN EP77, B:127, L:1.4962, AvgL:1.1560\n",
      "# TRAIN EP78, B:127, L:0.6578, AvgL:1.3375\n",
      "# TRAIN EP79, B:127, L:0.0000, AvgL:1.3952\n",
      "# TRAIN EP80, B:127, L:0.9282, AvgL:1.2049\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:5.26, b:0.00, w:119.00, 50p:3.00, 75p:6.00, 90p:12.00\n",
      "# TRAIN EP81, B:127, L:1.0366, AvgL:1.2882\n",
      "# TRAIN EP82, B:127, L:0.4589, AvgL:1.3783\n",
      "# TRAIN EP83, B:127, L:1.2726, AvgL:1.2419\n",
      "# TRAIN EP84, B:127, L:0.2875, AvgL:1.2786\n",
      "# TRAIN EP85, B:127, L:1.2499, AvgL:1.2088\n",
      "# TRAIN EP86, B:127, L:1.4408, AvgL:1.1551\n",
      "# TRAIN EP87, B:127, L:0.3176, AvgL:1.1894\n",
      "# TRAIN EP88, B:127, L:0.0718, AvgL:1.1851\n",
      "# TRAIN EP89, B:127, L:4.0678, AvgL:1.2487\n",
      "# TRAIN EP90, B:127, L:0.1557, AvgL:1.1344\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.48, b:0.00, w:48.00, 50p:2.00, 75p:6.00, 90p:11.00\n",
      "# TRAIN EP91, B:127, L:0.0139, AvgL:1.0748\n",
      "# TRAIN EP92, B:127, L:0.4522, AvgL:1.0826\n",
      "# TRAIN EP93, B:127, L:0.3336, AvgL:1.1082\n",
      "# TRAIN EP94, B:127, L:1.3499, AvgL:1.1977\n",
      "# TRAIN EP95, B:127, L:0.9769, AvgL:1.1679\n",
      "# TRAIN EP96, B:127, L:0.0000, AvgL:1.1872\n",
      "# TRAIN EP97, B:127, L:0.7556, AvgL:1.1804\n",
      "# TRAIN EP98, B:127, L:0.4846, AvgL:1.1126\n",
      "# TRAIN EP99, B:127, L:0.8811, AvgL:1.1615\n",
      "# TRAIN EP100, B:127, L:0.5208, AvgL:1.0762\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.34, b:0.00, w:49.00, 50p:2.00, 75p:5.00, 90p:10.00\n",
      "# TRAIN EP101, B:127, L:0.8936, AvgL:0.9823\n",
      "# TRAIN EP102, B:127, L:0.0000, AvgL:1.2030\n",
      "# TRAIN EP103, B:127, L:0.3605, AvgL:1.1813\n",
      "# TRAIN EP104, B:127, L:0.0000, AvgL:1.1323\n",
      "# TRAIN EP105, B:127, L:0.4027, AvgL:1.1362\n",
      "# TRAIN EP106, B:127, L:1.3629, AvgL:1.2123\n",
      "# TRAIN EP107, B:127, L:0.5536, AvgL:0.9424\n",
      "# TRAIN EP108, B:127, L:0.3916, AvgL:0.9571\n",
      "# TRAIN EP109, B:127, L:0.5412, AvgL:1.0278\n",
      "# TRAIN EP110, B:127, L:0.2032, AvgL:1.0195\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.26, b:0.00, w:111.00, 50p:2.00, 75p:5.00, 90p:9.90\n",
      "# TRAIN EP111, B:127, L:0.2286, AvgL:1.1368\n",
      "# TRAIN EP112, B:127, L:2.3619, AvgL:1.1302\n",
      "# TRAIN EP113, B:127, L:1.4106, AvgL:1.1981\n",
      "# TRAIN EP114, B:127, L:0.0061, AvgL:1.0157\n",
      "# TRAIN EP115, B:127, L:1.1565, AvgL:1.1811\n",
      "# TRAIN EP116, B:127, L:1.2113, AvgL:1.1148\n",
      "# TRAIN EP117, B:127, L:0.1613, AvgL:1.0785\n",
      "# TRAIN EP118, B:127, L:2.0262, AvgL:0.9927\n",
      "# TRAIN EP119, B:127, L:0.5278, AvgL:1.1945\n",
      "# TRAIN EP120, B:127, L:0.2849, AvgL:1.0321\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.40, b:0.00, w:105.00, 50p:2.00, 75p:5.00, 90p:9.00\n",
      "# TRAIN EP121, B:127, L:0.5342, AvgL:1.0223\n",
      "# TRAIN EP122, B:127, L:1.5900, AvgL:0.9647\n",
      "# TRAIN EP123, B:127, L:0.9057, AvgL:0.9540\n",
      "# TRAIN EP124, B:127, L:0.1322, AvgL:0.8874\n",
      "# TRAIN EP125, B:127, L:0.2388, AvgL:1.1169\n",
      "# TRAIN EP126, B:127, L:0.9308, AvgL:1.0338\n",
      "# TRAIN EP127, B:127, L:1.6233, AvgL:1.0028\n",
      "# TRAIN EP128, B:127, L:0.3398, AvgL:0.9027\n",
      "# TRAIN EP129, B:127, L:1.6961, AvgL:1.1226\n",
      "# TRAIN EP130, B:127, L:0.1793, AvgL:0.9936\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.14, b:0.00, w:74.00, 50p:2.00, 75p:5.00, 90p:10.00\n",
      "# TRAIN EP131, B:127, L:0.0000, AvgL:0.9386\n",
      "# TRAIN EP132, B:127, L:0.6351, AvgL:0.9490\n",
      "# TRAIN EP133, B:127, L:0.1374, AvgL:1.0006\n",
      "# TRAIN EP134, B:127, L:0.4611, AvgL:1.0062\n",
      "# TRAIN EP135, B:127, L:0.4108, AvgL:0.9778\n",
      "# TRAIN EP136, B:127, L:0.2055, AvgL:0.8995\n",
      "# TRAIN EP137, B:127, L:0.0409, AvgL:1.0123\n",
      "# TRAIN EP138, B:127, L:0.4768, AvgL:0.8921\n",
      "# TRAIN EP139, B:127, L:1.6358, AvgL:0.7482\n",
      "# TRAIN EP140, B:127, L:2.0338, AvgL:0.8511\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.51, b:0.00, w:90.00, 50p:2.00, 75p:5.00, 90p:10.00\n",
      "# TRAIN EP141, B:127, L:0.0634, AvgL:0.8257\n",
      "# TRAIN EP142, B:127, L:0.0000, AvgL:0.9834\n",
      "# TRAIN EP143, B:127, L:2.9450, AvgL:0.8802\n",
      "# TRAIN EP144, B:127, L:0.1290, AvgL:0.9666\n",
      "# TRAIN EP145, B:127, L:0.1848, AvgL:1.0176\n",
      "# TRAIN EP146, B:127, L:0.6939, AvgL:0.8815\n",
      "# TRAIN EP147, B:127, L:0.0011, AvgL:0.8569\n",
      "# TRAIN EP148, B:127, L:0.2568, AvgL:0.9936\n",
      "# TRAIN EP149, B:127, L:0.1855, AvgL:0.7904\n",
      "# TRAIN EP150, B:127, L:0.0434, AvgL:0.7760\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.16, b:0.00, w:68.00, 50p:3.00, 75p:6.00, 90p:9.00\n",
      "# TRAIN EP151, B:127, L:1.1861, AvgL:0.9823\n",
      "# TRAIN EP152, B:127, L:0.1303, AvgL:0.8423\n",
      "# TRAIN EP153, B:127, L:0.9931, AvgL:0.9730\n",
      "# TRAIN EP154, B:127, L:0.4521, AvgL:1.0013\n",
      "# TRAIN EP155, B:127, L:0.1811, AvgL:0.9596\n",
      "# TRAIN EP156, B:127, L:0.0000, AvgL:0.9112\n",
      "# TRAIN EP157, B:127, L:0.1192, AvgL:0.8778\n",
      "# TRAIN EP158, B:127, L:0.1145, AvgL:0.9771\n",
      "# TRAIN EP159, B:127, L:0.0481, AvgL:0.9300\n",
      "# TRAIN EP160, B:127, L:0.0000, AvgL:1.0588\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.65, b:0.00, w:74.00, 50p:3.00, 75p:7.00, 90p:11.00\n",
      "# TRAIN EP161, B:127, L:0.1368, AvgL:0.8818\n",
      "# TRAIN EP162, B:127, L:0.3129, AvgL:0.8584\n",
      "# TRAIN EP163, B:127, L:0.0000, AvgL:1.0325\n",
      "# TRAIN EP164, B:127, L:1.0272, AvgL:0.8012\n",
      "# TRAIN EP165, B:127, L:0.6029, AvgL:0.7437\n",
      "# TRAIN EP166, B:127, L:1.7876, AvgL:0.9127\n",
      "# TRAIN EP167, B:127, L:1.5476, AvgL:0.7722\n",
      "# TRAIN EP168, B:127, L:0.1780, AvgL:0.9863\n",
      "# TRAIN EP169, B:127, L:1.4060, AvgL:0.8272\n",
      "# TRAIN EP170, B:127, L:0.8790, AvgL:0.9006\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:3.88, b:0.00, w:61.00, 50p:2.00, 75p:5.00, 90p:9.00\n",
      "# TRAIN EP171, B:127, L:0.4092, AvgL:1.0233\n",
      "# TRAIN EP172, B:127, L:1.2026, AvgL:0.9783\n",
      "# TRAIN EP173, B:127, L:1.6827, AvgL:0.8475\n",
      "# TRAIN EP174, B:127, L:0.0045, AvgL:0.8750\n",
      "# TRAIN EP175, B:127, L:1.3054, AvgL:0.8424\n",
      "# TRAIN EP176, B:127, L:1.2323, AvgL:0.9768\n",
      "# TRAIN EP177, B:127, L:1.3863, AvgL:0.8952\n",
      "# TRAIN EP178, B:127, L:1.0541, AvgL:0.8895\n",
      "# TRAIN EP179, B:127, L:0.4873, AvgL:0.7465\n",
      "# TRAIN EP180, B:127, L:0.1621, AvgL:0.7556\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:3.96, b:0.00, w:85.00, 50p:2.00, 75p:5.00, 90p:10.00\n",
      "# TRAIN EP181, B:127, L:0.2116, AvgL:0.9998\n",
      "# TRAIN EP182, B:127, L:0.9534, AvgL:0.9111\n",
      "# TRAIN EP183, B:127, L:0.0213, AvgL:0.8362\n",
      "# TRAIN EP184, B:127, L:0.9828, AvgL:0.8325\n",
      "# TRAIN EP185, B:127, L:0.0000, AvgL:0.9010\n",
      "# TRAIN EP186, B:127, L:1.0324, AvgL:0.7858\n",
      "# TRAIN EP187, B:127, L:0.7004, AvgL:0.8742\n",
      "# TRAIN EP188, B:127, L:0.9802, AvgL:0.6665\n",
      "# TRAIN EP189, B:127, L:0.4603, AvgL:0.9221\n",
      "# TRAIN EP190, B:127, L:0.2026, AvgL:0.9686\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:3.77, b:0.00, w:86.00, 50p:2.00, 75p:5.00, 90p:9.00\n",
      "# TRAIN EP191, B:127, L:0.5724, AvgL:1.0022\n",
      "# TRAIN EP192, B:127, L:0.5824, AvgL:0.7769\n",
      "# TRAIN EP193, B:127, L:1.1010, AvgL:0.8196\n",
      "# TRAIN EP194, B:127, L:0.3660, AvgL:0.7282\n",
      "# TRAIN EP195, B:127, L:0.0000, AvgL:0.8475\n",
      "# TRAIN EP196, B:127, L:0.2772, AvgL:0.9053\n",
      "# TRAIN EP197, B:127, L:1.2374, AvgL:0.9807\n",
      "# TRAIN EP198, B:127, L:0.1513, AvgL:0.9614\n",
      "# TRAIN EP199, B:127, L:1.8126, AvgL:0.8563\n",
      "# TRAIN EP200, B:127, L:0.2981, AvgL:0.7637\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:3.93, b:0.00, w:68.00, 50p:2.00, 75p:5.00, 90p:9.00\n",
      "# TRAIN EP201, B:127, L:0.0000, AvgL:0.6711\n",
      "# TRAIN EP202, B:127, L:2.1918, AvgL:0.8488\n",
      "# TRAIN EP203, B:127, L:0.7544, AvgL:0.8953\n",
      "# TRAIN EP204, B:127, L:1.0429, AvgL:0.8057\n",
      "# TRAIN EP205, B:127, L:1.8880, AvgL:0.7891\n",
      "# TRAIN EP206, B:127, L:0.9204, AvgL:0.7460\n",
      "# TRAIN EP207, B:127, L:0.4215, AvgL:0.8308\n",
      "# TRAIN EP208, B:127, L:1.1237, AvgL:0.6520\n",
      "# TRAIN EP209, B:127, L:0.0000, AvgL:0.8850\n",
      "# TRAIN EP210, B:127, L:0.5686, AvgL:0.7975\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:3.76, b:0.00, w:36.00, 50p:2.00, 75p:5.00, 90p:9.00\n",
      "# TRAIN EP211, B:127, L:0.0000, AvgL:0.8780\n",
      "# TRAIN EP212, B:127, L:1.3426, AvgL:0.8552\n",
      "# TRAIN EP213, B:127, L:1.5211, AvgL:0.8001\n",
      "# TRAIN EP214, B:127, L:0.8153, AvgL:0.8681\n",
      "# TRAIN EP215, B:127, L:0.0000, AvgL:0.7115\n",
      "# TRAIN EP216, B:127, L:1.1930, AvgL:0.9097\n",
      "# TRAIN EP217, B:127, L:0.7961, AvgL:0.9230\n",
      "# TRAIN EP218, B:127, L:1.2003, AvgL:0.8933\n",
      "# TRAIN EP219, B:127, L:0.0000, AvgL:0.7612\n",
      "# TRAIN EP220, B:127, L:1.3713, AvgL:0.8189\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.05, b:0.00, w:72.00, 50p:2.00, 75p:5.00, 90p:10.00\n",
      "# TRAIN EP221, B:127, L:1.1153, AvgL:1.0129\n",
      "# TRAIN EP222, B:127, L:0.0042, AvgL:0.7990\n",
      "# TRAIN EP223, B:127, L:2.5140, AvgL:0.8706\n",
      "# TRAIN EP224, B:127, L:0.0454, AvgL:0.7317\n",
      "# TRAIN EP225, B:127, L:0.3774, AvgL:0.7241\n",
      "# TRAIN EP226, B:127, L:2.1472, AvgL:0.8371\n",
      "# TRAIN EP227, B:127, L:0.0852, AvgL:0.8499\n",
      "# TRAIN EP228, B:127, L:0.0000, AvgL:0.6640\n",
      "# TRAIN EP229, B:127, L:0.0463, AvgL:0.8345\n",
      "# TRAIN EP230, B:127, L:1.0933, AvgL:0.8408\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:3.72, b:0.00, w:77.00, 50p:2.00, 75p:5.00, 90p:9.90\n",
      "# TRAIN EP231, B:127, L:2.0884, AvgL:0.8235\n",
      "# TRAIN EP232, B:127, L:0.1747, AvgL:0.8391\n",
      "# TRAIN EP233, B:127, L:1.9001, AvgL:0.7696\n",
      "# TRAIN EP234, B:127, L:2.0416, AvgL:0.9270\n",
      "# TRAIN EP235, B:127, L:0.0013, AvgL:0.8100\n",
      "# TRAIN EP236, B:127, L:0.0000, AvgL:0.8941\n",
      "# TRAIN EP237, B:127, L:0.0110, AvgL:0.6431\n",
      "# TRAIN EP238, B:127, L:0.0004, AvgL:0.9354\n",
      "# TRAIN EP239, B:127, L:2.1314, AvgL:0.7589\n",
      "# TRAIN EP240, B:127, L:0.2075, AvgL:0.7076\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:3.76, b:0.00, w:20.00, 50p:2.00, 75p:6.00, 90p:9.00\n",
      "# TRAIN EP241, B:127, L:1.0374, AvgL:0.8564\n",
      "# TRAIN EP242, B:127, L:0.1215, AvgL:0.8175\n",
      "# TRAIN EP243, B:127, L:0.0000, AvgL:0.6899\n",
      "# TRAIN EP244, B:127, L:0.9640, AvgL:0.7166\n",
      "# TRAIN EP245, B:127, L:0.0000, AvgL:0.7291\n",
      "# TRAIN EP246, B:127, L:0.0139, AvgL:0.7429\n",
      "# TRAIN EP247, B:127, L:0.0000, AvgL:0.6850\n",
      "# TRAIN EP248, B:127, L:1.1856, AvgL:0.7741\n",
      "# TRAIN EP249, B:127, L:0.0000, AvgL:0.7352\n",
      "# TRAIN EP250, B:127, L:0.0034, AvgL:0.9005\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:3.69, b:0.00, w:35.00, 50p:2.00, 75p:5.00, 90p:9.00\n",
      "# TRAIN EP251, B:127, L:1.3878, AvgL:0.7046\n",
      "# TRAIN EP252, B:127, L:1.4861, AvgL:0.6154\n",
      "# TRAIN EP253, B:127, L:1.2515, AvgL:0.7148\n",
      "# TRAIN EP254, B:127, L:0.8031, AvgL:0.9008\n",
      "# TRAIN EP255, B:127, L:0.0000, AvgL:0.6472\n",
      "# TRAIN EP256, B:127, L:0.0051, AvgL:0.7730\n",
      "# TRAIN EP257, B:127, L:0.1725, AvgL:0.7597\n",
      "# TRAIN EP258, B:127, L:0.7641, AvgL:0.6933\n",
      "# TRAIN EP259, B:127, L:1.9255, AvgL:0.7516\n",
      "# TRAIN EP260, B:127, L:1.9968, AvgL:0.8630\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:3.70, b:0.00, w:93.00, 50p:2.00, 75p:4.25, 90p:9.00\n",
      "# TRAIN EP261, B:127, L:0.9830, AvgL:0.6716\n",
      "# TRAIN EP262, B:127, L:1.5367, AvgL:0.5944\n",
      "# TRAIN EP263, B:127, L:1.8397, AvgL:0.7113\n",
      "# TRAIN EP264, B:127, L:0.1102, AvgL:0.7139\n",
      "# TRAIN EP265, B:127, L:3.1490, AvgL:0.7778\n",
      "# TRAIN EP266, B:127, L:0.2135, AvgL:0.6302\n",
      "# TRAIN EP267, B:127, L:0.0000, AvgL:0.7090\n",
      "# TRAIN EP268, B:127, L:0.0399, AvgL:0.7535\n",
      "# TRAIN EP269, B:127, L:0.2365, AvgL:0.7614\n",
      "# TRAIN EP270, B:127, L:0.8655, AvgL:0.7090\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:3.91, b:0.00, w:62.00, 50p:3.00, 75p:5.00, 90p:9.00\n",
      "# TRAIN EP271, B:127, L:0.6254, AvgL:0.8831\n",
      "# TRAIN EP272, B:127, L:1.3774, AvgL:0.8388\n",
      "# TRAIN EP273, B:127, L:0.1620, AvgL:0.7460\n",
      "# TRAIN EP274, B:127, L:0.0549, AvgL:0.7064\n",
      "# TRAIN EP275, B:127, L:0.0000, AvgL:0.7564\n",
      "# TRAIN EP276, B:127, L:0.0000, AvgL:0.6772\n",
      "# TRAIN EP277, B:127, L:2.1082, AvgL:0.7596\n",
      "# TRAIN EP278, B:127, L:0.0015, AvgL:0.8129\n",
      "# TRAIN EP279, B:127, L:1.2278, AvgL:0.8310\n",
      "# TRAIN EP280, B:127, L:0.5138, AvgL:0.7159\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:4.16, b:0.00, w:59.00, 50p:3.00, 75p:5.00, 90p:10.00\n",
      "# TRAIN EP281, B:127, L:0.1451, AvgL:0.7817\n",
      "# TRAIN EP282, B:127, L:1.1978, AvgL:0.7110\n",
      "# TRAIN EP283, B:127, L:0.2345, AvgL:0.7203\n",
      "# TRAIN EP284, B:127, L:0.4702, AvgL:0.7171\n",
      "# TRAIN EP285, B:127, L:0.2969, AvgL:0.7356\n",
      "# TRAIN EP286, B:127, L:0.9613, AvgL:0.7729\n",
      "# TRAIN EP287, B:127, L:0.0000, AvgL:0.7787\n",
      "# TRAIN EP288, B:127, L:0.1489, AvgL:0.7546\n",
      "# TRAIN EP289, B:127, L:0.5790, AvgL:0.6405\n",
      "# TRAIN EP290, B:127, L:1.6072, AvgL:0.6782\n",
      "# TEST/FUNCTION B:63\n",
      "# TEST/FUNCTION avg.rank:3.71, b:0.00, w:21.00, 50p:2.00, 75p:5.00, 90p:9.00\n",
      "# TRAIN EP291, B:44, L:1.6874, AvgL:0.6433"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b312d53f6add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMTETrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mld_mg_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mld_mg_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-6826c09534c1>\u001b[0m in \u001b[0;36mMTETrainer\u001b[0;34m(p_nep, p_model, p_ld_train, p_ld_test, p_optim, p_lossfn)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md_ep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_nep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_ld_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a864e1465ed7>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, p_ind)\u001b[0m\n\u001b[1;32m     27\u001b[0m             tmp_prog, tmp_example = self.generator.generate(\n\u001b[1;32m     28\u001b[0m                 \u001b[0mfixed_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# this should be fixed to 2 since we learn size 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mexample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtmp_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             )\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtmp_prog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtmp_prog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Trinity/utils_morpheus.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, fixed_depth, example, probs)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mtmp_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_prog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m                     \u001b[0;31m# print(\"YES\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m                     return (\n",
      "\u001b[0;32m~/Trinity/utils_morpheus.py\u001b[0m in \u001b[0;36msanity_check\u001b[0;34m(self, p_prog, p_example)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;31m# 0.1) don't be equal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0meq_r\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Trinity/utils_morpheus.py\u001b[0m in \u001b[0;36meq_r\u001b[0;34m(actual, expect)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# logger.info(robjects.r(actual))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# logger.info(robjects.r(expect))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mret_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rpy2/robjects/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStrSexpVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rpy2/robjects/functions.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr_k\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         return (super(SignatureTranslatedFunction, self)\n\u001b[0;32m--> 192\u001b[0;31m                 .__call__(*args, **kwargs))\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rpy2/robjects/functions.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mnew_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy2rpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rpy2/rinterface_lib/conversion.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cdata_res_to_rinterface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mcdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m# TODO: test cdata is of the expected CType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cdata_to_rinterface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rpy2/rinterface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m                     \u001b[0mcall_r\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m                     \u001b[0membedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobalenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sexp__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m                     error_occured))\n\u001b[0m\u001b[1;32m    772\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merror_occured\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rinterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_geterrmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MTETrainer(1000000, mte, ld_mg_train, ld_mg_test, optimizer, m_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
