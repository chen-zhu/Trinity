{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransNeo/AlphaNeo Ranker\n",
    "- tell which pair of tables are functionally/logically closer on execution chain\n",
    "- Stage: Cambrian\n",
    "- Version: Spriggina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tyrell.spec as S\n",
    "from tyrell.decider import Example\n",
    "\n",
    "# Morpheus Version\n",
    "from MorpheusInterpreter import *\n",
    "from ProgramSpace import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankerDataset(Dataset):\n",
    "    def __init__(self, p_config=None, p_dataset=None, p_interpreter=None):\n",
    "        self.interpreter = p_interpreter\n",
    "        self.dataset = p_dataset\n",
    "        self.config = p_config\n",
    "        \n",
    "        # flatten the dataset in the form [str_example]\n",
    "        # we don't care about what the prog is\n",
    "        self.str_examples = [\n",
    "            self.dataset[dkey][i][1]\n",
    "            for dkey in self.dataset.keys()\n",
    "            for i in range(len(self.dataset[dkey]))\n",
    "        ]\n",
    "        self.n_exp = len(str_examples)\n",
    "        self.n_neg = self.config[\"ranker\"][\"n_neg\"]\n",
    "        self.n_row = self.config[\"ranker\"][\"n_row\"]\n",
    "        self.n_col = self.config[\"ranker\"][\"n_col\"]\n",
    "        \n",
    "        # == only works for chain: 1 input, 1 output ==\n",
    "        # then compute the abstraction map of the variables\n",
    "        self.LMTX = np.full(\n",
    "            (self.n_exp, self.n_row, self.n_col),\n",
    "            self.interpreter.CAMB_DICT[\"<PAD>\"],\n",
    "            dtype=int,\n",
    "        )\n",
    "        self.RMTX = np.full(\n",
    "            (self.n_exp, self.n_row, self.n_col),\n",
    "            self.interpreter.CAMB_DICT[\"<PAD>\"],\n",
    "            dtype=int,\n",
    "        )\n",
    "        for i in range(self.n_exp):\n",
    "            d_example = self.str_examples[i]\n",
    "            var_input = self.interpreter.load_data_into_var(d_example.input)\n",
    "            var_output= self.interpreter.load_data_into_var(d_example.output)\n",
    "            map_input = self.interpreter.camb_get_abs(var_input)\n",
    "            map_output= self.interpreter.camb_get_abs(var_output)\n",
    "            # store into matrices\n",
    "            self.LMTX[i,:,:] = map_input\n",
    "            self.RMTX[i,:,:] = map_output\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_exp\n",
    "    \n",
    "    '''\n",
    "    should always use batch_size=1 so as to ensure the ratio of negative examples\n",
    "    '''\n",
    "    def __getitem__(self, p_ind):\n",
    "        # generate random tables as negative examples\n",
    "        tmpWMTX = np.full(\n",
    "            (self.n_neg, self.n_row, self.n_col),\n",
    "            self.interpreter.CAMB_DICT[\"<PAD>\"],\n",
    "            dtype=int,\n",
    "        )\n",
    "        for i in range(self.n_neg):\n",
    "            var_table = self.interpreter.random_table()\n",
    "            map_table = self.interpreter.camb_get_abs(var_table)\n",
    "            tmpWMTX[i,:,:] = map_table\n",
    "        \n",
    "        # (left, right, negatives)\n",
    "        return (\n",
    "            self.LMTX[p_ind,:,:], # (map_r, map_c)\n",
    "            self.RMTX[p_ind,:,:], # (map_r, map_c)\n",
    "            tmpWMTX, # (n_neg, map_r, map_c)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEncoder(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(ValueEncoder, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        self.vocab_size = self.config[\"val\"][\"vocab_size\"]\n",
    "        self.embd_dim = self.config[\"val\"][\"embd_dim\"]\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embd_dim,\n",
    "            self.config[\"val\"][\"IDX_PAD\"],\n",
    "        )\n",
    "        \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels = self.config[\"val\"][\"embd_dim\"],\n",
    "            out_channels = self.config[\"val\"][\"conv_n_kernels\"],\n",
    "            kernel_size = self.config[\"val\"][\"conv_kernel_size\"],\n",
    "        )\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size = self.config[\"val\"][\"pool_kernel_size\"],\n",
    "            padding = self.config[\"val\"][\"IDX_PAD\"],\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            self.config[\"val\"][\"conv_n_kernels\"],\n",
    "            self.config[\"embd_dim\"],\n",
    "        )\n",
    "        \n",
    "    def forward(self, bp_map):\n",
    "        # batched maps, (B, map_r, map_c)\n",
    "        # in this version, every value only contains 1 map\n",
    "        B = bp_map.shape[0]\n",
    "        \n",
    "        # (B, map_r, map_c, val_embd_dim) -> (B, val_embd_dim, map_r, map_c)\n",
    "        d_embd = self.embedding(bp_map).permute(0,3,1,2)\n",
    "        \n",
    "        # (B, n_kernel, map_r, 1)\n",
    "        d_conv = F.relu(self.conv(d_embd))\n",
    "        \n",
    "        # (B, n_kernel)\n",
    "        d_pool = self.pool(d_conv).view(B,self.config[\"val\"][\"conv_n_kernels\"])\n",
    "        \n",
    "        # (B, embd_dim)\n",
    "        d_out = torch.sigmoid(\n",
    "            self.fc(d_pool)\n",
    "        )\n",
    "        \n",
    "        return d_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ranker(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(Ranker, self).__init__()\n",
    "        self.config = p_config\n",
    "        self.value_encoder = ValueEncoder(p_config=p_config)\n",
    "        self.fc0 = nn.Linear(\n",
    "            self.config[\"embd_dim\"]*2,\n",
    "            2048,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(\n",
    "            2048,\n",
    "            2,\n",
    "        )\n",
    "        \n",
    "    def forward(self, pL, pR, pN):\n",
    "        # pL/pR: (B=1, map_r, map_c)\n",
    "        # pN: (B=1, n_neg, map_r, map_c)\n",
    "        vL = self.value_encoder(pL) # (B, embd_dim)\n",
    "        vR = self.value_encoder(pR) # (B, embd_dim)\n",
    "        vN = self.value_encoder(\n",
    "            pN.view(\n",
    "                self.config[\"ranker\"][\"n_neg\"],\n",
    "                self.config[\"ranker\"][\"n_row\"],\n",
    "                self.config[\"ranker\"][\"n_col\"],\n",
    "            ),\n",
    "        ) # (B=n_neg, embd_dim)\n",
    "        \n",
    "        vLL = vL.expand(\n",
    "            self.config[\"ranker\"][\"n_neg\"]+1,\n",
    "            self.config[\"embd_dim\"],\n",
    "        ) # (n_neg+1, embd_dim)\n",
    "        vRN = torch.cat(\n",
    "            [vR,vN],\n",
    "            dim=0\n",
    "        ) # (n_neg+1, embd_dim)\n",
    "        \n",
    "        vII = torch.cat([vLL,vRN],dim=1) # (n_neg+1, embd_dim * 2)\n",
    "        \n",
    "        # == Notice: don't do any activation at the last layer ==\n",
    "        vOO = self.fc1(\n",
    "            F.relu(\n",
    "                self.fc0(\n",
    "                    vII\n",
    "                )\n",
    "            )\n",
    "        )# (n_neg+1, 2)\n",
    "        \n",
    "        return vOO\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RankerTrainer(p_config, p_model, p_ld_train_data, p_optim, p_lossfn):\n",
    "    for d_ep in range(p_config[\"ranker\"][\"n_ep\"]):\n",
    "        epoch_loss_list = []\n",
    "        \n",
    "        for batch_idx, (d_left, d_right, d_negative) in enumerate(p_ld_train_data):\n",
    "            p_model.train()\n",
    "            \n",
    "            if use_cuda:\n",
    "                td_left = Variable(d_left).cuda() # (B=1, map_r, map_c)\n",
    "                td_right = Variable(d_right).cuda() # (B=1, map_r, map_c)\n",
    "                td_negative = Variable(d_negative).cuda() # (B=1, n_neg, map_r, map_c)\n",
    "                td_label = Variable(torch.tensor(\n",
    "                    [1]+[0 for _ in range(p_config[\"ranker\"][\"n_neg\"])]\n",
    "                )).cuda()\n",
    "            else:\n",
    "                td_left = Variable(d_left) # (B=1, map_r, map_c)\n",
    "                td_right = Variable(d_right) # (B=1, map_r, map_c)\n",
    "                td_negative = Variable(d_negative) # (B=1, n_neg, map_r, map_c)\n",
    "                td_label = Variable(torch.tensor(\n",
    "                    [1]+[0 for _ in range(p_config[\"ranker\"][\"n_neg\"])]\n",
    "                ))\n",
    "                \n",
    "            # (n_neg+1, 2)\n",
    "            d_output = p_model(td_left, td_right, td_negative)\n",
    "            p_optim.zero_grad()\n",
    "            d_loss = p_lossfn(\n",
    "                F.log_softmax(d_output, dim=1),\n",
    "                td_label,\n",
    "            )\n",
    "            epoch_loss_list.append(d_loss.cpu().data.numpy())\n",
    "            d_loss.backward()\n",
    "            p_optim.step()\n",
    "            \n",
    "            print(\"\\r# EP:{}, B:{}, ep.loss:{:.2f}\".format(\n",
    "                d_ep, batch_idx, sum(epoch_loss_list),\n",
    "            ),end=\"\")\n",
    "        \n",
    "        # end of epoch print a new line\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_interpreter = MorpheusInterpreter()\n",
    "m_spec = S.parse_file('./example/camb3.tyrell')\n",
    "\n",
    "m_config = {\n",
    "    \"val\":{\n",
    "        \"vocab_size\": len(m_interpreter.CAMB_LIST),\n",
    "        \"embd_dim\": 16, # embedding dim of CAMB abstract token\n",
    "        \"conv_n_kernels\": 512,\n",
    "        \"conv_kernel_size\": (1,m_interpreter.CAMB_NCOL), \n",
    "        \"pool_kernel_size\": (m_interpreter.CAMB_NROW,1), \n",
    "        \"IDX_PAD\": 0,\n",
    "    },\n",
    "    \"embd_dim\": 128,\n",
    "    \"ranker\":{\n",
    "        \"data_path\": \"./0716MDsize1.pkl\",\n",
    "        \"n_row\": m_interpreter.CAMB_NROW,\n",
    "        \"n_col\": m_interpreter.CAMB_NCOL,\n",
    "        \"n_neg\": 10,\n",
    "        \"n_ep\": 1000000,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# load the data and dataset\n",
    "with open(m_config[\"meta_train\"][\"data_path\"],\"rb\") as f:\n",
    "    m_data = pickle.load(f)\n",
    "# re-distribute the dataset\n",
    "m_train_data = {}\n",
    "m_test_data = {}\n",
    "for dkey in m_data.keys():\n",
    "    tmp_one = int(len(m_data[dkey])/10)\n",
    "    tmp_list = list(range(len(m_data[dkey])))\n",
    "    random.shuffle(tmp_list)\n",
    "    tmp_tr = [m_data[dkey][i] for i in tmp_list[:9*tmp_one]]\n",
    "    tmp_te = [m_data[dkeu][i] for i in tmp_list[9*tmp_one:]]\n",
    "    m_train_data[dkey] = tmp_tr\n",
    "    m_test_data[dkey] = tmp_te\n",
    "    \n",
    "dt_train = RankerDataset(\n",
    "    p_config=m_config, \n",
    "    p_dataset=m_train_data, \n",
    "    p_interpreter=m_interpreter,\n",
    ")\n",
    "dt_test  = RankerDataset(\n",
    "    p_config=m_config,\n",
    "    p_dataset=m_test_data,\n",
    "    p_interpreter=m_interpreter,\n",
    ")\n",
    "ld_train = DataLoader(dataset=dt_train, batch_size=1, shuffle=True)\n",
    "ld_test  = DataLoader(dataset=dt_test, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "m_ranker = Ranker(p_config=m_config)\n",
    "if use_cuda:\n",
    "    m_ranker = m_ranker.cuda()\n",
    "optimizer = torch.optim.Adam(list(m_ranker.parameters()))\n",
    "lossfn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RankerTrainer(m_config, m_ranker, ld_train, optimizer, lossfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
