{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component/Abstraction Representation Learning using TransE\n",
    "- Stage: Cambrian\n",
    "- Version: Charniodiscus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morpheus Version\n",
    "from utils_morpheus import *\n",
    "from ProgramSpace import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphGenDataset(Dataset):\n",
    "    def __init__(self, p_spec=None, p_generator=None, p_interpreter=None, p_dps=None):\n",
    "        self.spec = p_spec\n",
    "        self.generator = p_generator\n",
    "        self.interpreter = p_interpreter\n",
    "        self.dps = p_dps\n",
    "        \n",
    "        # construct a shell list/dict so that every function call (shell) has id\n",
    "        self.shell_list = self.dps.get_neighboring_shells()\n",
    "        self.shell_dict = {\n",
    "            self.shell_list[i]:i for i in range(len(self.shell_list))\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1024\n",
    "    \n",
    "    def __getitem__(self, p_ind):\n",
    "        # basically ignore the p_ind parameter and generate randomly\n",
    "        # returns a pair of abstraction maps\n",
    "        # (tmp_in, tmp_out)\n",
    "        # which is (n_maps, CAMB_NROW, CAMB_NCOL)\n",
    "        # currently assuming single input single output\n",
    "        while True:\n",
    "            tmp_input = self.interpreter.random_table()\n",
    "            tmp_prog, tmp_example = self.generator.generate(\n",
    "                fixed_depth=2, # this should be fixed to 2 since we learn size 1\n",
    "                example=Example(input=[tmp_input], output=None),\n",
    "            )\n",
    "            if tmp_prog is not None and tmp_prog.is_apply():\n",
    "                break\n",
    "        \n",
    "        tmp_in = camb_get_abs(tmp_example.input[0])\n",
    "        tmp_out= camb_get_abs(tmp_example.output)\n",
    "        \n",
    "        tmp_func = self.shell_dict[\n",
    "            (\n",
    "                self.dps.prod_list.index(tmp_prog.production),\n",
    "                tuple(\n",
    "                    [self.dps.node_list.index(tmp_prog.args[i])\n",
    "                    for i in range(len(tmp_prog.args))]\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # =========================================================\n",
    "        # you should also produce a negative sample here and return\n",
    "        if random.random()<0.5:\n",
    "            rep_ind = 0 # means replacing input\n",
    "        else:\n",
    "            rep_ind = 1 # means replacing output\n",
    "            \n",
    "        while True:\n",
    "            rep_val = self.interpreter.random_table()\n",
    "            rep_enc = camb_get_abs(rep_val)\n",
    "            \n",
    "            # don't be the same\n",
    "            if rep_ind==0:\n",
    "                # replace input\n",
    "                if np.array_equal(rep_enc,tmp_in):\n",
    "                    continue\n",
    "                else:\n",
    "                    # not safe here\n",
    "                    # different inputs can result in same output\n",
    "                    # TODO: check and refine later\n",
    "                    break\n",
    "            else:\n",
    "                # replace output\n",
    "                if np.array_equal(rep_enc,tmp_out):\n",
    "                    continue\n",
    "                else:\n",
    "                    # safe here\n",
    "                    # same input will not result in different outputs\n",
    "                    break\n",
    "        \n",
    "        if rep_ind==0:\n",
    "            # replace input\n",
    "            return (tmp_in, tmp_func, tmp_out,\n",
    "                    rep_enc, tmp_func, tmp_out)\n",
    "        else:\n",
    "            # replace output\n",
    "            return (tmp_in, tmp_func, tmp_out,\n",
    "                    tmp_in, tmp_func, rep_enc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListModule(object):\n",
    "    def __init__(self, module, prefix, *args):\n",
    "        self.module = module\n",
    "        self.prefix = prefix\n",
    "        self.num_module = 0\n",
    "        for new_module in args:\n",
    "            self.append(new_module)\n",
    "    \n",
    "    def append(self, new_module):\n",
    "        if not isinstance(new_module, nn.Module):\n",
    "            raise ValueError('Not a Module')\n",
    "        else:\n",
    "            self.module.add_module(self.prefix + str(self.num_module), new_module)\n",
    "            self.num_module += 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.num_module\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if i<0 or i>=self.num_module:\n",
    "            raise IndexError('Out of bound')\n",
    "        return getattr(self.module, self.prefix+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValEncoder(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(ValEncoder, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        # first you need to define different abstraction tokens before using them\n",
    "        self.vocab_size = self.config[\"val\"][\"vocab_size\"]\n",
    "        self.embd_dim = self.config[\"val\"][\"embd_dim\"]\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embd_dim,\n",
    "            self.config[\"val\"][\"IDX_PAD\"],\n",
    "        )\n",
    "        \n",
    "        self.conv = nn.Conv3d(\n",
    "            in_channels = self.config[\"val\"][\"embd_dim\"],\n",
    "            out_channels = self.config[\"val\"][\"conv_n_kernels\"],\n",
    "            kernel_size = self.config[\"val\"][\"conv_kernel_size\"],\n",
    "        )\n",
    "        self.pool = nn.MaxPool3d(\n",
    "            kernel_size = self.config[\"val\"][\"pool_kernel_size\"],\n",
    "            padding = self.config[\"val\"][\"IDX_PAD\"],\n",
    "        )\n",
    "            \n",
    "        # temporarily use a single fully connected layer, as a test\n",
    "        self.fc = nn.Linear(\n",
    "            self.config[\"val\"][\"conv_n_kernels\"],\n",
    "            self.config[\"embd_dim\"], # this one is different from abs_embd_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, bp_maps):\n",
    "        # bp_maps: batch of maps\n",
    "        # in a numpy type, (B, n_maps, map_r, map_c)\n",
    "        # n_maps ordered as: a-map, b-map, ...\n",
    "        # prepared by the dataset and should support batch operation\n",
    "        # iterate through every map, inside every map, do batchly\n",
    "        \n",
    "        B = bp_maps.shape[0]\n",
    "        \n",
    "        # (B, n_maps, map_r, map_c, abs_embd_dim)\n",
    "        m_embd = self.embedding(bp_maps)\n",
    "        \n",
    "        # (B, abs_embd_dim, n_maps, map_r, map_c)\n",
    "        # fit the conv shape\n",
    "        m_embd = m_embd.permute(0,4,1,2,3)\n",
    "        \n",
    "        # (B, conv_n_kernels, n_maps, map_r, 1)\n",
    "        m_conv = self.conv(m_embd)\n",
    "        \n",
    "        # (B, conv_n_kernels, 1, 1, 1) -> (B, conv_n_kernels)\n",
    "        # use relu here to filter out irrelevant info\n",
    "        m_pool = F.relu(\n",
    "            self.pool(m_conv)\n",
    "        ).view(B, self.config[\"val\"][\"conv_n_kernels\"])\n",
    "        \n",
    "        # (B, embd_dim)\n",
    "        # use sigmoid here to preserve negative info\n",
    "        m_out = F.sigmoid(self.fc(m_pool))\n",
    "        \n",
    "        return m_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphTransE(nn.Module):\n",
    "    def __init__(self, p_config=None):\n",
    "        super(MorphTransE, self).__init__()\n",
    "        self.config = p_config\n",
    "        \n",
    "        # store the function embedding here\n",
    "        self.vocab_size = self.config[\"func\"][\"vocab_size\"]\n",
    "        self.embd_dim = self.config[\"embd_dim\"]\n",
    "        \n",
    "        self.function_embedding = nn.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embd_dim,\n",
    "            # no definition of padding index\n",
    "        )\n",
    "        self.value_encoder = ValEncoder(p_config=p_config)\n",
    "        \n",
    "    def forward(self, batch_triplets):\n",
    "        # batch_triplets: (\n",
    "        #   (B, n_maps, map_r, map_c),  --> in\n",
    "        #   (B, ),\n",
    "        #   (B, n_maps, map_r, map_c),  --> out\n",
    "        # )\n",
    "        v_in = self.value_encoder(batch_triplets[0]) # (B, embd_dim)\n",
    "        v_out = self.value_encoder(batch_triplets[2]) # (B, embd_dim)\n",
    "        v_func = self.function_embedding(batch_triplets[1]) # (B, embd_dim)\n",
    "        \n",
    "        vr_in = self.value_encoder(batch_triplets[3])\n",
    "        vr_out = self.value_encoder(batch_triplets[5])\n",
    "        vr_func = self.function_embedding(batch_triplets[4])\n",
    "        \n",
    "        nv_in = F.normalize(v_in,2,1)\n",
    "        nv_out = F.normalize(v_out,2,1)\n",
    "        nvr_in = F.normalize(vr_in,2,1)\n",
    "        nvr_out = F.normalize(nvr_out,2,1)\n",
    "        \n",
    "        # p=None, 2-norm\n",
    "        pos_score = torch.norm(nv_in+v_func-nv_out, dim=1)\n",
    "        neg_score = torch.norm(nvr_in+vr_func-nvr_out, dim=1)\n",
    "        \n",
    "        # (B, 1), ..\n",
    "        return (pos_score, neg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MTETrainer(p_nep, p_model, p_ld_data, p_optim, p_lossfn):\n",
    "    for d_ep in range(p_nep):\n",
    "        train_loss_list = []\n",
    "        for batch_idx, bts in enumerate(p_ld_data):\n",
    "            print(\"YES?\")\n",
    "            p_model.train()\n",
    "            if use_cuda:\n",
    "                td_bts = [Variable(bts[i]).cuda() for i in range(len(bts))]\n",
    "            else:\n",
    "                td_bts = [Variable(bts[i]) for i in range(len(bts))]\n",
    "            \n",
    "            d_scores = p_model(td_bts) # (pos,neg)\n",
    "            p_optim.zero_grad()\n",
    "            y = torch.tensor([-1], dtype=torch.float)\n",
    "            if use_cuda:\n",
    "                y = y.cuda()\n",
    "            d_loss = p_lossfn(d_scores[0],d_scores[1],y)\n",
    "            train_loss_list.append(d_loss)\n",
    "            d_loss.backward()\n",
    "            p_optim.step()\n",
    "            \n",
    "            print(\"\\r# EP{}, B:{}, L:{:.4f}, AvgL:{:.4f}\".format(\n",
    "                d_ep, batch_idx, d_loss,\n",
    "                sum(train_loss_list)/len(train_loss_list)\n",
    "            ),end=\"\")\n",
    "        print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_interpreter = MorpheusInterpreter()\n",
    "m_spec = S.parse_file('./example/set_select.tyrell')\n",
    "m_generator = MorpheusGenerator(\n",
    "    spec=m_spec,\n",
    "    interpreter=m_interpreter,\n",
    "    sfn=m_interpreter.sanity_check,\n",
    ")\n",
    "# dumb Program Space\n",
    "m_dps = ProgramSpace(\n",
    "    m_spec, m_interpreter, None, None\n",
    ")\n",
    "m_config = {\n",
    "    \"val\":{\n",
    "        \"vocab_size\": len(CAMB_LIST),\n",
    "        \"embd_dim\": 10, # embedding dim of CAMB abstract token\n",
    "        \"conv_n_kernels\": 200,\n",
    "        \"conv_kernel_size\": (1,1,CAMB_NCOL), # (k_map, k_row, k_col)\n",
    "        \"pool_kernel_size\": (2,CAMB_NROW,1), # (k_map, k_row, k_col)\n",
    "        \"IDX_PAD\": 0,\n",
    "    },\n",
    "    \"func\":{\n",
    "        \"vocab_size\": len(m_dps.prod_list),\n",
    "    },\n",
    "    \"embd_dim\":128,\n",
    "    \"margin\":0.01,\n",
    "}\n",
    "\n",
    "dt_mg = MorphGenDataset(p_spec=m_spec, p_generator=m_generator, p_interpreter=m_interpreter, p_dps=m_dps)\n",
    "ld_mg = DataLoader(dataset=dt_mg, batch_size=4, shuffle=False)\n",
    "\n",
    "mte = MorphTransE(p_config=m_config)\n",
    "m_loss = nn.MarginRankingLoss(margin=m_config[\"margin\"])\n",
    "if use_cuda:\n",
    "    mte = mte.cuda()\n",
    "    m_loss = m_loss.cuda()\n",
    "optimizer = torch.optim.Adam(mte.parameters())\n",
    "    \n",
    "# writer = SummaryWriter(\"runs/0704CAMB_TransE_select\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embd_dim': 128,\n",
       " 'func': {'vocab_size': 72},\n",
       " 'margin': 0.01,\n",
       " 'val': {'IDX_PAD': 0,\n",
       "  'conv_kernel_size': (1, 1, 15),\n",
       "  'conv_n_kernels': 200,\n",
       "  'embd_dim': 10,\n",
       "  'pool_kernel_size': (2, 15, 1),\n",
       "  'vocab_size': 5}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MTETrainer(1000000, mte, ld_mg, optimizer, m_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ld_mg:\n",
    "#     print(i)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
